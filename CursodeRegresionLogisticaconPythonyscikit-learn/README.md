# Curso de Regresi√≥n Log√≠stica con Python y scikit-learn

## Regresi√≥n Log√≠stica con Python y Scikit Learn

Claro, aqu√≠ tienes una gu√≠a clara y concisa para aplicar **regresi√≥n log√≠stica con Python y Scikit-Learn**, ideal para resolver problemas de **clasificaci√≥n binaria** (por ejemplo, si un cliente comprar√° o no, si un email es spam o no, etc.).

### ‚úÖ **1. Importar librer√≠as necesarias**

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
```

### ‚úÖ **2. Cargar y preparar los datos**

Para el ejemplo, usaremos el dataset de Scikit-Learn de c√°ncer de mama (binario: maligno/benigno).

```python
from sklearn.datasets import load_breast_cancer

# Cargar dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target)
```

### ‚úÖ **3. Dividir los datos en entrenamiento y prueba**

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### ‚úÖ **4. Entrenar el modelo de regresi√≥n log√≠stica**

```python
model = LogisticRegression(max_iter=10000)  # Ajustamos el n√∫mero m√°ximo de iteraciones
model.fit(X_train, y_train)
```

### ‚úÖ **5. Hacer predicciones**

```python
y_pred = model.predict(X_test)
```

### ‚úÖ **6. Evaluar el modelo**

```python
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
```

### ‚úÖ **Resultado esperado**

Deber√≠as ver un **accuracy bastante alto (\~95%-98%)**, ya que el dataset de c√°ncer de mama es limpio y bien balanceado.

### ‚úÖ ¬øQu√© sigue?

Puedes explorar m√°s:

* Normalizaci√≥n de datos (`StandardScaler`)
* Curvas ROC y AUC
* Regresi√≥n log√≠stica multiclase (`multi_class='multinomial'`)
* Regularizaci√≥n (`penalty='l1'`, `l2`, etc.)

### Resumen

#### ¬øQu√© es la regresi√≥n log√≠stica y c√≥mo se aplica en Machine Learning?

La regresi√≥n log√≠stica es una herramienta esencial dentro del √°mbito de la ciencia de datos y Machine Learning que, por su nombre, puede generar cierta confusi√≥n. A pesar de llamarse "regresi√≥n", realmente se utiliza para tareas de clasificaci√≥n. En un mundo donde los datos son el nuevo petr√≥leo, comprender la regresi√≥n log√≠stica te otorga una ventaja competitiva crucial. Es muy valorada por su capacidad para clasificar datos binarios a partir de un enfoque probabil√≠stico.

#### ¬øCu√°l es su funci√≥n en Machine Learning?

En Machine Learning, los algoritmos se dividen en varios tipos, incluyendo los supervisados, no supervisados y de refuerzo. La regresi√≥n log√≠stica pertenece a la categor√≠a de algoritmos supervisados, espec√≠ficamente en la familia de clasificaci√≥n. Su objetivo no es proporcionar un valor continuo, sino prever una clase binaria representada con 0 o 1, verdadero o falso.

#### ¬øC√≥mo funciona la funci√≥n sigmoidal?

El coraz√≥n de la regresi√≥n log√≠stica es la funci√≥n sigmoidal. Caracterizada por su forma en "S", esta funci√≥n transforma valores continuos en la probabilidad de pertenecer a una clase determinada:

- **Rango de la sigmoidal**: De 0 a 1, lo que la alinea perfectamente con los fundamentos de probabilidad.
- **Clasificaci√≥n binaria**: Si el valor resultante est√° igual o por encima de 0.5, se clasifica como 1; de lo contrario, como 0.

Este mecanismo de funcionamiento es esencial en la predicci√≥n de resultados binarios, como puede ser la aprobaci√≥n de un examen en funci√≥n de las horas de estudio dedicadas.

**Ejemplo pr√°ctico de regresi√≥n log√≠stica**

Para ilustrar este concepto, consideremos un escenario educativo. Imag√≠nate que est√°s evaluando la probabilidad de que un estudiante apruebe un examen basado en las horas de estudio:

1. **0 horas de estudio**: Es probable que no aprueben (clase 0).
2. **Mucho tiempo de estudio**: Es probable que aprueben (clase 1).

Dibujando los datos en un gr√°fico, las horas de estudio se representan como puntos que, al ser procesados por la funci√≥n sigmoidal, generan un modelo que predice si el estudiante aprobar√° o no.

#### Interpretaci√≥n de la probabilidad

La m√°xima contribuci√≥n de la regresi√≥n log√≠stica es su cualidad de interpretaci√≥n basada en probabilidades, proporcionando una perspectiva m√°s comprensible de los resultados:

- **Mayor o igual a 0.5**: El estudiante aprueba.
- **Menor a 0.5**: El estudiante no aprueba.

#### Aplicaciones y recomendaciones para el aprendizaje

Aprender a utilizar la regresi√≥n log√≠stica requiere ciertos conocimientos previos:

- **Matem√°ticas para Machine Learning**: Comprender c√≥mo las matem√°ticas se aplican a los modelos de datos.
- **An√°lisis de datos con Python y Pandas**: Habilidades en el manejo y an√°lisis de datos.
- **Visualizaci√≥n de datos con Matplotlib y Seaborn**: Facilitar la interpretaci√≥n mediante gr√°ficos.
- **Algoritmos de Machine Learning**: Bases s√≥lidas en regresi√≥n lineal y otros conceptos de aprendizaje autom√°tico.

Desarrollar una comprensi√≥n profunda y pr√°ctica de la regresi√≥n log√≠stica no solo refuerza tu formaci√≥n en ciencia de datos, sino que tambi√©n te prepara para abordar problemas reales con confianza y creatividad. ¬°Sigue explorando y creciendo en tu camino en el mundo de Machine Learning!

**Archivos de la clase**

[slides-curso-regresion-logistica.pdf](https://static.platzi.com/media/public/uploads/slides-curso-regresion-logistica_9cd293b7-6432-4524-8eb1-e94a707bb9ec.pdf)

**Lecturas recomendadas**

[Classifier Playground](http://www.ccom.ucsd.edu/~cdeotte/programs/classify.html)

## Regresi√≥n Log√≠stica con Python y Scikit Learn

¬°Perfecto! Aqu√≠ tienes un ejemplo **completo y comentado** de c√≥mo implementar una **Regresi√≥n Log√≠stica con Python y Scikit-Learn** para resolver un problema de **clasificaci√≥n binaria** paso a paso.

### üß† Objetivo

Utilizar regresi√≥n log√≠stica para predecir si una persona tiene o no diabetes, usando el conjunto de datos `Pima Indians Diabetes`.

### üì¶ 1. Instalar e importar librer√≠as necesarias

```bash
pip install pandas scikit-learn matplotlib seaborn
```

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
```

### üìä 2. Cargar el dataset

Supongamos que tienes el archivo `diabetes.csv` (puedes descargarlo desde [Kaggle](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)):

```python
df = pd.read_csv('diabetes.csv')
print(df.head())
```

### üßπ 3. Preparar los datos

Separar variables independientes (`X`) y la variable objetivo (`y`):

```python
X = df.drop('Outcome', axis=1)
y = df['Outcome']
```

### üîÄ 4. Dividir en entrenamiento y prueba

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### ü§ñ 5. Crear y entrenar el modelo

```python
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
```

### üß™ 6. Realizar predicciones

```python
y_pred = model.predict(X_test)
```

### üìà 7. Evaluar el modelo

```python
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Matriz de confusi√≥n:")
print(confusion_matrix(y_test, y_pred))
print("\nReporte de clasificaci√≥n:")
print(classification_report(y_test, y_pred))
```

### üìâ 8. Visualizar matriz de confusi√≥n (opcional)

```python
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicci√≥n")
plt.ylabel("Real")
plt.title("Matriz de Confusi√≥n")
plt.show()
```

### ‚úÖ Resultado esperado

Un `accuracy` entre **70% y 80%**, dependiendo del modelo y del dataset, es com√∫n en este problema.

### ¬øQuieres avanzar m√°s?

Puedes a√±adir:

* Regularizaci√≥n (`penalty='l1'`, `'l2'`)
* Evaluaci√≥n con curva ROC
* Balanceo de clases (`class_weight='balanced'`)
* Escalado de datos con `StandardScaler`

### Resumen

#### ¬øC√≥mo se configura el entorno y se cargan los datos en un modelo de regresi√≥n log√≠stica?

La regresi√≥n log√≠stica es una t√©cnica poderosa y vers√°til en el campo del Machine Learning, especialmente para la clasificaci√≥n de datos. El uso de Python y Scikit Learn facilita su implementaci√≥n, permiti√©ndonos abordar tareas complejas con relativa sencillez. Comenzaremos discutiendo c√≥mo configurar el entorno y cargar eficientemente los datos necesarios.

#### ¬øQu√© librer√≠as se necesitan?

Para este proyecto, necesitamos varias librer√≠as que nos ayudar√°n en distintos aspectos del proceso:

- **Scikit Learn**: Esencial para manipular datasets y aplicar regresi√≥n log√≠stica.
- **Pandas**: Para la manipulaci√≥n y an√°lisis de datos estructurados.
- **Matplotlib y Seaborn**: Para la visualizaci√≥n de datos.
- **NumPy**: Utilizado para efectuar operaciones sobre matrices y arrays.

Estas librer√≠as, ya precargadas en el entorno, nos permiten trabajar sin complicaciones. El dataset espec√≠fico que usaremos son im√°genes de d√≠gitos escritos a mano, disponibles mediante `LogDigit` desde `Scikit Learn.dataset`.

#### ¬øC√≥mo cargamos los datos?

Iniciamos cargando los datos en un objeto llamado Digits:

```python
from sklearn.datasets import load_digits
digits = load_digits()
```

El objeto `Digits` contiene varias propiedades relevantes, incluyendo los datos (`data`), los nombres de las columnas o caracter√≠sticas (`feature_names`), y una variable Target, que indica qu√© d√≠gito est√° representado en cada imagen.

#### ¬øC√≥mo se visualizan los datos?

Para ver de forma m√°s clara estas im√°genes de d√≠gitos, hacemos uso de NumPy para reestructurarlas en un formato de 8x8, que es la estructura documentada en el dataset original.

```python
import numpy as np

image = np.reshape(digits.data[0], (8, 8))
```

Podemos visualizar la imagen utilizando Matplotlib:

```python
import matplotlib.pyplot as plt

plt.imshow(image, cmap='gray')
plt.show()
```

Esta visualizaci√≥n nos permite entender mejor los datos que estamos manipulando, ofreciendo una base firme para el aprendizaje del modelo.

#### ¬øC√≥mo dividir los datos en entrenamiento y prueba?

Dividir adecuadamente nuestros datos entre conjuntos de entrenamiento y prueba es crucial para validar y evaluar el desempe√±o de nuestro modelo. Esta responsabilidad no solo sustenta los resultados obtenidos, sino que asegura la fiabilidad del algoritmo ante datos no vistos previamente.

#### ¬øPor qu√© es importante esta divisi√≥n?

La separaci√≥n de los datos en entrenamiento y prueba permite:

- Asegurar que nuestro modelo no est√° "aprendiendo de memoria" el dataset completo.
- Validar el modelo con datos que no ha visto antes, permiti√©ndonos evaluar su precisi√≥n de forma objetiva.

#### ¬øC√≥mo hacemos el split de datos?

La funci√≥n `train_test_split` de Scikit Learn se utiliza para dividir los datos:

from sklearn.model_selection import train_test_split

```python
x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=0)
```

Aqu√≠, `test_size=0.2` indica que el 20% del dataset se utilizar√° para pruebas. El `random_state` asegura que la divisi√≥n sea reproducible en futuras ejecuciones.

#### ¬øC√≥mo se entrena y eval√∫a un modelo de regresi√≥n log√≠stica?

Una vez que los datos est√°n listos y divididos, el siguiente paso es entrenar el modelo de regresi√≥n log√≠stica. Aqu√≠, se destacar√° c√≥mo configurar un modelo, entrenarlo, predecir resultados, y finalmente, evaluar su rendimiento.

#### ¬øC√≥mo configurar y entrenar el modelo?

La configuraci√≥n y entrenamiento del modelo son extremadamente sencillos:

```python
from sklearn.linear_model import LogisticRegression

logistic_reg = LogisticRegression(max_iter=200)
logistic_reg.fit(x_train, y_train)
```

La funci√≥n `fit` entrena el modelo utilizando el conjunto de entrenamiento.

#### ¬øC√≥mo se realizan predicciones?

Con el modelo entrenado, podemos obtener predicciones en el conjunto de prueba:

`predictions = logistic_reg.predict(x_test)`

Estas predicciones nos permitir√°n evaluar el desempe√±o del modelo compar√°ndolas con los valores reales de `y_test`.

#### ¬øC√≥mo evaluamos el modelo?

Para evaluar la efectividad del modelo, utilizamos una matriz de confusi√≥n:

```python
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, predictions)
```

Y para visualizarlo:

```python
import seaborn as sns

plt.figure(figsize=(9, 9))
sns.heatmap(cm, annot=True, linewidths=0.5, square=True, cmap='coolwarm')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')
plt.show()
```

Esta matriz permite identificar mayormente los aciertos y errores del modelo; los valores en la diagonal indican el n√∫mero de predicciones correctas.

Explorar la regresi√≥n log√≠stica utilizando Python y Scikit Learn es un excelente punto de partida para adentrarse en el mundo del machine learning. La simplicidad del c√≥digo y la precisi√≥n en la clasificaci√≥n demuestran la efectividad de esta t√©cnica. Invito a seguir indagando y practicando con modelos m√°s complejos, siguiendo este curso o explorando otros datasets y algoritmos. ¬°El aprendizaje nunca termina!

**Archivos de la clase**

[mi-primera-regresion-logistica.ipynb](https://static.platzi.com/media/public/uploads/mi_primera_regresion_logistica_cc427d4e-ac0d-4f86-b38d-dee909ec9aa2.ipynb)

**Lecturas recomendadas**

[MNIST classification using multinomial logistic + L1 ‚Äî scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html)

[Recognizing hand-written digits ‚Äî scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html)

[The Digit Dataset ‚Äî scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html)

[Mi_primera_regresion_logistica.ipynb - Google Drive](https://drive.google.com/file/d/1HQ8jnYgJsXScPo6TJ8vS_6SMd52hEFzh/view?usp=sharing)

## Cu√°ndo usar la regresi√≥n log√≠stica en modelos de clasificaci√≥n

La **regresi√≥n log√≠stica** se usa cuando tienes un problema de **clasificaci√≥n**, es decir, cuando la variable que quieres predecir (variable dependiente) **es categ√≥rica**, como:

* S√≠ o No
* 0 o 1
* Aprobado o Reprobado
* Enfermo o Sano

### ‚úÖ **Cu√°ndo usar regresi√≥n log√≠stica**

1. **Cuando el objetivo es clasificar en dos clases (binaria):**

   * Ej: Predecir si un correo es *spam* o *no spam*.

2. **Cuando la relaci√≥n entre las variables independientes (X) y la probabilidad del evento se puede modelar como una curva sigmoide.**

3. **Cuando los valores de salida deben ser interpretables como probabilidades.**

   * Por ejemplo, la probabilidad de que un paciente tenga una enfermedad.

4. **Cuando se necesita un modelo sencillo, eficiente y r√°pido de entrenar.**

### üî¢ Tipos de regresi√≥n log√≠stica

* **Binaria:** Solo dos clases (0 o 1).
* **Multinomial:** Tres o m√°s clases sin orden.
* **Ordinal:** Tres o m√°s clases con orden (p. ej., *bajo, medio, alto*).

### ‚ö†Ô∏è No usar regresi√≥n log√≠stica cuando...

* La variable objetivo es **continua** (usa regresi√≥n lineal u otro modelo).
* Hay relaciones **altamente no lineales** que no pueden ser bien modeladas con una transformaci√≥n log√≠stica (en ese caso, modelos como Random Forest, SVM o redes neuronales pueden funcionar mejor).

### Resumen

#### ¬øCu√°ndo usar la regresi√≥n log√≠stica?

La regresi√≥n log√≠stica es una herramienta poderosa para tareas de clasificaci√≥n y es crucial entender cu√°ndo es apropiado utilizarla. Con su f√°cil implementaci√≥n y la capacidad de interpretar coeficientes, es una opci√≥n valiosa en el arsenal de modelos de aprendizaje autom√°tico. A continuaci√≥n, descubriremos las ventajas, limitaciones y momentos m√°s adecuados para aplicar este algoritmo.

#### ¬øCu√°les son las ventajas de la regresi√≥n log√≠stica?

Este modelo presenta diferentes beneficios que lo convierten en una opci√≥n atractiva:

- **Facilidad de implementaci√≥n**: Como vimos anteriormente, se puede entrenar un modelo de regresi√≥n log√≠stica con solo unas pocas l√≠neas de c√≥digo.
- **Coeficientes interpretables**: Al igual que en la regresi√≥n lineal, los resultados que arroja el modelo son comprensibles y se pueden traducir a la realidad.
- **Inferencia de caracter√≠sticas**: Permite identificar cu√°n influyentes son las diferentes caracter√≠sticas en el resultado final de la clasificaci√≥n.
- **Clasificaciones con niveles de certeza**: No solo indica si el resultado es 0 o 1, sino que aporta un porcentaje de seguridad en dicha clasificaci√≥n.
- **Excelentes resultados con dataset linealmente separables**: Funciona √≥ptimamente cuando las variables tienen un comportamiento lineal.

#### ¬øQu√© limitaciones tiene la regresi√≥n log√≠stica?

A pesar de sus numerosas ventajas, la regresi√≥n log√≠stica tambi√©n tiene ciertas limitaciones:

- **Asume linealidad**: Supone que existe una relaci√≥n lineal entre las variables dependientes, lo cual no siempre ocurre en la pr√°ctica.
- **Overfitting en alta dimensionalidad**: Posee tendencia al overfitting cuando se enfrenta a datasets con muchas caracter√≠sticas.
- **Problemas con la multicolinearidad**: La presencia de caracter√≠sticas altamente correlacionadas puede afectar negativamente el rendimiento del modelo.
- **Requiere datasets grandes para mejores resultados**: Los datasets peque√±os pueden no proporcionar la cantidad suficiente de informaci√≥n para un modelo preciso.

#### ¬øCu√°ndo es ideal utilizar la regresi√≥n log√≠stica?

Este modelo es particularmente √∫til en las siguientes situaciones:

- Cuando se buscan soluciones sencillas y r√°pidas.
- Para estimar probabilidades de ocurrencia de un evento (clasificaci√≥n binaria).
- En datasets que son linealmente separables y tienen grandes vol√∫menes de datos.
- Ideal si el dataset est√° balanceado, con proporciones similares de las clases a estudiar.

#### ¬øPor qu√© no utilizar la regresi√≥n lineal para clasificaci√≥n?

Mientras que la regresi√≥n lineal pretende encontrar una recta que explique el comportamiento de los datos de forma continua, para datos que necesitan clasificaciones de verdaderos y falsos, este no es el caso. Al trazar una l√≠nea recta, podr√≠a no discernir adecuadamente entre las clases que se solapan, lo que llevar√≠a a un mal desempe√±o. La regresi√≥n log√≠stica, en cambio, transforma la l√≠nea recta en una sigmoide que permite mejorar la clasificaci√≥n al gestionar probabilidades, sirviendo as√≠ a su prop√≥sito de categorizaci√≥n.

La regresi√≥n log√≠stica surge como un recurso altamente valioso cuando se busca la clasificaci√≥n con certeza y simplicidad. Con sus ventajas y desventajas claramente delineadas, es crucial saber cu√°ndo elegir y aplicar este m√©todo para obtener los resultados deseados. ¬°Sigue investigando y ampliando tu conocimiento en esta fascinante √°rea!

## Regresi√≥n Log√≠stica: F√≥rmula y Aplicaci√≥n en Python

¬°Claro, Mario! Vamos a ver la **f√≥rmula de la Regresi√≥n Log√≠stica** y c√≥mo aplicarla en **Python usando Scikit-learn**. Este modelo es muy utilizado para **clasificaci√≥n binaria** (por ejemplo, predecir si un correo es *spam* o *no spam*).

### üìå F√≥rmula de la Regresi√≥n Log√≠stica

La regresi√≥n log√≠stica modela la **probabilidad** de que un ejemplo pertenezca a una clase (por ejemplo, clase 1):

$$
P(y = 1 \mid x) = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

Donde:

* $z = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n$
* $\sigma(z)$: funci√≥n sigmoide
* $w_0$: intercepto (bias)
* $w_i$: pesos de los atributos $x_i$

### üß™ Ejemplo en Python con Scikit-learn

Supongamos que queremos predecir si un estudiante pasar√° un examen bas√°ndonos en sus horas de estudio.

### ‚úÖ Paso 1: Importar librer√≠as

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
```

### ‚úÖ Paso 2: Crear un dataset simple

```python
# Horas de estudio y resultado (1=aprobado, 0=reprobado)
data = {
    'horas_estudio': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'resultado': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]
}
df = pd.DataFrame(data)
X = df[['horas_estudio']]
y = df['resultado']
```

### ‚úÖ Paso 3: Dividir datos y entrenar modelo

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

modelo = LogisticRegression()
modelo.fit(X_train, y_train)
```

### ‚úÖ Paso 4: Predecir y evaluar

```python
y_pred = modelo.predict(X_test)

print("Reporte de clasificaci√≥n:\n", classification_report(y_test, y_pred))
print("Matriz de confusi√≥n:\n", confusion_matrix(y_test, y_pred))
```

### ‚úÖ Paso 5: Predecir probabilidad

```python
nuevas_horas = np.array([[4.5]])
probabilidad = modelo.predict_proba(nuevas_horas)
print(f"Probabilidad de aprobar con 4.5 horas: {probabilidad[0][1]:.2f}")
```

### Resumen

¬øC√≥mo funciona la f√≥rmula de la regresi√≥n log√≠stica?
La regresi√≥n log√≠stica es un algoritmo crucial para la clasificaci√≥n de datos, permiti√©ndonos predecir la probabilidad de un evento binario, como "s√≠" o "no", "verdadero" o "falso", "positivo" o "negativo". Para lograrlo, utilizamos la funci√≥n sigmoide. Esta funci√≥n, representada por la f√≥rmula ( P = \frac{1}{1 + e^{-\zeta}} ), convierte cualquier valor en una probabilidad comprendida entre 0 y 1. Pero, ¬øc√≥mo se lleva a cabo este proceso y cu√°l es la base matem√°tica detr√°s de esta operaci√≥n?

#### ¬øQu√© es la funci√≥n sigmoide?

La funci√≥n sigmoide es una funci√≥n matem√°tica que transforma cualquier valor real en un valor comprendido entre 0 y 1, adquiriendo una forma de "S" al graficarse. Esta funci√≥n es particularmente √∫til en regresi√≥n log√≠stica, pues nos permite trabajar con probabilidades:

```python
import numpy as np
import matplotlib.pyplot as plt

# Definir una funci√≥n sigmoide
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Crear un rango de datos entre -10 y 10
z = np.linspace(-10, 10, 100)

# Calcular la funci√≥n sigmoide
sigmoid_values = sigmoid(z)

# Graficar la funci√≥n
plt.plot(z, sigmoid_values)
plt.title('Funci√≥n Sigmoide')
plt.xlabel('z')
plt.ylabel('Sigmoid(z)')
plt.grid(True)
plt.show()
```

Al aplicar la funci√≥n sigmoide, cualquier dato recibido, sin importar su magnitud, se transformar√° en un valor entre 0 y 1, ideal para representar probabilidades y hacer predicciones.

#### ¬øC√≥mo los "odds" y los "log odds" contribuyen a la regresi√≥n log√≠stica?

Un concepto fundamental en regresi√≥n log√≠stica es el de los "odds", que expresan la probabilidad del √©xito de un evento sobre la probabilidad de su fracaso. Por ejemplo, si tenemos una probabilidad de √©xito de 80%, los "odds" ser√≠an:

[ \text{odds} = \frac{0.80}{1 - 0.80} = 4 ]

Los "log odds" se emplean para manejar mejor los infinitos, ya que al aplicar el logaritmo natural a los "odds", toda la informaci√≥n se centra alrededor del cero, permitiendo a los algoritmos procesar estos valores de forma m√°s efectiva:

[ \text{log odds} = \ln(\text{odds}) ]

#### ¬øCu√°l es la relaci√≥n entre la regresi√≥n lineal y la regresi√≥n log√≠stica?

La regresi√≥n log√≠stica se basa en las mismas premisas que la regresi√≥n lineal, aunque con un objetivo diferente: predecir una probabilidad en lugar de un valor continuo. Utilizamos una f√≥rmula similar a la de la regresi√≥n lineal:

[ \beta_0 + \beta_1 \cdot x ]

Aqu√≠, (\beta_0) representa el intercepto y (\beta_1) la pendiente. En regresi√≥n log√≠stica, este modelo lineal se introduce en la funci√≥n sigmoide para obtener probabilidades.

Para ilustrar c√≥mo estas piezas se integran, veamos c√≥mo se transforma la f√≥rmula de la regresi√≥n lineal en una f√≥rmula de regresi√≥n log√≠stica:

[ P = \frac{e^{\beta_0 + \beta_1 \cdot x}}{1 + e^{\beta_0 + \beta_1 \cdot x}} ]

#### ¬øPor qu√© es √∫til la regresi√≥n log√≠stica?

La regresi√≥n log√≠stica permite abordar problemas de clasificaci√≥n binaria de manera eficiente y precisa. Al convertir valores continuos en probabilidades, facilita la toma de decisiones basada en datos. Esta capacidad de asignar una probabilidad a cada caso nos permite clasificar con certeza eventos como un diagn√≥stico m√©dico, la aprobaci√≥n de un cr√©dito, o el resultado de un partido deportivo.

A medida que ampl√≠es tus conocimientos en machine learning, descubrir√°s que la regresi√≥n log√≠stica es solo la punta del iceberg. Existen numerosos algoritmos y m√©todos para abordar problemas de clasificaci√≥n y predicci√≥n. Sin embargo, entender las bases de la regresi√≥n log√≠stica te brindar√° una ventaja significativa en el mundo del an√°lisis de datos. ¬°Sigue explorando y construyendo habilidades valiosas en este campo!

**Lecturas recomendadas**

[Classifier Playground](http://www.ccom.ucsd.edu/~cdeotte/programs/classify.html)

## Regresi√≥n Log√≠stica Aplicada a Dataset Binomial de Churn

La **Regresi√≥n Log√≠stica aplicada a un dataset binomial de Churn** (abandono de clientes) es una t√©cnica muy com√∫n en an√°lisis de datos para predecir si un cliente **se quedar√° (0)** o **se ir√° (1)**, usando variables como edad, ingresos, uso del servicio, etc.

Aqu√≠ tienes una gu√≠a clara y concisa con **explicaci√≥n + c√≥digo en Python con Scikit-Learn**:

### ‚úÖ 1. ¬øQu√© es la regresi√≥n log√≠stica?

Es un modelo de clasificaci√≥n supervisada usado cuando el **output es binario** (por ejemplo: `0` = se queda, `1` = se va).

La f√≥rmula general es:

$$
P(y = 1 | X) = \frac{1}{1 + e^{-(w_0 + w_1x_1 + w_2x_2 + \cdots + w_nx_n)}}
$$

### üìä 2. Dataset de Churn (ejemplo simulado)

Sup√≥n que tienes un dataset `churn.csv` con columnas como:

* `edad`
* `ingresos`
* `uso_mensual`
* `tiempo_en_meses`
* `churn` (0 = se queda, 1 = se va)

### üß™ 3. Aplicaci√≥n en Python

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# 1. Cargar el dataset
df = pd.read_csv("churn.csv")

# 2. Separar caracter√≠sticas y etiqueta
X = df[["edad", "ingresos", "uso_mensual", "tiempo_en_meses"]]
y = df["churn"]

# 3. Dividir en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Modelo de regresi√≥n log√≠stica
modelo = LogisticRegression()
modelo.fit(X_train, y_train)

# 5. Predicciones y evaluaci√≥n
y_pred = modelo.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
```

### üìà 4. ¬øC√≥mo interpretar?

* **Confusion Matrix**: Muestra los verdaderos positivos, negativos, falsos positivos y negativos.
* **Precisi√≥n / Recall / F1-Score**: Evaluaci√≥n de la calidad del modelo.

### Resumen

#### ¬øC√≥mo aplicar la regresi√≥n log√≠stica desde cero?

La regresi√≥n log√≠stica es una poderosa herramienta dentro del aprendizaje autom√°tico y la inteligencia artificial utilizada principalmente para problemas de clasificaci√≥n. Este proceso, que empieza desde la preparaci√≥n de los datos hasta la implementaci√≥n del modelo, es fundamental para obtener resultados precisos y confiables. Descubramos c√≥mo aplicar la regresi√≥n log√≠stica en un proyecto desde cero.

#### ¬øQu√© es la regresi√≥n log√≠stica y c√≥mo se clasifica?

La regresi√≥n log√≠stica es un tipo de modelo estad√≠stico que se utiliza para predecir resultados binarios en una muestra de datos. A este tipo de problemas se les llama com√∫nmente "dataset binomiales". Un ejemplo cl√°sico es predecir si un cliente de una compa√±√≠a har√° "churn" (es decir, cancelar√° su suscripci√≥n) o no. En general, la regresi√≥n log√≠stica se especializa en:

- **Datasets binomiales**: con solo dos resultados posibles (0 o 1, verdadero o falso, s√≠ o no).
- **Datasets multinomiales**: con m√°s de dos posibles clasificaciones, aunque la especialidad de la regresi√≥n log√≠stica es con datasets binomiales.

#### ¬øC√≥mo preparar los datos efectivamente?

Una parte cr√≠tica del proyecto es la preparaci√≥n de los datos. Un buen procesamiento te ayudar√° a obtener resultados m√°s precisos y eficientes. Aqu√≠ te presento los pasos esenciales del proceso:

1. **Eliminar duplicados** y procesar valores nulos para evitar sesgos en el modelo.
2. **Remover columnas innecesarias** que no aporten valor a la clasificaci√≥n.
3. **Convertir datos categ√≥ricos en num√©ricos**, ya que los algoritmos de machine learning funcionan mejor con n√∫meros.
4. **Escalar los datos** para facilitar el manejo del algoritmo.

### ¬øQu√© dataset se utiliza para este proyecto?

Para este proyecto, se utiliza un dataset de "churn" de Kaggle, que se relaciona con el evento en el que un cliente da de baja los servicios de una compa√±√≠a. Las caracter√≠sticas del dataset incluyen:

- **Servicios contratados**: como tel√©fono, l√≠nea de internet, seguridad online, etc.
- **Informaci√≥n del cliente**: tipo de contrato, m√©todo de pago, facturaci√≥n, etc.
- **Datos demogr√°ficos**: g√©nero, edad, rango salarial, entre otros.

#### ¬øC√≥mo implementar la limpieza y transformaci√≥n de datos en Python?

A continuaci√≥n, se presenta un extracto del c√≥digo en Python necesario para la preparaci√≥n de datos usando librer√≠as comunes como Pandas y NumPy:

```python
# Importar librer√≠as necesarias
import pandas as pd
import numpy as np

# Cargar los datos
df_data = pd.read_csv('ruta/al/dataset.csv')

# Verificar y transformar columnas num√©ricas
df_data['TotalCharges'] = pd.to_numeric(df_data['TotalCharges'], errors='coerce')

# Manejar valores nulos
df_data.dropna(inplace=True)

# Eliminar columnas innecesarias
df_data.drop('customerID', axis=1, inplace=True)

# Convertir la variable objetivo a num√©rica
df_data['Churn'] = df_data['Churn'].replace({'Yes': 1, 'No': 0})

# Aplicar One-Hot Encoding a variables categ√≥ricas
df_data = pd.get_dummies(df_data)
```

#### ¬øQu√© sigue despu√©s de la limpieza de datos?

Despu√©s de la limpieza y transformaci√≥n inicial de los datos, el siguiente paso es lidiar con la multicolinealidad y escalar los datos. Estos pasos son cruciales para asegurar que el modelo de regresi√≥n log√≠stica funcione de manera coherente y con mayor precisi√≥n.

Este enfoque met√≥dico asegura resultados s√≥lidos en cualquier proyecto de aprendizaje autom√°tico. ¬°Sigue aprendiendo y profundizando en cada paso de este proceso! Explorando y convirtiendo datos a su forma m√°s conducente para los algoritmos, establecer√°s una base robusta para posteriores an√°lisis y modelos predictivos.

**Archivos de la clase**

[regresion-logistica-binomial.ipynb](https://static.platzi.com/media/public/uploads/regresion_logistica_binomial_87729390-4a2c-4332-9fee-d8f5397f550c.ipynb)

**Lecturas recomendadas**

[Telco Customer Churn | Kaggle](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)

[regresion_logistica_binomial.ipynb - Google Drive](https://drive.google.com/file/d/1q7QYevfV-hfGPaiSFnAdxAUbxwhvSmIG/view?usp=sharing)