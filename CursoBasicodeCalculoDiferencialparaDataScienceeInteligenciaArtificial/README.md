# Curso B√°sico de C√°lculo Diferencial para Data Science e Inteligencia Artificial

## ¬øQu√© es el c√°lculo diferencial?

El **c√°lculo diferencial** es una rama de las matem√°ticas que se enfoca en el **estudio de c√≥mo cambian las funciones**. Su objetivo principal es **entender y calcular la tasa de cambio** de una cantidad con respecto a otra.

### üîç ¬øQu√© estudia el c√°lculo diferencial?

1. **Derivadas**:
   La derivada de una funci√≥n en un punto mide **la pendiente** o la **tasa de cambio instant√°nea** de la funci√≥n en ese punto.
   Por ejemplo, si una funci√≥n representa la posici√≥n de un objeto, su derivada representa la **velocidad**.

2. **L√≠mites**:
   El concepto de **l√≠mite** es fundamental para definir formalmente qu√© es una derivada.

### üìê Aplicaciones del c√°lculo diferencial:

* En f√≠sica: para analizar **movimiento, velocidad y aceleraci√≥n**.
* En econom√≠a: para estudiar **costos marginales y tasas de crecimiento**.
* En ingenier√≠a: para **optimizar dise√±os** o analizar sistemas din√°micos.
* En machine learning: para **ajustar modelos** mediante el c√°lculo del gradiente.

### üß† Ejemplo b√°sico:

Si tienes una funci√≥n:

$$
f(x) = x^2
$$

Su derivada es:

$$
f'(x) = 2x
$$

Esto significa que en $x = 3$, la tasa de cambio de la funci√≥n es $2 \times 3 = 6$.

### Resumen

Antes de estudiar el c√°lculo diferencial es necesario comprender: ¬øQu√© es el c√°lculo?, ¬øPara qu√© nos sirve?, ¬øCu√°l es el prop√≥sito? Empecemos por definiciones gen√©ricas.

**Conceptos generales de C√°lculo diferencial e integral**

#### C√°lculo:

Es realizar operaciones de manera dada para llegar a un resultado.

#### C√°lculo diferencial:

Parte del c√°lculo infinitesimal (que estudia las funciones cuando tienen cambios muy peque√±os, cercanos a cero) y del an√°lisis matem√°tico que estudia c√≥mo cambian las funciones continuas cuando sus variables sufren cambios infinitesimales. El principal objeto de estudio en el c√°lculo diferencial es la **derivada** (o raz√≥n de cambio infinitesimal). Un ejemplo de esto es el c√°lculo de la velocidad instant√°nea de un objeto en movimiento.

#### C√°lculo integral:

Estudio de la anti derivaci√≥n, es decir, la operaci√≥n inversa a la derivada. Busca reconstruir funciones a partir de su raz√≥n de cambio. El c√°lculo integral est√° fuera del alcance de este curso.

## ¬øQu√© es un l√≠mite?

Un **l√≠mite** en matem√°ticas describe el **comportamiento de una funci√≥n o secuencia** a medida que sus entradas se **acercan a un valor espec√≠fico**.

### üìå Definici√≥n b√°sica:

El **l√≠mite de una funci√≥n** $f(x)$ cuando $x$ se acerca a un valor $a$, se escribe como:

$$
\lim_{x \to a} f(x)
$$

Esto significa:

> "¬øQu√© valor se **acerca** la funci√≥n $f(x)$ cuando $x$ se **aproxima a $a$**?"

### üß† Ejemplo sencillo:

Sup√≥n que tienes:

$$
f(x) = 2x
$$

Entonces:

$$
\lim_{x \to 3} 2x = 6
$$

Porque cuando te acercas a $x = 3$, $f(x) = 2x$ se acerca a 6.

### ‚ö†Ô∏è Importancia de los l√≠mites:

* Permiten **definir la derivada**, que es el n√∫cleo del c√°lculo diferencial.
* Ayudan a **analizar funciones en puntos problem√°ticos**, como donde no est√°n definidas exactamente.
* Son fundamentales en el estudio de **continuidad** y **an√°lisis matem√°tico**.

### üßÆ ¬øY si el valor no existe?

Por ejemplo:

$$
\lim_{x \to 0} \frac{1}{x}
$$

Este **no tiene l√≠mite finito**, porque a medida que $x$ se acerca a 0, el valor **crece infinitamente** (positivo o negativo, dependiendo del lado).

### Resumen

Un l√≠mite es un an√°lisis de tendencia respecto a un punto. Es decir, hacia donde va el valor de nuestra funci√≥n a medida que ‚Äú**x**‚Äù se acerca al valor ‚Äú**a**‚Äù, pero sin tocar nunca ese valor ‚Äú**a**‚Äù. Por eso se llama l√≠mite, porque ‚Äútenemos prohibido tocar‚Äù ese valor ‚Äú**a**‚Äù.

![F√≥rmula de un l√≠mite](images/formulalimita.jpg)

Una analog√≠a para entender el l√≠mite es la siguiente: sup√≥n que tienes una fogata encendida frente a t√≠, y quieres comprobar que el fuego que est√°s viendo existe, que tus ojos no te enga√±an. Para ello, acercas las manos al fuego, cada vez m√°s despacio, a ver si las sientes m√°s calientes, pero nunca llegas a estar directamente en el fuego porque te puedes quemar.

## Resoluci√≥n de l√≠mites e interpretaci√≥n

¬°Perfecto! Vamos a ver c√≥mo **resolver l√≠mites paso a paso** y c√≥mo **interpretarlos** gr√°ficamente y conceptualmente.

### üßÆ **Ejemplo 1: L√≠mite directo**

### Problema:

$$
\lim_{x \to 2} (3x + 1)
$$

### ‚úèÔ∏è Resoluci√≥n:

Este es un l√≠mite simple. Sustituimos directamente el valor de $x$:

$$
\lim_{x \to 2} (3x + 1) = 3(2) + 1 = 7
$$

### ‚úÖ Interpretaci√≥n:

A medida que $x$ se **acerca a 2**, el valor de $3x + 1$ se **acerca a 7**. No hay discontinuidades ni problemas, porque la funci√≥n es lineal y continua.

### üßÆ **Ejemplo 2: L√≠mite con indeterminaci√≥n**

### Problema:

$$
\lim_{x \to 1} \frac{x^2 - 1}{x - 1}
$$

### üîç Paso 1: Sustituimos directamente

$$
\frac{1^2 - 1}{1 - 1} = \frac{0}{0}
$$

Esto es una **indeterminaci√≥n**, as√≠ que tenemos que **simplificar**.

### üîß Paso 2: Factorizamos el numerador

$$
\frac{x^2 - 1}{x - 1} = \frac{(x - 1)(x + 1)}{x - 1}
$$

Cancelamos el t√©rmino com√∫n:

$$
= x + 1 \quad \text{(cuando } x \ne 1\text{)}
$$

### ‚úÖ Resultado del l√≠mite:

$$
\lim_{x \to 1} \frac{x^2 - 1}{x - 1} = \lim_{x \to 1} (x + 1) = 2
$$

### üìä Interpretaci√≥n gr√°fica

En el ejemplo anterior, la funci√≥n original tiene una **discontinuidad evitable** en $x = 1$, pero **el l√≠mite existe** porque los valores de la funci√≥n se acercan a 2 desde ambos lados.

### üìò Ejemplo 3: L√≠mite infinito

$$
\lim_{x \to 0^+} \frac{1}{x}
$$

Cuando $x \to 0$ **desde la derecha** (valores positivos muy peque√±os), la funci√≥n:

$$
\frac{1}{x} \to +\infty
$$

### ‚úÖ Interpretaci√≥n:

El l√≠mite **no existe como n√∫mero finito**, pero decimos que **tiende a infinito**.

### üîö Resumen

| Tipo de l√≠mite         | Qu√© hacer                                    |
| ---------------------- | -------------------------------------------- |
| Sustituci√≥n directa    | Solo eval√∫a normalmente                      |
| Indeterminaci√≥n 0/0    | Factoriza, racionaliza o usa L'H√¥pital       |
| Infinito o no definido | Analiza comportamiento a izquierda y derecha |
| L√≠mite lateral         | Usa $x \to a^-$ o $x \to a^+$                |

### Resumen

Para resolver un [l√≠mite](https://platzi.com/clases/2726-calculo-diferencial-ds/46053-que-es-un-limite/ "l√≠mite") veamos el concepto de l√≠mites laterales. Un l√≠mite lateral es cuando nos acercamos a un valor por un lado, ya sea por la izquierda o la derecha. Los l√≠mites laterales se denotan de la siguiente manera.

![limite lateral izquierda.PNG](images/limitelateralizquierda.jpg)

Esto es un l√≠mite lateral a la izquierda. Un l√≠mite lateral a la derecha se denota del mismo modo, excepto que elevamos la constante ‚Äúa‚Äù al signo positivo.

![limite lateral derecha](images/limitelateralderecha.jpg)

Los l√≠mites laterales son importantes, ya que para que el [l√≠mite](https://platzi.com/clases/2726-calculo-diferencial-ds/46053-que-es-un-limite/ "l√≠mite") en general exista, los dos l√≠mites laterales deben ser iguales.

![limite general](images/limitegeneral.jpg)

#### Ejercicio de l√≠mites e interpretaci√≥n

Resolvamos el siguiente l√≠mite

![ejercicio de limite](images/ejerciciodelimite.jpg)

Si nos damos cuenta, la funci√≥n no est√° definida en dos, porque al hacer `x=2` nos queda una divisi√≥n entre cero. **Para resolver l√≠mites, debemos recurrir a trucos** como la factorizaci√≥n en este caso. F√≠jate que el numerador es una diferencia de cuadrados, por lo que podemos reescribir el l√≠mite como.

![factorizacion de limite](images/factorizaciondelimite.jpg)

En este caso, podemos simplemente evaluar el l√≠mite y nos queda que es igual a cuatro. Esto quiere decir que a medida que nos vamos acercando a dos, la funci√≥n se aproxima a cuatro. Pero recuerda, **la funci√≥n no est√° definida en dos**.

![Vista gr√°fica del l√≠mite](images/Vistagraficadellimite.jpg)

## Definici√≥n de la derivada

La **derivada** de una funci√≥n mide c√≥mo cambia su valor en respuesta a peque√±os cambios en su variable independiente. Esencialmente, responde a la pregunta:

> **¬øQu√© tan r√°pido cambia una funci√≥n en un punto?**

### üìå **Definici√≥n formal (con l√≠mites)**

La derivada de una funci√≥n $f(x)$ en un punto $x = a$ se define como:

$$
f'(a) = \lim_{h \to 0} \frac{f(a+h) - f(a)}{h}
$$

üîç Esto representa la **pendiente de la recta tangente** a la curva de $f(x)$ en el punto $x = a$.

### üìâ Interpretaci√≥n gr√°fica

* Si la derivada es **positiva**, la funci√≥n **sube** en ese punto.
* Si la derivada es **negativa**, la funci√≥n **baja** en ese punto.
* Si la derivada es **cero**, la funci√≥n tiene un **punto cr√≠tico** (puede ser un m√°ximo, m√≠nimo o un punto de inflexi√≥n).

### üìò Ejemplo

Sea $f(x) = x^2$. Entonces:

$$
f'(x) = \lim_{h \to 0} \frac{(x + h)^2 - x^2}{h}
= \lim_{h \to 0} \frac{x^2 + 2xh + h^2 - x^2}{h}
= \lim_{h \to 0} \frac{2xh + h^2}{h}
= \lim_{h \to 0} (2x + h) = 2x
$$

‚úÖ Por lo tanto, la derivada de $f(x) = x^2$ es $f'(x) = 2x$.

### Resumen

El problema de la derivada nace de tratar de **encontrar la recta tangente a un punto en una curva**. La primera soluci√≥n (por parte de Isaac Newton) fu√© tomar una recta secante, es decir una recta que corta a la curva en dos puntos.

**Encontrando la pendiende de la recta tangente**

![Recta secante a una curva y su pendiente](images/Rectasecanteaunacurvaysupendiente.jpg)

![dereivada](images/captura-de-pantalla-2022-02-10-a-las-5-10-43-p-m_eddea120-37a6-4af3-b3ae-d0912e4e765b.jpg)

De la imagen anterior nos damos cuenta que estos dos puntos est√°n dados por ![1.PNG](images/1.jpg) y ![2.PNG](images/2.jpg) donde ‚Äúh‚Äù es la distancia horizontal entre dichos puntos. Mediante estos dos puntos, podemos calcular la pendiente de la recta secante con la f√≥rmula ![3.PNG](images/3.jpg), donde m es la pendiente, ![4.PNG](images/4.jpg) y ![5.PNG](images/5.jpg) corresponden a las coordenadas de ![p1.PNG](images/p1.jpg), y ![x1.PNG](images/x1.jpg) y ![y1.PNG](images/y1.jpg) a las coordenadas de ![p1.PNG](images/p1.jpg).

Sin embargo, esto nos da la pendiente de la recta secante. Queremos encontrar la de la recta tangente. Para ello, debemos recortar la distancia ‚Äú**h**‚Äù, hasta que sea muy cercana a cero. Entonces tomamos el l√≠mite ![9.PNG](images/9.jpg), y esto nos da la pendiente de la recta tangente en un punto x de la curva. A este l√≠mite es lo que llamamos **derivada**.

La **derivada** tambi√©n se puede ver en t√©rminos de incrementos. El numerador ser√≠a el incremento ![7y.PNG](images/7y.jpg) entre las funciones o tambi√©n el valor de y, mientras que el denominador ser√≠a el incremento ![8.PNG](images/8x.jpg) entre los valores de x.

Recuerda que el objetivo del curso no es hacer c√°lculo de la manera tradicional (a l√°piz y papel), si no **entender los fundamentos matem√°ticos que se aplican en distintos algoritmos de inteligencia artificial**.

## La derivada como raz√≥n de cambio

La **derivada como raz√≥n de cambio** se refiere a c√≥mo var√≠a una cantidad respecto a otra. Es una de las interpretaciones m√°s √∫tiles y comunes de la derivada, especialmente en f√≠sica, econom√≠a y ciencias aplicadas.

### üìå Definici√≥n intuitiva

Si tienes una funci√≥n $y = f(x)$, la derivada $f'(x)$ representa la **raz√≥n de cambio instant√°nea** de $y$ con respecto a $x$. Es decir:

$$
f'(x) = \frac{dy}{dx}
$$

Significa cu√°nto cambia $y$ por cada peque√±a unidad de cambio en $x$.

### üìä Ejemplos pr√°cticos

#### 1. **Velocidad en f√≠sica**

Si $s(t)$ es la posici√≥n de un objeto en funci√≥n del tiempo $t$, entonces:

$$
s'(t) = \text{velocidad instant√°nea}
$$

Es decir, cu√°n r√°pido cambia la posici√≥n en un momento espec√≠fico.

#### 2. **Crecimiento poblacional**

Si $P(t)$ es el tama√±o de una poblaci√≥n, entonces $P'(t)$ es la **tasa de crecimiento poblacional** en el tiempo $t$.

#### 3. **Econom√≠a**

Si $C(x)$ es el costo de producir $x$ unidades, entonces $C'(x)$ es el **costo marginal**, o cu√°nto aumentar√° el costo al producir una unidad adicional.

### üß† Conclusi√≥n

> La derivada como raz√≥n de cambio permite entender c√≥mo una variable responde a los cambios en otra. Es una herramienta clave para analizar din√°micas en sistemas reales.

### Resumen

Sup√≥n que llevas un tiempo estudiando la relaci√≥n entre los a√±os de experiencia como desarrollador de software (developer) y el salario de este trabajo. Llegas a la conclusi√≥n de que esta relaci√≥n es cuadr√°tica, y que el gr√°fico se ve como una par√°bola. Llamemos ‚Äú**s**‚Äù al salario y ‚Äú**a**‚Äù a la cantidad de a√±os de experiencia. La relaci√≥n entre ambas variables ser√≠a ![SA](images/sa.jpg).

S**i quisieras saber que tanto sube el salario en funci√≥n de los a√±os, podemos calcular la derivada**. Esta nos va a decir que tan r√°pido cambia ‚Äú**s**‚Äù respecto a ‚Äú**a**‚Äù. La derivada de una funci√≥n cuadr√°tica ![SA](images/fx=x.jpg) es simplemente ![SA](images/f'x=x.jpg), por lo que en este caso nos queda ![SA](images/s'a.jpg). En general, la derivada de una funci√≥n ![SA](images/fx=xf'.jpg). [Aqu√≠](https://static.platzi.com/media/public/uploads/derivadas_9e208991-949b-4c24-bedf-cb94464320a3.gif "Aqu√≠") puedes ver una tabla de drivadas comunes.

Ojo, en este **caso como la derivada es positiva (un n√∫mero elevado al cuadrado siempre es positivo) sabemos que la ‚Äúrapidez de cambio‚Äù aumenta junto con ‚Äúa‚Äù**. Si la derivada fuera negativa, significa que la tasa de cambio disminuye a medida que aumenta ‚Äú**a**‚Äù.

![Derivada del salario con respecto a los a√±os.](images/Derivadadelsalarioconrespectoalosanos.jpg)

## Notaciones de la Derivada en C√°lculo

Existen diferentes formas de expresar la derivada si de notaciones hablamos. Cada una de ellas fue propuesta por un cient√≠fico diferente al momento de desarrollar los principios del c√°lculo.

Si sabemos que la variable x es la variable independiente y y la variable dependiente a trav√©s de la relaci√≥n **y=f(x)**. Algunas notaciones para la derivada son las siguientes:

![Notaci√≥n de Leibniz](images/NotaciondeLeibniz.jpg)

#### Notaci√≥n de Leibniz

La notaci√≥n de Leibniz surge del s√≠mbolo dy/dx que representa un operador de diferenciaci√≥n y no debemos confundirlo como una divisi√≥n. Si quisi√©ramos expresar una segunda derivada usando la notaci√≥n de Leibniz se puede mostrar como:

![ec1.png](images/ec1.png)

Y para mostrar la n-√©sima derivada se expresa de la forma:

![ec2.png](images/ec2.png)

Esta notaci√≥n nos sirve para entender como la derivada puede ser expresada como los incrementos tanto de x como de y cuando el incremento de x tiende a cero.

![Notaci√≥n de Lagrange](images/NotaciondeLagrange.jpg)

**Notaci√≥n de Lagrange**

La notaci√≥n m√°s sencilla de todas es la de Lagrange. Esta notaci√≥n expresa que la funci√≥n es una derivada usando una comilla simple antes del argumento, llamada prima.

![Notaciones de la Derivadaen Calculo](NotacionesdelaDerivadaenCalculo.jpg)

Esta expresi√≥n se lee como ‚Äúefe prima de equis‚Äù. La cual representa la primera derivada de una funci√≥n. Si deseamos expresar la segunda derivada ser√≠a:

![Notaciones de la Derivadaen Calculo 1](NotacionesdelaDerivadaenCalculo1.png.jpg)

Y para mostrar la n-√©sima derivada se expresa de la forma:

![Notaciones de la Derivadaen Calculo](NotacionesdelaDerivadaenCalculo2.jpg)



Notaci√≥n de Newton
Por √∫ltimo tenemos la notaci√≥n de Newton. Esta notaci√≥n es muy usada en campos como la f√≠sica y la ingenier√≠a debido a su simplicidad para expresar la primera y segunda derivada. Se usa sobre todo en funciones relacionadas al tiempo en campos como la mec√°nica. Por ejemplo, como una funci√≥n que representa el movimiento de una part√≠cula.

Su representaci√≥n de la primera y segunda derivada es la siguiente:

·∫ã ·∫ç

En esta clase has aprendido cu√°les son las notaciones m√°s comunes que se usan para representar una derivada. Existen m√°s como es la notaci√≥n de Euler que se puede ver al inicio de esta lectura con el operador de diferenciaci√≥n D.

Nos vemos en la siguiente clase para aprender m√°s sobre c√°lculo. üíö

## Implementaci√≥n de la derivada discreta

La **derivada discreta** es una aproximaci√≥n de la derivada continua usada cuando se trabaja con datos en puntos separados (discretos), como en programaci√≥n, an√°lisis num√©rico o datos de sensores. En lugar de usar un l√≠mite, se usa una **diferencia finita**.

### üî¢ F√≥rmula de la derivada discreta

Para una funci√≥n $f(x)$ evaluada en puntos discretos $x_0, x_1, x_2, ..., x_n$, la derivada discreta se aproxima con:

$$
f'(x_i) \approx \frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}
$$

Esto se llama **diferencia hacia adelante**.

### üìå Otras aproximaciones

1. **Diferencia hacia atr√°s:**

$$
f'(x_i) \approx \frac{f(x_i) - f(x_{i-1})}{x_i - x_{i-1}}
$$

2. **Diferencia centrada (m√°s precisa):**

$$
f'(x_i) \approx \frac{f(x_{i+1}) - f(x_{i-1})}{x_{i+1} - x_{i-1}}
$$

### üßÆ Ejemplo en Python

```python
def derivada_discreta(x, y):
    # Asume x y y son listas o arrays del mismo tama√±o
    derivada = []
    for i in range(len(x) - 1):
        dx = x[i+1] - x[i]
        dy = y[i+1] - y[i]
        derivada.append(dy / dx)
    return derivada
```

#### Ejemplo de uso:

```python
x = [0, 1, 2, 3]
y = [0, 1, 4, 9]  # f(x) = x¬≤
print(derivada_discreta(x, y))  # Aproximaci√≥n de f'(x)
# Resultado: [1.0, 3.0, 5.0] ‚Üí se aproxima a 2x
```

### ‚úÖ Aplicaciones comunes

* An√°lisis de datos num√©ricos (gr√°ficas, sensores, series temporales)
* F√≠sica computacional
* Simulaci√≥n de sistemas din√°micos
* Procesamiento de se√±ales

### Resumen

En ciencia de datos nos interesa llevar los conceptos matem√°ticos como la derivada a una computadora. En [Google Colab](https://colab.research.google.com/?utm_source=scs-index "Google Colab") llevaremos a cabo la implementaci√≥n y visualizaci√≥n de las derivadas.

#### Implementando y derivando una funci√≥n en Colab

Una vez en Google Colab, importamos las librer√≠as que necesitamos. Usamos [NumPy](https://platzi.com/cursos/pandas-numpy/ "NumPy") para la manipulaci√≥n matem√°tica, y [matplotlib](https://platzi.com/cursos/matplotlib-seaborn/ "matplotlib") para la visualizaci√≥n.

```python
import numpy as np
import matplotlib.pyplot as plt
```

Entonces definimos una funci√≥n f, que en este caso corresponde a una cuadr√°tica. Tambi√©n definimos el conjunto de datos x al que le aplicamos la funci√≥n (recuerda, **las computadoras trabajan con valores discretos, no cont√≠nuos**). En este caso, **x** es un conjunto de 1000 datos que va de -10 a 10 ([m√°s sobre la funci√≥n linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html "m√°s sobre la funci√≥n linspace")). Una vez definida la funci√≥n y las variables, graficamos con plt.plot

```python
def f(x):
    return x**2
    
x = np.linspace(-10,10,1000)
y = f(x)

plt.plot(x, y, 'b')
plt.grid()
```
![x**2 graficado](images/x--2graficado.jpg)

Ahora, para implementar la derivada; como estamos trabajando con valores discretos, debemos implementarla de manera aproximada con la definici√≥n que usa l√≠mites. Recuerda que esta definici√≥n es ![9.pgn](images/9.jpg). Debemos escoger un valor de h lo suficientemente peque√±o para tener una buena aproximaci√≥n. La implementaci√≥n en c√≥digo es:

```python
def df(f):
    h = 0.000001
    return ( f(x+h) -f(x) )/h
```

Y para graficar la funci√≥n con su derivada simplemente usamos plt.plot

```python
plt.plot(x, y, 'b')
plt.plot(x, df(f), 'r')
plt.grid()
```
![x cuadrado y su derivada](images/xcuadradoysuderivada.jpg)

Podemos probar cambiando ![fx=2--2.jpg](images/fx=2--2.jpg). Para ello redefinimos la funci√≥n como:

```python
def f(x):
    return np.sin(x)
```

Y volvemos a graficar.

![funcionseno](images/funcionseno.jpg)

## La importancia de la regla de la cadena

La **regla de la cadena** es fundamental en c√°lculo diferencial y especialmente crucial en el **entrenamiento de modelos de Machine Learning**, incluyendo **redes neuronales**. Aqu√≠ te explico su importancia en distintos contextos:

### üìö ¬øQu√© es la regla de la cadena?

La regla de la cadena se usa para **derivar funciones compuestas**, es decir, funciones dentro de funciones.

$$
\text{Si } y = f(g(x)), \text{ entonces } \frac{dy}{dx} = f'(g(x)) \cdot g'(x)
$$

### üß† ¬øPor qu√© es importante?

#### 1. **En redes neuronales: backpropagation**

La regla de la cadena permite calcular c√≥mo cambian los pesos de una red neuronal para minimizar el error.

* Cada capa de una red es una funci√≥n compuesta de las anteriores.
* La regla de la cadena permite ‚Äúretropropagar‚Äù el error desde la salida hasta la entrada.
* Sin la regla de la cadena, **no podr√≠amos ajustar los pesos correctamente** durante el entrenamiento.

#### 2. **En funciones complejas de la vida real**

Muchos fen√≥menos (f√≠sicos, econ√≥micos, biol√≥gicos) se modelan con funciones compuestas. Para predecir o entender c√≥mo cambia una variable con respecto a otra, se necesita usar esta regla.

#### 3. **En optimizaci√≥n y aprendizaje autom√°tico**

La mayor√≠a de los modelos optimizan funciones de error compuestas. Para encontrar los m√≠nimos, se necesita derivar usando la regla de la cadena.

### üîç Ejemplo simple

Sea:

$$
f(x) = \sin(x^2)
$$

Aqu√≠:

* $g(x) = x^2$
* $f(g) = \sin(g)$

Entonces:

$$
f'(x) = \cos(x^2) \cdot 2x
$$

Esto es gracias a la regla de la cadena.

### ‚úÖ En resumen

La **regla de la cadena** es esencial porque:

* Hace posible derivar funciones compuestas.
* Es la base del algoritmo **backpropagation**, usado para entrenar redes neuronales.
* Permite entender c√≥mo peque√±os cambios en la entrada afectan la salida en sistemas complejos.

### Resumen

**La regla de la cadena se usa para derivar funciones compuestas**. Recuerda que una **funci√≥n compuesta** es una funci√≥n que recibe otra funci√≥n. **Mediante la regla de la cadena podemos obtener la raz√≥n de cambio de una variable inicial respecto a una variable final**. Esto se usa en algoritmos de redes neuronales como el [backpropagation](https://platzi.com/clases/2263-redes-neuronales/37442-backpropagation/ "backpropagation")

Para esto, multiplicamos la derivada de la funci√≥n externa por la derivada de la funci√≥n interna.

![F√≥rmula regla de la cadena](images/Formularegladelacadena.jpg)

#### Ejemplo de regla de la cadena

Para este ejemplo recordamos que las derivadas se pueden ver como una raz√≥n de cambio. Tengamos en cuenta que la derivada de una recta es su pendiente, y la pendiente determina que tanto crece la recta. Por lo tanto, es l√≥gico que sea su raz√≥n de cambio.

![Rectas y raz√≥n de cambio](images/Rectasyrazondecambio.jpg)

Podemos ver que hay dos gr√°ficas que se relacionan con la altura. Si con la **edad (x)** obtenemos la **altura (y)**, y con la **altura** podemos obtener el **peso (z)**, entonces podemos obtener la raz√≥n de cambio del **peso** directamente de la **edad** con la composici√≥n de funciones $z(y(x))$ y la regla de la cadena.

![Aplicaci√≥n de la regla de la cadena](images/Aplicaciondelaregladelacadena.jpg)

## ¬øQu√© es un m√°ximo y un m√≠nimo?

En c√°lculo, los **m√°ximos** y **m√≠nimos** son puntos donde una funci√≥n alcanza sus **valores extremos**. Se dividen en dos tipos: **m√°ximos/m√≠nimos locales** y **globales**.

### üìå Definiciones

#### üî∫ M√°ximo

Un punto donde la funci√≥n alcanza un valor **mayor** que sus valores cercanos.

* **M√°ximo local:**
  En un peque√±o intervalo alrededor del punto, la funci√≥n tiene su valor m√°s alto.
  Ejemplo: el pico de una colina.

* **M√°ximo global (absoluto):**
  Es el valor m√°s alto en **toda la funci√≥n**.

#### üîª M√≠nimo

Un punto donde la funci√≥n alcanza un valor **menor** que sus valores cercanos.

* **M√≠nimo local:**
  En una peque√±a vecindad, es el valor m√°s bajo.
  Ejemplo: el fondo de un valle.

* **M√≠nimo global:**
  El punto m√°s bajo de toda la funci√≥n.

### üìê ¬øC√≥mo se encuentran?

Mediante el **c√°lculo de derivadas**:

1. **Deriva la funci√≥n:**
   Encuentra $f'(x)$

2. **Encuentra los puntos cr√≠ticos:**
   Resuelve $f'(x) = 0$ o donde $f'(x)$ no existe.

3. **Usa la segunda derivada para clasificar:**

   * Si $f''(x) > 0$: m√≠nimo local (la curva se abre hacia arriba).
   * Si $f''(x) < 0$: m√°ximo local (la curva se abre hacia abajo).
   * Si $f''(x) = 0$: podr√≠a ser un punto de inflexi√≥n (requiere m√°s an√°lisis).

### üß† ¬øPor qu√© son importantes?

* **Optimizaci√≥n:** Se usan para maximizar ganancias, minimizar costos, encontrar el mejor ajuste, etc.
* **F√≠sica:** Para encontrar puntos de equilibrio o condiciones extremas.
* **Machine Learning:** Se busca el m√≠nimo de la funci√≥n de p√©rdida durante el entrenamiento.

### ‚úÖ Ejemplo:

$$
f(x) = -x^2 + 4x
$$

1. Derivada:

$$
f'(x) = -2x + 4
$$

2. Puntos cr√≠ticos:

$$
-2x + 4 = 0 \Rightarrow x = 2
$$

3. Segunda derivada:

$$
f''(x) = -2 < 0 \Rightarrow \text{m√°ximo local en } x = 2
$$

### Resumen

#### Obteniendo m√°ximos y m√≠nimos con la derivada

Imag√≠nate que la siguiente curva es una monta√±a rusa, y que la recta tangente a esta curva en cualquier punto es el carrito de la monta√±a rusa. Cuando el carrito va subiendo, este tiene cierta inclinaci√≥n. Cuando va bajando, tiene una inclinaci√≥n opuesta. Esta inclinaci√≥n nos la da la derivada. Como sabr√°s, **si la derivada es positiva, la curva incrementa, y al contrario si la derivada es negativa**.

[Curva con m√°ximos y m√≠nimos](images/Curvaconmaximosyminimos.jpg)

Ahora, **¬øqu√© pasa si la derivada es cero?** Cuando la derivada vale cero en alg√∫n punto, a esto se le conoce como punto cr√≠tico. En este caso, la recta tangente a dicho punto es completamente horizontal. Es en este momento que nos encontramos un posible m√°ximo o m√≠nimo. **¬øC√≥mo determinamos si este punto corresponde a un m√°ximo o un m√≠nimo?** Responde en los comentarios antes de seguir leyendo‚Ä¶

Si respondiste que estudiando la derivada en puntos a la izquierda y a la derecha del punto cr√≠tico, est√°s en lo correcto. Se determina que hay un punto m√°ximo LOCAL si la derivada cambia de positivo a negativo y viceversa si es un punto m√≠nimo. A esto se le conoce como el **criterio de la primera derivada**.

Tambi√©n existe el criterio de la segunda derivada, que consiste en estudiar que pasa cuando derivamos por segunda vez la funci√≥n en un punto cr√≠tico. Te invito a investigarlo (ya que est√° fuera del alcance de este curso).

## Optimizando nuestro primer problema

¬°Perfecto! Vamos a **optimizar nuestro primer problema paso a paso** usando c√°lculo. Este ejemplo cl√°sico es ideal para aprender el proceso de optimizaci√≥n usando derivadas.

### üß© **Problema:**

> Una empresa vende helados. Cada helado se vende a \$5. Cada unidad cuesta \$1 en materiales y \$2 en distribuci√≥n. ¬øCu√°ntos helados deben vender para maximizar la ganancia?

### üß† **Paso 1: Entender el objetivo**

Queremos **maximizar la ganancia**.

* **Precio de venta por unidad:** \$5
* **Costo por unidad:** \$1 (materiales) + \$2 (distribuci√≥n) = \$3
* **Ganancia por unidad:** \$5 ‚àí \$3 = \$2

Si vendemos $x$ helados, la **ganancia total** es:

$$
G(x) = 2x
$$

### üòÖ Pero... ¬°esto no tiene m√°ximo!

La funci√≥n $G(x) = 2x$ crece sin l√≠mite. Necesitamos **m√°s restricciones**. Supongamos una nueva versi√≥n:

### üß© **Problema corregido:**

> Se quiere construir una cerca rectangular junto a un r√≠o, usando 100 metros de valla. No se necesita valla en el lado del r√≠o. ¬øCu√°les deben ser las dimensiones para **maximizar el √°rea**?

### üìê **Paso 2: Definir variables**

* Lado **paralelo** al r√≠o: $x$
* Lados **perpendiculares** al r√≠o: $y$

Solo usamos valla en 3 lados: uno de $x$, dos de $y$.
Restricci√≥n:

$$
x + 2y = 100
$$

Queremos **maximizar el √°rea**:

$$
A = x \cdot y
$$

### üîÅ **Paso 3: Sustituir y reducir a una variable**

De la restricci√≥n:

$$
x = 100 - 2y
$$

Sustituimos en el √°rea:

$$
A(y) = (100 - 2y)y = 100y - 2y^2
$$

### üßÆ **Paso 4: Derivar y optimizar**

$$
A'(y) = 100 - 4y
$$

Puntos cr√≠ticos:

$$
100 - 4y = 0 \Rightarrow y = 25
$$

Segunda derivada:

$$
A''(y) = -4 < 0 \Rightarrow \text{M√°ximo}
$$

### ‚úÖ **Resultado**

* $y = 25$
* $x = 100 - 2(25) = 50$

üëâ Las dimensiones que **maximizan el √°rea** son:

$$
\boxed{x = 50\text{ m},\ y = 25\text{ m}}
$$

### Resumen

**Los problemas de optimizaci√≥n en general requieren encontrar m√≠nimos o m√°ximos**. Veamos esto con el siguiente problema: queremos construir una oficina con solo 50 mts de per√≠metro de paredes abarcando el √°rea m√°s grande posible (punto m√°ximo). La oficina solo tiene 3 paredes y una vista al mar.

#### Resolviendo el problema

![Visualizaci√≥n del problema](images/Visualizaciondelproblema.jpg)

**Primero debemos encontrar la funci√≥n a optimizar**. En este caso, queremos encontrar el √°rea m√°xima. De acuerdo al dibujo de arriba, el √°rea est√° dada por `A = x * y`, y el per√≠metro de las tres paredes est√° dado por `p=2x+y=50`. Teniendo estos datos, podr√≠amos intentar resolverlo al tanteo. Sin embargo, la mejor forma de resolverlo parecido a un sistema de ecuaciones.

Si tenemos que `2x+y=50`, podemos reordenar y nos queda `y=50-2x`. Entonces sustitu√≠mos esta expresi√≥n en la f√≥rmula del √°rea para que nos quede solo en funci√≥n de `x`. Nos queda: `A(x)=x(50-2x)=50x-2x*x`.

**Ahora toca diferenciar la funci√≥n del √°rea e igualarla a cero**. Quedando as√≠: `A‚Äô(x) = -4x + 50 = 0`. Resolvemos para x, y nos da un punto cr√≠tico en `x=25/2`. Solo nos queda determinar si este es un m√°ximo. Para ello evaluamos en la derivada un punto a la izquierda y a la derecha de 25/2. Es decir:

```
A'(12)=-4*12+50=2
A'(13)=-4*13+50=-2
```

Como a la izquierda de `x` la derivada es positiva, y a la derecha es negativa, podemos decir que `x=25/2` es un m√°ximo. Solo queda sustituir en `y=50-2x`. Nos da que `y=25`. Por lo tanto, las medidas de las paredes de la oficina con la mayor √°rea son `x=(25/2)m` y `y=25m` . El √°rea nos da 1.PNG![A=12.5m*25m=312.5](images/am.jpg).

#### Conclusi√≥n

Es importante entender c√≥mo resolver problemas de optimizaci√≥n para saber qu√© hay detr√°s de distintos algoritmos de machine learning como el descenso del gradiente. La ventaja que tenemos en la ciencia de datos es que dichos problemas ya est√°n resueltos en c√≥digo por otros data scientists. Sin embargo, es √∫til entender de donde vienen dichas soluciones.

## ¬øC√≥mo son las derivadas en las funciones de activaci√≥n?

Las derivadas de las **funciones de activaci√≥n** son fundamentales en el entrenamiento de redes neuronales, porque permiten **ajustar los pesos** durante el aprendizaje usando **backpropagation**. Aqu√≠ te explico c√≥mo son y por qu√© importan:

### üß† ¬øPor qu√© necesitamos derivadas en activaciones?

Cuando entrenamos una red neuronal, usamos una t√©cnica llamada **descenso del gradiente**, que necesita calcular c√≥mo cambia el error al ajustar cada peso. Eso se hace a trav√©s de derivadas (o gradientes).
La **derivada de la funci√≥n de activaci√≥n** permite propagar el error hacia atr√°s desde la salida hasta las capas ocultas.

### ‚öôÔ∏è Ejemplos comunes de funciones de activaci√≥n y sus derivadas:

| Funci√≥n de Activaci√≥n | F√≥rmula                                              | Derivada                                                                                      |
| --------------------- | ---------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| **Sigmoide**          | $\sigma(x) = \frac{1}{1 + e^{-x}}$                   | $\sigma'(x) = \sigma(x)(1 - \sigma(x))$                                                       |
| **Tanh**              | $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$       | $\tanh'(x) = 1 - \tanh^2(x)$                                                                  |
| **ReLU**              | $\text{ReLU}(x) = \max(0, x)$                        | $\text{ReLU}'(x) = \begin{cases} 1 & \text{si } x > 0 \\ 0 & \text{si } x \leq 0 \end{cases}$ |
| **Leaky ReLU**        | $\text{LReLU}(x) = \max(0.01x, x)$                   | $\text{LReLU}'(x) = \begin{cases} 1 & x > 0 \\ 0.01 & x \leq 0 \end{cases}$                   |
| **Softmax**           | $\text{Softmax}(x_i) = \frac{e^{x_i}}{\sum e^{x_j}}$ | Tiene una derivada m√°s compleja (matriz Jacobiana) usada en clasificaci√≥n multiclase          |

### üü¢ ¬øQu√© significa esto en la pr√°ctica?

* La **sigmoide** y **tanh** son suaves, pero pueden causar el problema de **gradientes peque√±os** (vanishing gradients).
* La **ReLU** es muy usada porque es simple y eficiente, y no sufre tanto del problema anterior.
* La **Softmax** se usa en la √∫ltima capa para clasificaci√≥n multiclase.

### üìå Resumen

* Las derivadas permiten que el modelo **aprenda ajustando pesos**.
* Elegir la **funci√≥n de activaci√≥n adecuada** impacta directamente en la velocidad y eficacia del aprendizaje.
* ¬°Sin derivadas, no hay backpropagation!

### Resumen

En [este notebook](https://colab.research.google.com/drive/1xXXphqsaEczh1vo4T1wumkqM1xybpqOf?usp=sharing "este notebook") de Google Colab exploramos las derivadas de distintas [funciones de activaci√≥n](https://platzi.com/clases/2701-funciones-matematicas/45566-funciones-de-activacion/ "funciones de activaci√≥n").

#### Derivadas de funciones de activaci√≥n

Mediante la funci√≥n derivada discreta que programamos en clases anteriores, podemos obtener una derivada aproximada de las funciones de activaci√≥n.

```python
def df(f):
  h=0.000001
  return (f(x+h)-f(x))/h
```

#### Derivada de una funci√≥n lineal

**Las funciones lineales tambi√©n pueden servir como funciones de activaci√≥n de una red neuronal**. Por esto es importante entender que la derivada de una funci√≥n lineal es simplemente su pendiente. Es decir

```python
"""Sea
f(x) = mx+b
f'(x) = m
"""

def f(x):
  return x

plt.plot(x, f(x), 'b')
plt.plot(x,df(f), 'r')
plt.grid()
```

![Derivada lineal](images/Derivadalineal.jpg)

#### Derivada de la funci√≥n de Heaviside

Recordemos que la funci√≥n de Heavyside est√° dada por partes, de la siguiente forma:
1.PNG
![Heaviside](images/Heaviside.jpg)

![Gr√°fica funci√≥n de heavyside](images/Graficafunciondeheavyside.jpg)

Si vemos la gr√°fica de la funci√≥n, nos damos cuenta que para `x=0` la funci√≥n ‚Äúcrece‚Äù completamente vertical hasta `y=1`. Es decir, la recta tangente en x=0 tiene pendiente infinita. **Se puede demostrar que la derivada de la funci√≥n de Heavyside corresponde a la ‚ÄúDelta de Dirac‚Äù**. Te invito a investigar la **Delta de Dirac** por tu cuenta, pero en res√∫men, esta funci√≥n tiende a infinito cuando `x` tiende a cero (en este caso), y vale cero para todos los dem√°s valores.

#### Derivada de la funci√≥n sigmoide

La funci√≥n sigmoide es usada tanto en redes neuronales como en [regresi√≥n log√≠stica](https://platzi.com/clases/2081-ds-probabilidad/33070-regresion-logistica/ "regresi√≥n log√≠stica"). Esta funci√≥n se expresa como:

![funci√≥n sigmoide](images/funcionsigmoide.jpg)

Y su derivada:
3.PNG
![derivada](images/derivada.jpg)

Optimizar esta funci√≥n en redes neuronales puede llevar a un problema conocido como **‚Äúvanishing gradient‚Äù**, debido a la complejidad de la funci√≥n. Te invito a investigarlo. Por ahora, te dejo el c√≥digo y el gr√°fico de esta funci√≥n con su derivada.

```python
def f(x):
  return 1/(1 + np.exp(-x))
   
plt.plot(x, f(x), 'b')
plt.plot(x,df(f), 'r')
plt.grid()
```

![funci√≥n sigmoide y su derivada](images/funcionsigmoideysuderivada.jpg)

#### Derivada de la funci√≥n tangente hiperb√≥lica

Con esta funci√≥n ocurre algo similar que con la sigmoide. La complejidad de su derivada puede causar problemas durante la optimizaci√≥n. La funci√≥n tangente hiperb√≥lica est√° dada por: ![tangente hiperb√≥lica](images/tangentehiperbolica.jpg)

Y su derivada

![su derivada](images/suderivada.jpg)

![tanh y su derivada](images/tanhysuderivada.jpg)

#### Derivada de la funci√≥n ReLU

La funci√≥n ReLU es especialmente √∫til en las capas intermedias de una red neuronal, gracias a su relativa sencillez. La funci√≥n ReLu est√° definida como $R(x)=max(0,x)$, o bien:

![rx](images/rx.jpg)

Y su derivada est√° dada por

![rxd](images/rxd.jpg)

Nota: ReLu no tiene derivada en cero

![relu](images/relu.jpg)

#### Conclusi√≥n

Entender estas funciones y sus derivadas nos ayudar√° a comprender los fundamentos necesarios para desarrollar algoritmos de machine learning y redes neuronales. Esto no se trata de una receta de cocina, y sino de pensar de manera anal√≠tica usando dichos fundamentos.

## ¬øQuieres un Curso de C√°lculo Integral para Data Science e Inteligencia Artificial?

Aprendiste la importancia de la derivada en m√∫ltiples √°reas. Ya sea en su uso como raz√≥n de cambio, en problemas de optimizaci√≥n y en funciones de activaci√≥n.

### ¬øQu√© sigue?

El objetivo de estudiar la derivada, as√≠ como otros fundamentos matem√°ticos para data science, es que los m√©todos aplicados en esta rama dejen de ser una caja negra. Es importante saber qu√© hay por detr√°s de los algoritmos y m√©todos aplicados en data science para poder desarrollar nuevos m√©todos u optimizar otros.

En pr√≥ximos cursos veremos c√°lculo multivariable para ciencia de datos.

Recuerda tomar el ex√°men, y compartir el certificado con el profesor Enrique en su [Twitter](https://twitter.com/codevars "Twitter").

**Lecturas recomendadas**

[https://twitter.com/codevars](https://twitter.com/codevars)