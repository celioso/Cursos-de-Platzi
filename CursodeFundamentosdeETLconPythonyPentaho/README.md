# Curso de Fundamentos de ETL con Python y Pentaho

## 쯈u칠 es un ETL en ingenier칤a de datos?

Un **ETL** (Extract, Transform, Load) es un proceso en la ingenier칤a de datos que se utiliza para mover y transformar datos desde diferentes fuentes hacia un destino centralizado, como un **data warehouse** o un **data lake**, para su an치lisis o procesamiento. Este proceso consta de tres fases principales:

### **1. Extract (Extracci칩n):**
- **Objetivo:** Obtener datos de una o m치s fuentes heterog칠neas.
- **Fuentes t칤picas:** Bases de datos SQL, archivos (CSV, JSON, XML), APIs, sistemas ERP, hojas de c치lculo, entre otros.
- **Desaf칤os comunes:** Manejo de datos inconsistentes, conexiones lentas y formatos variados.

### **2. Transform (Transformaci칩n):**
- **Objetivo:** Procesar y convertir los datos para que sean consistentes, limpios y adecuados para el an치lisis.
- **Ejemplos de transformaciones:**
  - Normalizaci칩n de formatos (ejemplo: fechas o monedas).
  - Enriquecimiento de datos (agregar datos adicionales desde otra fuente).
  - Limpieza de datos (eliminar duplicados, corregir errores).
  - Agregaciones o c치lculos (ejemplo: sumar ingresos mensuales).
- **Beneficio:** Garantiza que los datos sean 칰tiles y coherentes antes de su almacenamiento.

### **3. Load (Carga):**
- **Objetivo:** Transferir los datos transformados al sistema de destino.
- **Tipos de carga:**
  - **Carga completa:** Se transfiere todo el conjunto de datos.
  - **Carga incremental:** Solo se cargan los datos nuevos o modificados.
- **Destinos t칤picos:** Bases de datos anal칤ticas, herramientas de BI (Business Intelligence), o sistemas de almacenamiento en la nube.

### **Importancia del ETL:**
- Proporciona una base s칩lida para el an치lisis de datos y la toma de decisiones basada en datos confiables.
- Permite integrar informaci칩n de m칰ltiples fuentes en un 칰nico sistema.
- Mejora la calidad y accesibilidad de los datos.

### Herramientas ETL populares:
- **Open Source:** Apache Nifi, Talend Open Studio, Pentaho.
- **Comerciales:** Informatica, Microsoft SQL Server Integration Services (SSIS).
- **Basadas en la nube:** AWS Glue, Google Dataflow, Azure Data Factory.

Aqu칤 tienes un ejemplo b치sico de un proceso ETL en Python utilizando la biblioteca `pandas`. Este script leer치 datos de un archivo CSV, realizar치 algunas transformaciones b치sicas y luego los cargar치 en un archivo de salida:

### Ejemplo de ETL con Python y pandas

```python
import pandas as pd

# ETL Pipeline

# --- Extract ---
# Leer datos desde un archivo CSV
input_file = 'datos_entrada.csv'  # Archivo de entrada
try:
    data = pd.read_csv(input_file)
    print("Datos cargados exitosamente:")
    print(data.head())
except FileNotFoundError:
    print(f"El archivo {input_file} no existe. Por favor, verifica la ruta.")

# --- Transform ---
# Transformaciones b치sicas:
# 1. Renombrar columnas
data.rename(columns={"Nombre": "name", "Edad": "age", "Pa칤s": "country"}, inplace=True)

# 2. Convertir la edad a n칰meros enteros
data['age'] = pd.to_numeric(data['age'], errors='coerce')

# 3. Filtrar filas donde la edad no sea nula y mayores de 18 a침os
data = data[data['age'] >= 18]

# 4. Capitalizar nombres de pa칤s
data['country'] = data['country'].str.capitalize()

print("\nDatos despu칠s de la transformaci칩n:")
print(data.head())

# --- Load ---
# Guardar datos transformados en un nuevo archivo
output_file = 'datos_salida.csv'
data.to_csv(output_file, index=False)
print(f"\nDatos procesados guardados en {output_file}")
```

### 쯈u칠 hace este script?

1. **Extract (Extracci칩n):**
   - Lee un archivo CSV llamado `datos_entrada.csv`.
   - Muestra las primeras filas para verificar el contenido.

2. **Transform (Transformaci칩n):**
   - Renombra columnas para estandarizar los nombres.
   - Convierte la columna de edad (`Edad`) en n칰meros enteros.
   - Filtra registros donde la edad sea mayor o igual a 18.
   - Ajusta los nombres de los pa칤ses para tener formato capitalizado.

3. **Load (Carga):**
   - Guarda el resultado transformado en un nuevo archivo llamado `datos_salida.csv`.

### Estructura de entrada esperada (`datos_entrada.csv`):
```csv
Nombre,Edad,Pa칤s
Juan,25,Colombia
Maria,17,per칰
Carlos,30,Chile
Ana,,Brasil
```

### Resultado (`datos_salida.csv`):
```csv
name,age,country
Juan,25,Colombia
Carlos,30,Chile
```
### **ETL vs. ELT: Prop칩sito y Usos**

La elecci칩n entre **ETL (Extract, Transform, Load)** y **ELT (Extract, Load, Transform)** depende del caso de uso, la infraestructura y los requisitos del negocio. Ambos son enfoques utilizados para mover y procesar datos, pero tienen diferencias clave en c칩mo y d칩nde se realiza la transformaci칩n de los datos.

### **1. ETL (Extract, Transform, Load)**

#### **쯈u칠 es?**
- En ETL, los datos se extraen de las fuentes, se transforman en un entorno intermedio (como un servidor o herramienta dedicada), y luego se cargan en el sistema de destino.

#### **쯇ara qu칠 se utiliza?**
- **Sistemas tradicionales de an치lisis de datos** como **Data Warehouses** (almacenes de datos).
- Cuando el sistema de destino tiene capacidades limitadas para procesar datos o se requieren transformaciones complejas antes de la carga.
- **Casos donde se prioriza la calidad de los datos antes del an치lisis.**

#### **Ventajas:**
1. **Transformaci칩n temprana:** Los datos llegan al destino ya listos para el an치lisis.
2. **Control sobre la calidad de datos:** Permite realizar validaciones estrictas y limpiezas antes de la carga.
3. **Compatible con sistemas antiguos:** Ideal para bases de datos tradicionales que no manejan grandes vol칰menes de datos.

#### **Limitaciones:**
- Puede ser m치s lento, especialmente con vol칰menes grandes de datos.
- Requiere una infraestructura intermedia para realizar las transformaciones.

### **2. ELT (Extract, Load, Transform)**

#### **쯈u칠 es?**
- En ELT, los datos se extraen de las fuentes, se cargan directamente en el sistema de destino (por ejemplo, un Data Lake o un Data Warehouse moderno), y luego se transforman utilizando el poder computacional del sistema de destino.

#### **쯇ara qu칠 se utiliza?**
- **Sistemas modernos en la nube**, como **Data Lakes** (Google BigQuery, Amazon Redshift, Snowflake).
- Cuando los vol칰menes de datos son masivos y los sistemas de destino tienen alta capacidad de procesamiento.
- **An치lisis en tiempo real o cuasi-tiempo real**, donde los datos deben estar r치pidamente disponibles.

#### **Ventajas:**
1. **Velocidad de carga inicial:** Los datos se mueven r치pidamente al destino.
2. **Escalabilidad:** Aprovecha la potencia de procesamiento de sistemas en la nube.
3. **Flexibilidad:** Permite explorar y transformar los datos seg칰n las necesidades posteriores.

#### **Limitaciones:**
- Requiere un sistema de destino robusto con capacidades avanzadas.
- Puede cargar datos "sucios" inicialmente, lo que podr칤a generar problemas si no se transforman correctamente.

### **Comparaci칩n Directa**

| Caracter칤stica              | **ETL**                           | **ELT**                          |
|-----------------------------|------------------------------------|-----------------------------------|
| **Momento de transformaci칩n** | Antes de cargar los datos          | Despu칠s de cargar los datos       |
| **Sistema de destino**       | Data Warehouse tradicional         | Data Lake o Data Warehouse moderno |
| **Procesamiento de datos**   | Externo al destino                 | Interno en el destino            |
| **Vol칰menes de datos**       | Moderados                          | Grandes o masivos                |
| **Velocidad inicial**        | M치s lenta                          | M치s r치pida                       |
| **Limpieza y calidad inicial**| Alta calidad antes de la carga     | Requiere procesamiento posterior |

### **쮺u치ndo usar ETL?**
- Cuando los datos requieren transformaciones complejas o detalladas antes de ser 칰tiles.
- Cuando trabajas con sistemas antiguos o limitados en capacidad.
- Cuando los datos necesitan estar listos para an치lisis inmediatamente despu칠s de la carga.

### **쮺u치ndo usar ELT?**
- Cuando se manejan grandes vol칰menes de datos no estructurados.
- Si el sistema de destino tiene gran capacidad de procesamiento y escalabilidad (por ejemplo, BigQuery, Snowflake).
- Cuando se necesita flexibilidad para analizar o transformar datos en diferentes momentos.

## Conceptos base de ETL

### Conceptos Base de ETL (Extract, Transform, Load)

ETL (Extraer, Transformar, Cargar) es un proceso fundamental en la ingenier칤a de datos que permite trasladar y procesar datos desde m칰ltiples fuentes hacia un destino final para an치lisis o almacenamiento. A continuaci칩n, se describen los conceptos base:

### **1. Extracci칩n (Extract)**
**Definici칩n:** Es el proceso de recopilar datos desde una o varias fuentes heterog칠neas. Las fuentes pueden incluir bases de datos relacionales, archivos planos (CSV, JSON, XML), APIs, logs, o sistemas ERP.  

**Caracter칤sticas:**
- **Variedad de fuentes:** Datos estructurados (tablas SQL) y no estructurados (archivos de texto, im치genes).
- **Objetivo:** Obtener datos sin alterar su formato original.
- **Herramientas comunes:** Conectores de bases de datos, APIs REST, scripts personalizados.

**Ejemplo:**  
Conectar a una base de datos SQL para extraer una tabla de usuarios:
```sql
SELECT * FROM usuarios;
```

### **2. Transformaci칩n (Transform)**
**Definici칩n:** Es la etapa donde los datos se limpian, estandarizan, enriquecen o transforman para adaptarse a las necesidades del negocio o del sistema de destino.  

**Operaciones t칤picas:**
- **Limpieza:** Eliminar valores nulos, duplicados o inconsistentes.
- **Normalizaci칩n:** Cambiar formatos de fecha o convertir unidades de medida.
- **C치lculos:** Crear nuevas columnas (por ejemplo, calcular ingresos anuales a partir de ingresos mensuales).
- **Enriquecimiento:** Combinar datos de m칰ltiples fuentes.
- **Validaci칩n:** Asegurarse de que los datos cumplen con reglas de negocio espec칤ficas.

**Ejemplo:**  
Convertir un archivo CSV de ventas en un formato estandarizado:
```python
import pandas as pd

# Cargar datos
data = pd.read_csv("ventas.csv")

# Limpiar y transformar
data['fecha'] = pd.to_datetime(data['fecha'])
data['total'] = data['cantidad'] * data['precio_unitario']
data = data.dropna()  # Eliminar valores nulos
```

### **3. Carga (Load)**
**Definici칩n:** Es el proceso de mover los datos transformados al sistema de destino, como un almac칠n de datos (Data Warehouse), base de datos, o sistema de an치lisis.  

**Tipos de carga:**
- **Carga completa:** Sobrescribe los datos existentes en cada ejecuci칩n.
- **Carga incremental:** Solo se cargan los datos nuevos o modificados.
- **Carga en tiempo real:** Los datos se env칤an continuamente al destino.

**Herramientas comunes:** 
- SQL para bases de datos relacionales.
- APIs o conectores espec칤ficos para sistemas en la nube como Amazon S3 o Google BigQuery.

**Ejemplo:**  
Insertar los datos procesados en una tabla de SQL:
```sql
INSERT INTO ventas_procesadas (fecha, producto, cantidad, total)
VALUES ('2024-01-01', 'Laptop', 10, 15000);
```

### **Objetivo del Proceso ETL**
El prop칩sito principal de ETL es consolidar datos dispersos en un solo lugar, procesarlos para que sean 칰tiles y garantizar que est칠n listos para el an치lisis o la toma de decisiones. Esto incluye:
- **Integraci칩n:** Combinar datos de diferentes fuentes.
- **Consistencia:** Proveer datos limpios y estructurados.
- **Eficiencia:** Reducir la complejidad del acceso y an치lisis.

### **ETL vs. ELT**
Aunque ETL es el enfoque tradicional, **ELT (Extract, Load, Transform)** es una variaci칩n que carga los datos directamente en el almac칠n de datos antes de transformarlos. Esto se utiliza especialmente en sistemas modernos basados en la nube.

## Consideraciones de ETL

### **Consideraciones Clave en un Proceso ETL**

El 칠xito de un proyecto ETL (Extract, Transform, Load) depende de la planificaci칩n cuidadosa, la comprensi칩n de las necesidades del negocio y la calidad de la ejecuci칩n. Aqu칤 tienes las principales consideraciones al implementar un proceso ETL:

### **1. Comprensi칩n de los Requisitos**
- **Objetivos del negocio:** Define claramente qu칠 se espera lograr con el proceso ETL (reportes, an치lisis, monitoreo, etc.).
- **Volumen de datos:** Considera la cantidad de datos a procesar y su frecuencia (diaria, semanal, en tiempo real).
- **Fuente de datos:** Identifica todas las fuentes de datos (bases de datos relacionales, APIs, archivos CSV, etc.) y su formato.

### **2. Calidad de los Datos**
- **Integridad de datos:** Verifica que los datos de las fuentes sean completos y precisos.
- **Consistencia:** Aseg칰rate de que los datos tengan un formato est치ndar (por ejemplo, fechas y monedas).
- **Manejo de datos err칩neos:** Implementa estrategias para tratar datos faltantes, duplicados o corruptos.

### **3. Escalabilidad**
- **Crecimiento futuro:** Dise침a el sistema para manejar un incremento en el volumen y variedad de datos.
- **Escalabilidad horizontal:** Utiliza herramientas capaces de procesar datos en paralelo para mantener el rendimiento.

### **4. Rendimiento**
- **Tiempo de procesamiento:** Minimiza el tiempo necesario para extraer, transformar y cargar datos, especialmente para procesos cr칤ticos.
- **Optimizaci칩n de consultas:** Utiliza 칤ndices y otras t칠cnicas para acelerar las operaciones en bases de datos.

### **5. Seguridad**
- **Encriptaci칩n:** Protege los datos sensibles durante la transferencia (en tr치nsito) y el almacenamiento (en reposo).
- **Control de acceso:** Implementa pol칤ticas de seguridad para limitar qui칠n puede acceder a los datos y realizar cambios.
- **Regulaciones:** Cumple con normativas como GDPR, HIPAA o CCPA, seg칰n sea necesario.

### **6. Mantenimiento**
- **Monitoreo:** Configura alertas para detectar fallos en tiempo real.
- **Registro de errores:** Implementa un sistema de logging para rastrear problemas en el flujo de datos.
- **Actualizaciones:** Aseg칰rate de que las herramientas ETL puedan actualizarse sin interrupciones significativas.

### **7. Herramientas y Tecnolog칤a**
- **Elecci칩n de herramientas:** Decide entre herramientas comerciales (Informatica, Talend) o plataformas en la nube (AWS Glue, Google Dataflow).
- **Compatibilidad:** Aseg칰rate de que la herramienta seleccionada pueda conectarse a todas las fuentes de datos necesarias.

### **8. Transformaciones de Datos**
- **Complejidad:** Eval칰a qu칠 tan complejas son las transformaciones necesarias (filtros, agregaciones, cambios de formato).
- **Flexibilidad:** Dise침a transformaciones modulares y reutilizables.
- **Pruebas:** Verifica que las transformaciones produzcan resultados correctos antes de cargarlas en el destino.

### **9. Procesos de Carga**
- **Tipo de carga:** Define si ser치 completa o incremental (solo datos nuevos o modificados).
- **Manejo de fallos:** Implementa mecanismos para reiniciar cargas fallidas sin duplicar datos.
- **Orden de carga:** Aseg칰rate de que las dependencias entre tablas se respeten.

### **10. Documentaci칩n**
- **Mapeo de datos:** Documenta c칩mo se transforman los datos desde las fuentes hasta el destino.
- **Gu칤as operativas:** Proporciona instrucciones claras para administrar y solucionar problemas del flujo ETL.
- **Versionado:** Registra cambios en el dise침o del flujo para facilitar auditor칤as y mantenimiento.

### **11. Consideraciones Adicionales**
- **ETL vs. ELT:** Eval칰a si un enfoque ELT podr칤a ser m치s adecuado para el caso de uso espec칤fico.
- **Costos:** Considera los costos de licencias, infraestructura y mantenimiento de las herramientas ETL.
- **Pruebas:** Realiza pruebas exhaustivas antes de implementar en producci칩n.

Al abordar cada una de estas 치reas, puedes garantizar que el proceso ETL sea robusto, eficiente y alineado con las necesidades del negocio. 

## Servicios y herramientas para ETL

### **Servicios y Herramientas para ETL**

El 칠xito de los procesos de ETL depende en gran medida de las herramientas y servicios que facilitan la extracci칩n, transformaci칩n y carga de datos. A continuaci칩n, se describen las principales opciones divididas en categor칤as clave:

### **1. Herramientas ETL Tradicionales**
Estas herramientas est치n dise침adas espec칤ficamente para procesos ETL en entornos locales o h칤bridos.

- **Informatica PowerCenter**  
  Una de las herramientas m치s populares y robustas para ETL. Ofrece funciones avanzadas para transformar y gestionar grandes vol칰menes de datos.

- **Talend Data Integration**  
  Plataforma de c칩digo abierto que incluye conectores para diversas fuentes de datos y capacidades avanzadas de transformaci칩n.

- **IBM DataStage**  
  Herramienta empresarial para grandes proyectos ETL, ideal para integraciones complejas y procesamiento de big data.

- **Microsoft SQL Server Integration Services (SSIS)**  
  Parte de Microsoft SQL Server, ofrece capacidades ETL para usuarios que trabajan con bases de datos SQL.

### **2. Herramientas ETL en la Nube**
Dise침adas para aprovechar la escalabilidad y flexibilidad de la nube, estas herramientas integran flujos ETL con servicios en la nube.

- **AWS Glue**  
  Servicio ETL totalmente administrado en AWS que permite ejecutar transformaciones de datos basadas en Python (PySpark).

- **Google Cloud Dataflow**  
  Ofrece capacidades de procesamiento en tiempo real y por lotes para pipelines de datos en la nube de Google.

- **Azure Data Factory**  
  Soluci칩n de integraci칩n de datos de Microsoft Azure que permite mover y transformar datos entre m칰ltiples or칤genes y destinos.

- **Snowflake + Matillion**  
  Snowflake es un almac칠n de datos en la nube, y Matillion es una herramienta ETL dise침ada espec칤ficamente para integrarse con Snowflake.

### **3. Herramientas Open Source**
Opciones gratuitas que ofrecen flexibilidad y personalizaci칩n para desarrolladores y peque침os equipos.

- **Apache Nifi**  
  Herramienta de integraci칩n de datos visual para flujos ETL. Ideal para flujos en tiempo real y automatizaci칩n.

- **Apache Airflow**  
  Aunque no es una herramienta ETL tradicional, permite programar y orquestar pipelines ETL.

- **Pentaho Data Integration (PDI)**  
  Herramienta de c칩digo abierto que proporciona un enfoque visual para construir y ejecutar flujos ETL.

### **4. Herramientas de Orquestaci칩n de Datos**
Estas herramientas gestionan pipelines de datos m치s complejos, combinando ETL con otras funcionalidades.

- **Fivetran**  
  Automatiza la extracci칩n de datos y los carga en destinos populares como BigQuery, Snowflake o Redshift.

- **Stitch**  
  Una herramienta ligera para mover datos r치pidamente hacia almacenes de datos.

- **dbt (Data Build Tool)**  
  Aunque es m치s una herramienta ELT, ayuda a gestionar transformaciones SQL en almacenes de datos modernos.

### **5. Herramientas de Big Data y Procesamiento en Tiempo Real**
Dise침adas para manejar vol칰menes masivos de datos y ofrecer capacidades en tiempo real.

- **Apache Spark**  
  Plataforma de an치lisis distribuido que permite realizar ETL a gran escala con alta velocidad.

- **Kafka + Kafka Streams**  
  Para flujos ETL en tiempo real con mensajes entre sistemas distribuidos.

- **Databricks**  
  Plataforma basada en Apache Spark que permite construir pipelines ETL avanzados.

### **6. Servicios ETL Especializados**
Soluciones dise침adas para necesidades espec칤ficas de sectores o casos de uso.

- **Alteryx**  
  Enfocada en la integraci칩n y an치lisis de datos, ideal para usuarios que requieren an치lisis avanzado sin codificaci칩n extensa.

- **SAP Data Services**  
  Herramienta ETL orientada a la integraci칩n de datos empresariales en entornos SAP.

- **Boomi (Dell Boomi)**  
  Soluci칩n basada en la nube que facilita la integraci칩n de datos entre aplicaciones SaaS y locales.

### **7. Factores para Elegir una Herramienta ETL**
1. **Volumen de datos:** 쮼s un entorno peque침o o big data?  
2. **Compatibilidad:** 쯉e conecta f치cilmente a tus fuentes y destinos?  
3. **Rendimiento:** 쯇uede manejar la frecuencia y carga de datos?  
4. **Facilidad de uso:** 쮼s necesario programar o es m치s visual?  
5. **Costo:** Considera herramientas gratuitas frente a licencias comerciales.  
6. **Escalabilidad:** 쯉e adapta al crecimiento futuro de los datos?  

**Lecturas recomendadas**

[pandas - Python Data Analysis Library](https://pandas.pydata.org/)

[Curso B치sico de Manipulaci칩n y Transformaci칩n de Datos con Pandas y NumPy - Platzi](https://platzi.com/cursos/pandas-numpy/)

[Tutorial desde cero para dominar Pandas [Python]](https://platzi.com/blog/pandas/)

[Pentaho from Hitachi Vantara - Browse Files at SourceForge.net](https://sourceforge.net/projects/pentaho/files/)

[Pentaho Data Integration - Hitachi Vantara Lumada and Pentaho Documentation](https://help.hitachivantara.com/Documentation/Pentaho/7.0/0D0/Pentaho_Data_Integration)

[Descargar Pentaho](https://pentaho.com/pentaho-developer-edition/)

## Sources

El t칠rmino "sources" en el contexto de ETL y procesamiento de datos se refiere a las **fuentes de datos**. Estas fuentes son los or칤genes de la informaci칩n que se extrae para ser procesada y transformada dentro de los sistemas ETL. Pueden provenir de diferentes tipos de sistemas o bases de datos, tanto estructurados como no estructurados. A continuaci칩n, te explico algunos conceptos clave sobre las fuentes de datos en un proceso ETL, en espa침ol:

### Fuentes de Datos en un Proceso ETL:
1. **Bases de Datos Relacionales (RDBMS)**:
   Las bases de datos como **MySQL**, **PostgreSQL**, **Oracle** o **SQL Server** suelen ser fuentes comunes para los procesos ETL. Los datos extra칤dos de estas fuentes generalmente est치n estructurados y organizados en tablas, lo que facilita su extracci칩n.

2. **Archivos de Texto y CSV**:
   Archivos planos como **CSV**, **JSON**, **XML** o **TXT** son comunes en muchos procesos ETL. Estos archivos pueden contener datos en formato tabular o jer치rquico, pero requieren procesamiento para ser transformados en un formato adecuado para el an치lisis.

3. **APIs**:
   Las **APIs (Interfaces de Programaci칩n de Aplicaciones)** permiten acceder a datos de aplicaciones externas, como redes sociales, plataformas de comercio electr칩nico o sistemas de informaci칩n. Los datos extra칤dos a trav칠s de una API generalmente est치n en formato JSON o XML.

4. **Sistemas de Almacenamiento en la Nube**:
   Fuentes como **Amazon S3**, **Google Cloud Storage**, o **Azure Blob Storage** son muy utilizadas, ya que permiten almacenar grandes vol칰menes de datos no estructurados que se pueden extraer para su procesamiento ETL.

5. **Sistemas NoSQL**:
   Bases de datos NoSQL como **MongoDB**, **Cassandra**, o **CouchDB** son comunes cuando los datos no siguen una estructura r칤gida de tablas y relaciones. Estos sistemas pueden ser fuentes para datos semi-estructurados o no estructurados.

6. **Flujos de Datos en Tiempo Real**:
   Los sistemas que generan datos en tiempo real, como los sensores IoT, o las plataformas de streaming como **Apache Kafka**, pueden ser fuentes de datos para procesos ETL de transmisi칩n continua (streaming ETL), donde los datos son procesados en tiempo real en lugar de ser extra칤dos en lotes.

### Explicaci칩n de las Fuentes de Datos en el Contexto ETL:
Las **fuentes de datos** son un componente crucial en los procesos ETL (Extract, Transform, Load), ya que son el primer paso para obtener la informaci칩n necesaria para su an치lisis o almacenamiento. En este proceso:

- **Extract (Extracci칩n)**: Se extraen los datos de las fuentes. Aqu칤 es donde se encuentran las "sources" que alimentan el proceso.
- **Transform (Transformaci칩n)**: Los datos extra칤dos se limpian, se validan y se estructuran seg칰n sea necesario. 
- **Load (Carga)**: Finalmente, los datos transformados se cargan en un sistema de destino, como un **Data Warehouse** o un **Data Lake**, donde pueden ser utilizados para an치lisis y reportes.

### Ejemplos de Fuentes Comunes en la Industria:
- En la **industria financiera**, las fuentes de datos pueden incluir bases de datos de transacciones bancarias, sistemas de tarjetas de cr칠dito, APIs de pagos, etc.
- En la **industria de salud**, las fuentes pueden ser registros m칠dicos electr칩nicos, dispositivos m칠dicos, o incluso datos de investigaci칩n cient칤fica.
- En la **industria petrolera**, las fuentes de datos pueden ser sensores en los pozos de petr칩leo, sistemas de monitoreo de maquinaria, y bases de datos que contienen registros de producci칩n.

En resumen, las **sources** en un proceso ETL son todas aquellas plataformas, bases de datos, archivos, APIs o sistemas que proveen los datos que se van a extraer, transformar y cargar en el flujo de trabajo ETL. Estas fuentes son fundamentales porque determinan la calidad, cantidad y tipo de datos que se utilizar치n en los an치lisis posteriores.

## Configuraci칩n de base de datos source y entorno para ETL en Python

춰Hola! En esta clase conocer치s c칩mo configurar una base de datos con SQL, que ser치 una de las 3 fuentes para extraer datos en el proyecto de ETL. Las otras dos fuentes son un archivo JSON y otro CSV que conocer치s en clases posteriores.

Adem치s, conocer치s c칩mo conectarte a esta base de datos OLTP con un software de administraci칩n de bases de datos. Puede ser DataSpell, DBeaver o el de tu preferencia.

Te sugiero usar DataSpell. M치s adelante de este tutorial ver치s c칩mo configurarlo.

游눠Algo que tenemos que destacar es que la base de datos SQL source no se tendr칤a que crear en un proceso de ETL. Esta base de datos ya estar칤a creada en alg칰n lado de la infraestructura de los sistemas y aplicaciones de la empresa donde est칠s colaborando.

En este caso lo estamos haciendo por fines educativos para que tengas una base de datos de donde tomar datos y conozcas el proceso de extracci칩n.

Para la configuraci칩n de nuestra base de datos source usaremos PostgreSQL. Podemos utilizarlo de dos formas, una instalaci칩n local de PostgreSQL o una configuraci칩n por Docker. Te sugiero hacerlo por Docker.

### 1. Crear container en Docker

Recordemos que Docker es un entorno de gesti칩n de contenedores, de manera que usaremos una imagen base con toda la configuraci칩n que requerimos sin instalar necesariamente en nuestra m치quina. Solo utilizando los recursos del sistema para correr dicha imagen, algo similar a una m치quina virtual.

Por ahora, solo necesitas haber tomado el [Curso de Python: PIP y Entornos Virtuales](https://platzi.com/cursos/python-pip/ "Curso de Python: PIP y Entornos Virtuales") para conocer lo esencial de c칩mo usar esta herramienta con Python. En ese curso encontrar치s la[ clase para saber c칩mo instalarlo en tu computador](https://platzi.com/clases/4261-python-pip/55136-instalacion-de-docker/ " clase para saber c칩mo instalarlo en tu computador").

Una vez que tengas instalado Docker en tu computador, ejecuta este comando en tu terminal:

WSL 2, Linux o macOS

```bash
sudo docker run -d --name=postgres -p 5432:5432 -v postgres-volume:/var/lib/postgresql/data -e POSTGRES_PASSWORD=mysecretpass postgres
```

Windows

```bash
docker run -d --name=postgres -p 5432:5432 -v postgres-volume:/var/lib/postgresql/data -e POSTGRES_PASSWORD=mysecretpass postgres
```

Como podr치s notar, en este comando se espec칤fico lo siguiente para la creaci칩n de la base de datos con Docker:

- Nombre del container: `--name=postgres`
- Puerto a compartir con la m치quina local: `-p 5432:5432`
- Volumen para el manejo de disco e informaci칩n: `-v postgres-volume:/var/lib/postgresql/data`
- Password en PostgreSQL: `POSTGRES_PASSWORD=mysecretpass`

### 1.5 Instalaci칩n local de PostgreSQL (opcional)

De no usar Docker podr칤as ver la clase del curso de PostgreSQL en donde aprendes a instalarlo localmente, pero te sugiero intentarlo con Docker ya que puede agilizar tu flujo de trabajo. 游땔

### 2. Validar container creado

Una vez que hayas creado el container de Docker usa el comando `docker ps` en tu terminal. Podr치s ver todos los contenedores que se encuentran en ejecuci칩n actualmente y una descripci칩n.

Deber치s ver la IMAGE postgres.

![IMAGE postgres](images/etl01.png)

### 3. Configurar DataSpell

Para conectarte a la base de datos usar치s un software de administraci칩n de bases de datos. Existen varios que puedes utilizar. Para el seguimiento del curso te sugiero utilizar **DataSpell** o, en su defecto, **DBeaver**.

DataSpell es un **IDE** completo para ciencia de de datos donde, adem치s de conectarte y hacer consultas a bases de datos, podr치s ejecutar Jupyter Notebooks. 춰Todo en el mismo lugar! 游눩游낗

![DataSpell](images/dataspell.png)

游눠 Una de sus desventajas es que es de pago, pero tiene un per칤odo de prueba de 30 d칤as para que lo pruebes con este curso. Adem치s existen ciertas opciones para obtener [licencias para estudiantes de bachillerato y universidad](https://www.jetbrains.com/community/education/#students "licencias para estudiantes de bachillerato y universidad").

丘멆잺游붦 En caso de que decidas usar DBeaver en lugar de DataSpell, utiliza tu entorno local de Jupyter Notebooks con Anaconda para la ejecuci칩n del c칩digo Python de las siguientes clases. 游냀

### Instalaci칩n de DataSpell

1. Para instalar DataSpell ve a [su sitio web aqu칤](https://www.jetbrains.com/dataspell/ "su sitio web aqu칤") y descarga la versi칩n para tu sistema operativo.游닌

3. Inst치lalo siguiendo las instrucciones que te aparezcan en el instalador.

丘멆잺 Cuando te solicite actualizar PATH Variable acepta marcando la opci칩n que te indique. Esto es para evitar errores de ambientes en el futuro. En Windows se ve as칤:

![PATH](images/PATH.png)

Al finalizar te pedir치 reiniciar el computador:

![restar dataspell](images/restar_dataspell.png)

4. Abre DataSpell ya que se haya instalado. Al hacer esto por primera vez te pedir치 iniciar sesi칩n. Elige la versi칩n free trial registrando tu cuenta para ello.

5. Una vez que tengas tu cuenta configurada te pedir치 elegir un int칠rprete de Python 游냀.

Previamente deber치s tener instalado **Anaconda** en tu sistema operativo. Te recomiendo que crees un **ambiente de Anaconda** (**Conda environment**) 칰nico para el proyecto del curso. Llama al ambiente `fundamentos-etl`.

Elige el ambiente de Anaconda que usar치s para el proyecto y presiona el bot칩n Launch DataSpell.

![Welcome to DataSpel](images/WelcometoDataSpell.png)

Elegir un int칠rprete de Anaconda servir치 para ejecutar Jupyter Notebooks en DataSpell.

6. Crea un nuevo Workspace en **DataSpell**. Presiona el bot칩n File en la barra superior y luego elige la opci칩n New Workspace Directory.

![new workspace directory](images/new_workspace_directory.png)

Llama` fundamentos-etl` al workspace y presiona el bot칩n azul **Create**.

![new etl workspace](images/new_etl_workspace.png)

### Elegir ambiente de WSL2 (opcional si usas WSL)

Si quieres usar DataSpell con tu entorno en Windows con WSL 2, deber치s conectar DataSpell al ambiente de Anaconda que tenga tu WSL.游냀

0. Crea un ambiente de Anaconda en tu WSL dedicado al proyecto de tu curso si todav칤a no lo has hecho. Ll치malo fundamentos-etl

`conda create --name fundamentos-etl python=3.9`

1. Despu칠s ve a DataSpell en su parte inferior donde aparece el int칠rprete. Presiona la direcci칩n que aparece y elige la opci칩n **Interpreter Settings**.

![elegir interprete dataspell](images/elegir_interprete_dataspell.png)

2. Escoge el workspace `fundamentos-etl` creado anteriormente en DataSpell.

丘멆잺OJO: el workspace y el Anaconda Environment no son lo mismo. El Anaconda Environment lo vamos a cargar dentro del Workspace de DataSpell.

Despu칠s presiona el bot칩n **Add Interpreter** e inmediatamente selecciona la opci칩n **On WS**L.

![elegir_interprete_wsl_dataspell.png](images/elegir_interprete_wsl_dataspell.png)

3. Elige la distribuci칩n de Linux a usar y da clic en el bot칩n Next cuando aparezca el mensaje "Instrospection completed succesfully!

![ws instrospection](images/wsl instrospection.png)

4. Elige el int칠rprete a usar. Este puede ser un Virtualvenv Environment, el System Interpreter o un Conda Environment. Elige la opci칩n de Conda Environment.

![select interpreter.png](images/select_interpreter.png)

5. Mara la casilla **Use existing environment**. Elige el Conda Environment de WSL que usar치s para tu proyecto. Anteriormente debiste crearlo desde tu terminal en WSL y llamarlo `fundamentos-etl`.

Finalmente, presiona el bot칩n azul **Create**.

![conda fundamentos etl environment](images/conda-fundamentos-etl-environment.png)

6. Para terminar el proceso presiona el bot칩n azul OK en la parte inferior.

![workspace](images/workspace.png)

7. Listo, ya deber치 aparecer tu entorno de Anaconda en WSL cargado en la parte inferior de DataSpell.

![ambiente cargado](images/ambiente-cargado.png)

丘멆잺Si te aparece un error que indique que el ambiente no puede ser usado como el int칠rprete del workspace es porque est치s intentando cargar el ambiente en el workspace general y no en un workspace de DataSpell que creaste.

[Aqu칤](https://www.jetbrains.com/help/dataspell/using-wsl-as-a-remote-interpreter.html "Aqu칤") encuentras la gu칤a oficial de c칩mo conectar tu DataSpell al int칠rprete de Python o Anaconda en WSL, por si necesitas aprender a configurarlo a detalle.

Recuerda que otra alternativa en Windows es instalar [Anaconda para Windows](https://www.anaconda.com/products/distribution "Anaconda para Windows") y conectar DataSpell directamente a esta versi칩n.

### 4. Conexi칩n a la base de datos PostgreSQL

Sigue estos pasos para conectarte a la base de datos postgres desde DataSpell.

1. Abre DataSpell en tu computador.

![data spell](images/dataspells.png)

2. Ve a la pesta침a de **Database** y en ella da clic en el **bot칩n de signo de +**.

![database dataspell.png](images/database_dataspell.png)

3. Selecciona la opci칩n de **Data Source** y dentro del men칰 desplegable elige la opci칩n de **PostgreSQL**.

![workspace](images/workspace25.png)

4. Introduce los datos siguientes en la conexi칩n:

- **Name**: local_postgres
- **Host**: localhost
- **Port**: 5432
- **User**: postgres
- **Database**: postgres
- **Url (opcional)**: jdbc:postgresql://localhost:5432/postgres
- **Password**: mysecretpass

5. Da clic en el bot칩n de Test Connection para probar la conexi칩n. Puede que te solicite actualizar unos drivers, ac칠ptalos. Una vez que indique que la conexi칩n es exitosa, da clic en el bot칩n OK.

![etl 02](images/etl02.png)

6. Listo, ya tienes tu base de datos conectada en DataSpell.

![postgres data spell](images/postgresdataspell.png)

### 4. Cargar datos en la base de datos Postgres

Dentro de DataSpell, ya con la conexi칩n a la base de datos previamente creada, ejecutar치s el script ***postgres_public_trades.sql***.

Desc치rgalo [aqu칤 de Google Drive](https://drive.google.com/file/u/2/d/19U7l0kp3mEh8SYYG6BjoDp0kVPYWDsqI/view?usp=share_link "aqu칤 de Google Drive"). 游닌

丘멆잺Este archivo pesa cerca de 500 MB, por lo que puede demorar su descarga. Contiene la creaci칩n de una tabla llamada trades y los insert de registros de la tabla.

丘멆잺Es posible que al intentar correr este script en **DBeaver** no sea posible por falta de memoria. Te sugerimos cortarlo en varias partes y cargar cada script independientemente.

![etl 03](images/etl031.png)

Una vez descargado el archivo **postgres_public_trades.sql** sigue estos pasos para cargar los datos con DataSpell:

1. Da clic derecho sobre la base de datos de PostgreSQL.

![etl 04](images/etl04.png)

2. Posteriormente da clic en SQL Script y luego en Run SQL Scripts.

![dataspell run sql script](images/dataspell_run_sql_script.png)

3. Ubica el script descargado dentro de tu computador y da clic en OK.

![etl 06](images/etl06.png)

丘멆잺La creaci칩n de la tabla y la carga de datos puede demorar cerca de 15-20 minutos en DataSpell.

![script sql done](images/script_sql_done.png)

### 5. Prueba la tabla trades

Una vez terminada la ejecuci칩n del script, consulta la tabla Trades ya cargada. Abre el editor de queries desde tu base de datos en DataSpell e ingresa la siguiente consulta:

`SELECT * FROM trades;`

![etl 07](images/etl07.png)

춰Listo! Ya tienes lo esencial para comenzar a extraer datos de una base de datos OLTP y correr tus notebooks de Python.

Avanza a la siguiente clase. 丘뙖잺

## Extracci칩n de datos con Python y Pandas

La **extracci칩n de datos** con Python y Pandas es una pr치ctica com칰n en el an치lisis de datos y en procesos ETL (Extract, Transform, Load). **Pandas** es una biblioteca poderosa que permite manipular y analizar datos estructurados f치cilmente. A continuaci칩n, se describen los m칠todos m치s utilizados para extraer datos con Pandas:

---

## 1. **Extracci칩n desde archivos comunes**

### a) Archivos CSV
```python
import pandas as pd

# Cargar un archivo CSV
df = pd.read_csv('archivo.csv')

# Mostrar las primeras filas
print(df.head())
```

### b) Archivos Excel
```python
# Cargar un archivo Excel
df = pd.read_excel('archivo.xlsx', sheet_name='Hoja1')

# Mostrar resumen de datos
print(df.info())
```

### c) Archivos de texto delimitados
```python
# Cargar archivo con delimitadores personalizados (ejemplo: tabulaci칩n)
df = pd.read_csv('archivo.txt', delimiter='\t')

# Mostrar estad칤sticas descriptivas
print(df.describe())
```

---

## 2. **Extracci칩n desde Bases de Datos**

Pandas puede conectarse a bases de datos relacionales como **MySQL**, **PostgreSQL** y otras utilizando bibliotecas como `SQLAlchemy`.

### Ejemplo con SQLite
```python
import sqlite3

# Conectar a la base de datos
conn = sqlite3.connect('base_de_datos.db')

# Ejecutar una consulta y cargar los datos en un DataFrame
query = "SELECT * FROM tabla"
df = pd.read_sql_query(query, conn)

# Cerrar conexi칩n
conn.close()

print(df)
```

### Ejemplo con MySQL y SQLAlchemy
```python
from sqlalchemy import create_engine

# Crear conexi칩n
engine = create_engine('mysql+pymysql://usuario:contrase침a@host/nombre_base_datos')

# Ejecutar consulta
query = "SELECT * FROM tabla"
df = pd.read_sql(query, engine)

print(df)
```

---

## 3. **Extracci칩n desde APIs**

Pandas puede trabajar con datos obtenidos de APIs, que usualmente est치n en formato **JSON**.

### Ejemplo con `requests`
```python
import pandas as pd
import requests

# Realizar la solicitud
response = requests.get('https://api.ejemplo.com/data')
data = response.json()

# Convertir a DataFrame
df = pd.json_normalize(data)

print(df.head())
```

---

## 4. **Extracci칩n desde fuentes en la nube**

### a) Desde Amazon S3
```python
import boto3
import pandas as pd

# Configurar cliente S3
s3 = boto3.client('s3')

# Descargar archivo
s3.download_file('mi-bucket', 'ruta/al/archivo.csv', 'archivo_local.csv')

# Leer el archivo descargado
df = pd.read_csv('archivo_local.csv')

print(df)
```

### b) Desde Google Sheets
Utilizando la API de Google Sheets.
```python
import pandas as pd
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# Configurar las credenciales
scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
credentials = ServiceAccountCredentials.from_json_keyfile_name('credenciales.json', scope)
client = gspread.authorize(credentials)

# Obtener hoja de c치lculo
sheet = client.open("nombre_hoja").sheet1

# Convertir a DataFrame
data = sheet.get_all_records()
df = pd.DataFrame(data)

print(df.head())
```

---

## 5. **Extracci칩n desde flujos en tiempo real**

Pandas no est치 dise침ado para datos en tiempo real, pero puede integrarse con herramientas como **Kafka** o **Spark** para leer datos en tiempo real y transformarlos.

---

## Recomendaciones:
1. **Validaci칩n de datos:** Verificar datos faltantes o inconsistencias despu칠s de la extracci칩n usando m칠todos como `df.isnull().sum()`.
2. **Eficiencia:** Si trabajas con datos grandes, considera usar `chunksize` al leer archivos o bases de datos.
   ```python
   for chunk in pd.read_csv('archivo_grande.csv', chunksize=1000):
       print(chunk.head())
   ```

Con estas t칠cnicas, puedes extraer datos desde diversas fuentes y trabajar con ellos eficientemente en tus proyectos de ingenier칤a de datos. 游땕

**Lecturas recomendadas**

[template_ETL_OEC.ipynb - Google Drive](https://drive.google.com/file/d/1P5kQo5_0bkzLNakbBwlf-9evT9PYtae_/view?usp=share_link)

Curso de Python: PIP y Entornos Virtuales - Platzi

[JetBrains DataSpell: The IDE for Data Scientists](https://www.jetbrains.com/dataspell/)

[country_data.json - Google Drive](https://drive.google.com/file/d/19QM_nHhUZ4s3yZcV7ePis5mD1dTtCl9Z/view?usp=share_link)

[hs_codes.csv - Google Drive](https://drive.google.com/file/d/1C6EwxoQmROiC27gvTsUNBVPhrtfH8mDI/view?usp=share_link)

[postgres_public_trades.sql - Google Drive](https://drive.google.com/file/d/19U7l0kp3mEh8SYYG6BjoDp0kVPYWDsqI/view?usp=share_link)

[guia_ETL_OEC.ipynb - Google Drive](https://drive.google.com/file/d/1LjmlMnpajBsNnTqWozh8T-5NpbetIzO_/view?usp=share_link)

## Transformaci칩n

En el contexto de **ETL (Extract, Transform, Load)**, la **Transformaci칩n** es la etapa intermedia del proceso. Su prop칩sito principal es convertir los datos extra칤dos desde las fuentes en un formato adecuado para ser almacenados y utilizados en el sistema de destino, como un Data Warehouse.  

### Principales tareas en la Transformaci칩n:
1. **Limpieza de datos**:  
   - Eliminar duplicados.
   - Gestionar valores faltantes.
   - Corregir errores en los datos.

2. **Estandarizaci칩n**:  
   - Asegurar que los datos tienen el mismo formato y estructura.
   - Por ejemplo, unificar formatos de fechas o convertir unidades.

3. **Enriquecimiento de datos**:  
   - Combinar datos de m칰ltiples fuentes para agregar m치s contexto o informaci칩n 칰til.

4. **C치lculos y derivaciones**:  
   - Crear nuevas columnas o m칠tricas a partir de los datos existentes, como calcular ingresos netos o m치rgenes de ganancia.

5. **Filtrado**:  
   - Seleccionar solo los datos relevantes o necesarios para la aplicaci칩n objetivo.

6. **Validaci칩n**:  
   - Verificar que los datos transformados cumplen con las reglas de negocio y est치ndares requeridos.

7. **Agrupaci칩n y agregaci칩n**:  
   - Resumir datos, como calcular promedios, totales, o conteos por categor칤as.

### Ejemplo en Python con Pandas:
Supongamos que extraemos datos de ventas de un archivo CSV y necesitamos realizar transformaciones b치sicas.

```python
import pandas as pd

# Cargar datos
data = pd.read_csv("ventas.csv")

# Limpieza: Eliminar duplicados
data = data.drop_duplicates()

# Estandarizaci칩n: Convertir fechas al mismo formato
data['fecha'] = pd.to_datetime(data['fecha'], format='%Y-%m-%d')

# Enriquecimiento: Calcular total de ventas
data['total_ventas'] = data['precio_unitario'] * data['cantidad']

# Filtrar: Seleccionar datos relevantes
data = data[data['total_ventas'] > 1000]

print(data.head())
```

En este ejemplo, se realizan tareas comunes de transformaci칩n antes de cargar los datos transformados en un destino.  

La transformaci칩n es crucial porque asegura que los datos sean consistentes, precisos y 칰tiles para el an치lisis.

## Transformaci칩n de datos de transacciones

La **transformaci칩n de datos de transacciones** es un paso clave dentro de un proceso de **ETL** (Extracci칩n, Transformaci칩n y Carga) en el que los datos se procesan para que sean m치s 칰tiles y adecuados para su an치lisis posterior. En el contexto de las transacciones financieras o de ventas, este paso implica la conversi칩n de datos crudos provenientes de diferentes fuentes en un formato m치s estandarizado, limpio y estructurado. Aqu칤 te dejo algunos aspectos clave y ejemplos de c칩mo se realiza la transformaci칩n de datos de transacciones:

### 1. **Limpieza de Datos**
   La limpieza es fundamental para asegurar que no haya errores en los datos antes de cargarlos a la base de datos o al sistema de an치lisis.

   - **Eliminar registros duplicados**: Si tienes registros de transacciones duplicados, necesitar치s eliminarlos.
   - **Rellenar valores nulos**: Algunas transacciones pueden tener valores faltantes, como un monto o una fecha. Dependiendo de las reglas del negocio, podr칤as decidir rellenar estos valores con un valor predeterminado o eliminarlos.
   - **Formato de fechas**: Es posible que las fechas de las transacciones vengan en diferentes formatos (por ejemplo, `DD/MM/YYYY` o `MM-DD-YYYY`). Se deben estandarizar en un formato 칰nico.

   **Ejemplo en Python (Pandas)**:
   ```python
   import pandas as pd

   # Eliminar duplicados
   df = df.drop_duplicates(subset=["transaction_id"])

   # Rellenar valores nulos
   df["transaction_amount"].fillna(0, inplace=True)

   # Convertir fechas
   df["transaction_date"] = pd.to_datetime(df["transaction_date"], format="%Y-%m-%d")
   ```

### 2. **Agregaci칩n de Datos**
   Las transacciones a menudo deben agregarse para obtener m칠tricas clave como el total de ventas por d칤a, el total de transacciones por usuario, etc. Esto se hace mediante operaciones como la suma, el promedio, el conteo, etc.

   - **Total de ventas diarias**: Si cada transacci칩n tiene un monto asociado, puedes agregar las ventas por d칤a.
   - **Transacciones por usuario**: Puedes contar el n칰mero de transacciones realizadas por cada cliente o usuario.

   **Ejemplo en Python (Pandas)**:
   ```python
   # Total de ventas por d칤a
   df_daily_sales = df.groupby("transaction_date")["transaction_amount"].sum().reset_index()

   # N칰mero de transacciones por cliente
   df_transactions_per_user = df.groupby("user_id")["transaction_id"].count().reset_index()
   ```

### 3. **Normalizaci칩n y Estandarizaci칩n**
   Los datos de transacciones pueden tener diferentes unidades o escalas. Es importante estandarizar estos valores para que sean consistentes.

   - **Normalizaci칩n de montos**: Si tienes transacciones en diferentes monedas, deber칤as convertirlas a una moneda com칰n.
   - **Transformar categor칤as**: Las categor칤as de productos o servicios pueden tener diferentes etiquetas (por ejemplo, "Electr칩nica", "Electrodom칠sticos", "Tech"). Puedes agruparlos bajo categor칤as estandarizadas.

   **Ejemplo en Python (Pandas)**:
   ```python
   # Convertir montos a una moneda com칰n (suponiendo una tasa de cambio de 1 USD = 0.85 EUR)
   df["transaction_amount_usd"] = df["transaction_amount"] * 0.85

   # Estandarizar categor칤as de productos
   category_map = {"Electrodom칠sticos": "Electronics", "Tech": "Electronics"}
   df["product_category"] = df["product_category"].replace(category_map)
   ```

### 4. **C치lculo de M칠tricas Derivadas**
   Las m칠tricas derivadas son c치lculos adicionales basados en los datos de transacciones que pueden ayudar a tomar decisiones o hacer an치lisis.

   - **Monto de transacciones por usuario**: Calcular cu치nto ha gastado cada usuario.
   - **Margen de beneficio**: Si tienes datos sobre el costo y el precio de los productos, puedes calcular el margen de beneficio.

   **Ejemplo en Python (Pandas)**:
   ```python
   # Calcular el monto total por usuario
   df_user_spending = df.groupby("user_id")["transaction_amount"].sum().reset_index()

   # Calcular margen de beneficio
   df["profit_margin"] = (df["transaction_amount"] - df["cost_amount"]) / df["transaction_amount"]
   ```

### 5. **Enriquecimiento de Datos**
   A veces es necesario enriquecer los datos de transacciones con informaci칩n adicional que provenga de otras fuentes. Esto podr칤a incluir detalles sobre el usuario, ubicaci칩n, productos o promociones.

   - **Datos de cliente**: Puedes agregar informaci칩n sobre los clientes, como el nombre, la ubicaci칩n o su nivel de fidelidad.
   - **Categor칤as de productos**: Si tienes una lista de productos con su categor칤a, puedes a침adirla a cada transacci칩n.

   **Ejemplo en Python (Pandas)**:
   ```python
   # Suponiendo que tienes un DataFrame con informaci칩n de clientes
   df_customers = pd.DataFrame({"user_id": [1, 2], "customer_name": ["Alice", "Bob"]})

   # Unir la informaci칩n de los clientes con las transacciones
   df = pd.merge(df, df_customers, on="user_id", how="left")
   ```

### 6. **Formato de Salida**
   Finalmente, los datos de transacciones transformados deben estar en el formato adecuado para su almacenamiento o an치lisis posterior. Pueden almacenarse en bases de datos, archivos CSV, formatos como JSON o Parquet, entre otros.

   **Ejemplo en Python (Pandas)**:
   ```python
   # Guardar los datos transformados en un archivo CSV
   df.to_csv("transacciones_transformadas.csv", index=False)
   ```

### Resumen del Proceso de Transformaci칩n de Datos de Transacciones:
1. **Limpieza de datos**: Eliminar duplicados, rellenar valores nulos, convertir fechas.
2. **Agregaci칩n de datos**: Sumar transacciones por fecha, contar transacciones por usuario.
3. **Normalizaci칩n**: Convertir unidades (como moneda) y estandarizar categor칤as.
4. **C치lculo de m칠tricas derivadas**: Calcular m칠tricas adicionales como el gasto total por cliente o el margen de beneficio.
5. **Enriquecimiento de datos**: Agregar datos adicionales como la informaci칩n del cliente.
6. **Exportaci칩n y almacenamiento**: Guardar los datos en el formato deseado (CSV, base de datos, etc.).

Este proceso de transformaci칩n es clave para preparar los datos para su an치lisis o para generar informes de negocio confiables.

**Lecturas recomendadas**

[country_data.json - Google Drive](https://drive.google.com/file/d/19QM_nHhUZ4s3yZcV7ePis5mD1dTtCl9Z/view?usp=share_link)

[template_ETL_OEC.ipynb - Google Drive](https://drive.google.com/file/d/1P5kQo5_0bkzLNakbBwlf-9evT9PYtae_/view?usp=share_link)

## Carga

En el contexto de un proceso ETL (Extract, Transform, Load), la **carga (Load)** se refiere al paso final en el que los datos transformados se cargan en el sistema de destino o en el almac칠n de datos (Data Warehouse). Este paso es crucial porque los datos deben estar disponibles para su an치lisis o uso en el sistema al que se dirigen.

### Tipos de carga de datos:
1. **Carga completa (Full Load)**:
   - Se cargan todos los datos desde cero. Es 칰til cuando el dataset es peque침o o cuando se necesita reemplazar completamente los datos existentes en el sistema de destino.
   
2. **Carga incremental (Incremental Load)**:
   - Solo se cargan los datos nuevos o modificados desde la 칰ltima carga. Esto es eficiente para datasets grandes, ya que solo se agregan cambios, no todo el conjunto de datos.

3. **Carga en tiempo real (Real-Time Load)**:
   - Los datos se cargan en tiempo real, lo que significa que se actualizan casi inmediatamente despu칠s de que los datos son transformados. Es 칰til en aplicaciones que requieren datos actualizados constantemente.

### Consideraciones en la carga de datos:
- **Consistencia**: Asegurarse de que los datos en el sistema de destino est칠n completos y sin errores.
- **Performance**: La carga de grandes vol칰menes de datos debe realizarse de manera eficiente para evitar bloqueos o cuellos de botella en el sistema de destino.
- **Automatizaci칩n**: La carga debe ser un proceso automatizado, ejecutado a intervalos regulares o en funci칩n de cambios en los datos.

## Configuraci칩n de cl칰ster en AWS Redshift

춰Hola, te doy la bienvenida! Me da gusto encontrarte en este curso. Soy [Alexis](https://platzi.com/profes/alexinaraujo/ "Alexis"), profesor de AWS en Platzi.

Te acompa침ar칠 para crear un cl칰ster y su base de datos en **AWS Redshift** y un **bucket en S3**. Los usaremos para el target del ETL de este curso.

Antes de continuar, quiero recordarte que es importante que tengas el conocimiento de lo que aprendemos en estos dos cursos donde tengo el gusto de ser tu profe:

- [Curso de Introducci칩n a AWS: Fundamentos de Cloud Computing](https://platzi.com/cursos/aws-fundamentos/ "Curso de Introducci칩n a AWS: Fundamentos de Cloud Computing")
- [Curso de Introducci칩n a AWS: C칩mputo, Almacenamiento y Bases de Datos](https://platzi.com/cursos/aws-computo/ "Curso de Introducci칩n a AWS: C칩mputo, Almacenamiento y Bases de Datos")

Te servir치 para que este proceso te sea mucho m치s familiar y para que ya tengas creada tu cuenta de AWS.

Si ya lo tienes sigue estos pasos para configurar el target de tu ETL.

### Entra a AWS Free Tier

1. Abre el sitio web de AWS Free Tier aqu칤.
丘멆잺Es importante recordar que el free tier tiene un per칤odo de 1 a침o para que no te cobre AWS.

2. Dentro del buscador de detalles de nivel gratuito busca **redshift**.

![aws free](images/aws_free.png)

3. Observa que podr치s tener una prueba gratuita de 2 meses de AWS Redshift al elegir un nodo tipo DC2.Large

![redshift free](images/redshift_free.png)

![DC2 large aws](images/DC2_large_aws.png)

丘멆잺丘멆잺丘멆잺 Recuerda que es muy muy importante que elijas ese tipo de nodo DC2.Large al crear el cl칰ster de AWS Redshift. Para que sea gratuito por 2 meses, de lo contrario se te har치n cobros a tu tarjeta.

Ya que sabes esto, avanza a crear el cl칰ster en los siguientes pasos.

### Creaci칩n de cl칰ster en AWS Redshift

### 1. Inicia sesi칩n en la consola de AWS

Da clic en iniciar sesi칩n en la parte superior derecha o [aqu칤](http://console.aws.amazon.com/ "aqu칤").

![inicia sesion aws](images/inicia_sesion_aws.png)

### 2. Elige tu regi칩n

Despu칠s de iniciar sesi칩n en tu cuenta de AWS elige la regi칩n donde crear치s el cl칰ster en la parte superior derecha de la consola de AWS.

![consola region aws](images/consola_region_aws.png)

Te sugiero elegir la regi칩n de Oregon (us-west-2) que es la que usamos en este curso, pero puedes seleccionar la que mejor te convenga. Solo recuerda cual es para que la pongas al cargar los datos desde la notebook de Python.

### 3. Buscar el servicio de Redshift

Busca redshift en el cuadro de b칰squeda de servicios y da clic en la opci칩n **Amazon Redshift** que aparece.

![buscar redshift](images/buscar_redshift.png)

Esto te llevar치 a la consola de Redshift.

![redshift console](images/redshift_console.png)

### 4. Ve al panel de cl칰steres

Dentro de la consola de Redshift ve al panel de la izquierda que se despliega con el 칤cono de las tres rayas horizontales. Selecciona la opci칩n de **Cl칰steres**.

![clusteres redshift](images/clusteres_redshift.png)

### 5. Crea el cl칰ster

Dentro del panel de cl칰steres da clic en el bot칩n naranja **Crear cl칰ster**.

![rear cluster redshift](images/rear_cluster_redshift.png)

En la configuraci칩n del cl칰ster da nombre al cl칰ster. Puedes llamarlo demo-platzi-curso-etl. Elige la prueba gratuita.

En el resumen de configuraci칩n calculada deber치s ver que el tipo de nodo sea dc2.large. 
丘멆잺Recuerda que esto es importante de verificar para que sea gratuito el uso de tu Redshift durante 2 meses.

![free cluster](images/free_cluster.png)

Una vez seleccionados estos campos, desciende para establecer el usuario y contrase침a del cl칰ster de Redshift.

丘멆잺Recuerda guardar en un lugar seguro estas credenciales, como en un gestor de contrase침as, para que puedas conectarte al cl칰ster.

Nombra **demoplatzi** al **Nombre de usuario** y pon una contrase침a segura.

![admin pass cluster](images/admin_pass_cluster.png)

Finalmente, da clic en el bot칩n naranja **Crear cl칰ster**.

꺁Espera hasta que el estado de creaci칩n del cl칰ster lo marque en verde como Available. Esto puede demorar varios minutos, para revisar el estado da clic en el bot칩n refrescar de la flecha en c칤rculo.

![cluster listo](images/cluster_listo.png)

### 6. Modificar las reglas de seguridad del cl칰ster

Entra al cl칰ster dando clic en el nombre dentro del panel de cl칰steres.

![cluster listo](images/cluster_listo1.png)

Entra a la pesta침a de **Propiedades**.

![cluster listo 1](images/propiedades.png)

Baja hasta la secci칩n de Configuraci칩n de **red** y **seguridad** y da clic sobre el **Grupo de seguridad de la VPC**.

![VPC sec](images/VPC_sec.png)

Selecciona el **ID del grupo de seguridad**.

![id grupo seguridad](images/id_grupo_seguridad.png)

Baja y da clic en **Editar reglas de entrada**.

![reglas_de_entrada.png](images/reglas_de_entrada.png)

Da clic en el bot칩n inferior **Agregar regla**

![agregar regla](images/agregar_regla.png)

En tipo elige Redshift y en **origen** elige 0.0.0.0/0. Finalmente, da clic en el bot칩n naranja **Guardar** reglas.

![nueva regla entrada](images/nueva_regla_entrada.png)

Regresa al panel del cl칰ster a la zona de configuraci칩n de red y seguridad. Da clic en el bot칩n editar.

![editar seguridad](images/editar_seguridad.png)

Desciende y en la parte inferior marca la casilla **Activar accesibilidad p칰blica**. Da clic en el bot칩n naranja **Guardar cambios**.

![red seguridad publica](images/red_seguridad_publica.png)

丘멆잺Esto es algo que no debe hacerse en entornos de producci칩n. En este caso lo har치s al ser un demo con fines educativos, para evitar complicaciones de configuraci칩n adicional de accesos de seguridad.

### 7. Con칠ctate a Redshift desde tu gestor de bases de datos.

Abre tu gestor, ya sea DataSpell, DBeaver o pgAdmin.

Dentro de tu gestor crea una nueva conexi칩n a una base de datos de tipo Redshift. Es muy importante buscar la opci칩n de Redshift.

En el caso de DataSpell ve a la pesta침a de Database y en ella da clic en el bot칩n de signo de +.

![database dataspell1](images/database_dataspell1.png)

Selecciona la opci칩n de **Data Source** y dentro del men칰 desplegable busca y elige la opci칩n de Amazon Redshift.

workspace.png
![workspace 1](images/workspace1.png)

Regresa al panel del administrador del cl칰ster de Redshift en la consola de AWS y copia el punto de enlace.

![copiar punto enlace](images/copiar-punto-enlace.png)

Regresa a la interfaz de tu gestor de bases de datos e ingresa los los siguientes datos para conectar a la base de datos:

- **Host**: es la url del punto de enlace que copiaste eliminando la parte final :5439/dev. Tendr치 una forma como [server.redshift.amazonaws.com](http://server.redshift.amazonaws.com/ "server.redshift.amazonaws.com")
- **Port**: 5439
- **User**: demoplatzi o el que hayas puesto.
- **Password**: la que le hayas puesto a tu cl칰ster de AWS Redshift cuando lo creaste en el paso 5.
- **Database**: dev

![database con redshift](images/database_con_redshift.png)

Da clic en el bot칩n de **Test Connection** o su correspondiente para probar la conexi칩n. Si est치s en DataSpell te pedir치 actualizar unos drivers, dile que OK.

Una vez que indique que la conexi칩n es exitosa, da clic en el **bot칩n OK**, o en el bot칩n disponible de otro gestor, para aceptar la conexi칩n.

![conexion redshift ok](images/conexion_redshift_ok.png)

춰Listo tienes creado tu cl칰ster con una base de datos dev en Redshift y te has conectado a ella! 游

![conexi칩n redshift](images/conexi칩n_redshift.png)

### Creaci칩n de bucket de almacenamiento en AWS S3

Para el ETL crear치s un bucket en S3 donde temporalmente almacenar치s unos archivos CSV donde guardar치s los datos de las tablas que has creado en el proceso de transformaci칩n.

### 1. Entra al panel de S3 desde la consola de AWS.

Busca **S3** en el buscador de la consola y selecciona la opci칩n de S3.

![s3](images/s3.png)

Da clic en el bot칩n naranja **Crear bucket** en el panel de S3.

![panel s3](images/panel-s3.png)

Da un nombre al bucket. Este nombre debe ser 칰nico, ya que no puede haber m치s de un bucket de S3 con el mismo nombre. Tambi칠n aseg칰rate que la regi칩n de AWS sea la misma que hayas elegido para tu cl칰ster de Redshift, en nuestro caso fue **us-west-2**.

![crear bucket 3](images/crear_bucket_s3.png)

Desciende hasta abajo y da clic en el bot칩n naranja **Crear bucket**.

![boton crear bucket](images/boton-crear-bucket.png)

춰Listo, ya deber치 aparecer que tienes tu bucket creado! 游녪游낗

![bucket creado](images/bucket_creado.png)

Avanza a la siguiente clase para crear las tablas donde cargar치s los datos de los archivos CSV que crear치s durante el proceso de carga y donde configurar치s ciertas variables de entorno por seguridad. 丘뙖잺俱뫮잺

Recomiendo utilizar variables de entorno con nombres personalizados, ya que por ejemplo la variable de entorno USER es el nombre de usuario de tu sistema operativo en el caso de linux. Yo lo Personalic칠 as칤:

```bash
export AWS_ACCESS_KEY_ID=********
export AWS_SECRET_ACCESS_KEY=******
export AWS_HOST="tu host"
export AWS_DATABASE=dev
export AWS_USER=demoplatzi
export AWS_PASSWORD=******
```

## Carga de datos con Python

La **carga de datos con Python** es el paso final en un proceso de ETL (Extracci칩n, Transformaci칩n y Carga). En este proceso, los datos ya transformados se insertan en el sistema de destino, como un Data Warehouse, una base de datos relacional o un servicio en la nube. Aqu칤 te explico c칩mo realizar la carga utilizando Python y la librer칤a `pandas` en combinaci칩n con `SQLAlchemy` y `psycopg2`.

### Pasos para cargar datos:

#### 1. **Preparar los datos**
Los datos deben estar en un formato estructurado (como un DataFrame de pandas) y listos para cargarse en la base de datos destino.

```python
import pandas as pd

# Datos de ejemplo
data = {
    'product_id': [1, 2, 3],
    'product_name': ['Laptop', 'Mouse', 'Keyboard'],
    'sales': [1200, 300, 450]
}

df = pd.DataFrame(data)
```

#### 2. **Configurar la conexi칩n**
Usa `SQLAlchemy` para conectarte a la base de datos. Configura correctamente el **string de conexi칩n** con las credenciales y el host de tu base de datos.

```python
from sqlalchemy import create_engine

# String de conexi칩n para una base de datos PostgreSQL o Amazon Redshift
engine = create_engine("postgresql+psycopg2://username:password@host:port/database")
```

Reemplaza:
- `username`: Nombre de usuario
- `password`: Contrase침a
- `host`: Direcci칩n del servidor
- `port`: Puerto (por ejemplo, `5439` para Redshift)
- `database`: Nombre de la base de datos

#### 3. **Escribir los datos en la base**
Utiliza el m칠todo `to_sql` de pandas para insertar el DataFrame en una tabla.

```python
# Cargar los datos en la base de datos
df.to_sql(
    name='sales_data',         # Nombre de la tabla destino
    con=engine,                # Conexi칩n a la base de datos
    if_exists='replace',       # Qu칠 hacer si la tabla ya existe ('replace', 'append', 'fail')
    index=False                # Si no quieres cargar el 칤ndice del DataFrame como columna
)

print("Datos cargados correctamente.")
```

#### 4. **Verificar la carga**
Consulta la base de datos para asegurarte de que los datos se han cargado correctamente.

```python
# Leer los datos para verificar
df_loaded = pd.read_sql("SELECT * FROM sales_data", con=engine)
print(df_loaded)
```

### Ejemplo completo:

```python
import pandas as pd
from sqlalchemy import create_engine

# Preparar los datos
data = {
    'product_id': [1, 2, 3],
    'product_name': ['Laptop', 'Mouse', 'Keyboard'],
    'sales': [1200, 300, 450]
}
df = pd.DataFrame(data)

# Configurar conexi칩n
engine = create_engine("postgresql+psycopg2://username:password@host:port/database")

# Cargar datos en la base de datos
df.to_sql(
    name='sales_data',
    con=engine,
    if_exists='replace',
    index=False
)

# Verificar los datos cargados
df_loaded = pd.read_sql("SELECT * FROM sales_data", con=engine)
print(df_loaded)
```

### Recomendaciones:
1. **Validaci칩n previa**: Antes de cargar los datos, revisa que no contengan valores nulos o inconsistencias.
2. **Estrategia de carga**:
   - `replace`: Sobrescribe la tabla si ya existe.
   - `append`: Agrega los datos nuevos sin eliminar los existentes.
3. **Manejo de errores**: Implementa bloques `try-except` para capturar errores en la conexi칩n o la carga.

Con este enfoque puedes integrar datos a bases de datos de manera confiable en el contexto de un proceso ETL.

**Lecturas recomendadas**

[Examples of using the Amazon Redshift Python connector - Amazon Redshift](https://docs.aws.amazon.com/redshift/latest/mgmt/python-connect-examples.html)

[template_ETL_OEC.ipynb - Google Drive](https://drive.google.com/file/d/1P5kQo5_0bkzLNakbBwlf-9evT9PYtae_/view?usp=share_link)

[Curso de AWS Redshift para Manejo de Big Data - Platzi](https://platzi.com/cursos/redshift-big-data/)

[country_data.json - Google Drive](https://drive.google.com/file/d/19QM_nHhUZ4s3yZcV7ePis5mD1dTtCl9Z/view?usp=share_link)

[hs_codes.csv - Google Drive](https://drive.google.com/file/d/1C6EwxoQmROiC27gvTsUNBVPhrtfH8mDI/view?usp=share_link)

[postgres_public_trades.sql - Google Drive](https://drive.google.com/file/d/19U7l0kp3mEh8SYYG6BjoDp0kVPYWDsqI/view?usp=share_link)

[guia_ETL_OEC.ipynb - Google Drive](https://drive.google.com/file/d/1LjmlMnpajBsNnTqWozh8T-5NpbetIzO_/view?usp=share_link)

### Nota: Seguridad .env

Un archivo **`.env`** se utiliza para almacenar configuraciones sensibles o variables de entorno de un proyecto, como credenciales, configuraciones de conexi칩n y otros par치metros que no deseas incluir directamente en tu c칩digo fuente. Es una pr치ctica com칰n para mantener tu c칩digo seguro y facilitar la configuraci칩n de entornos (desarrollo, pruebas, producci칩n).

---

### 쯈u칠 puede contener un archivo `.env`?

Un archivo `.env` suele tener pares clave-valor separados por un signo igual `=`. Por ejemplo:

```plaintext
DB_HOST=localhost
DB_PORT=5432
DB_NAME=my_database
DB_USER=admin
DB_PASSWORD=securepassword
SECRET_KEY=mysecretkey123
DEBUG=True
```

---

### 쮺칩mo usar un archivo `.env` en Python?

Python puede manejar archivos `.env` utilizando librer칤as como **`python-dotenv`**. Esta librer칤a carga autom치ticamente las variables definidas en el archivo `.env` y las hace accesibles a trav칠s de `os.environ`.

#### 1. **Instalar `python-dotenv`**
Primero, instala la librer칤a usando pip:

```bash
pip install python-dotenv
```

---

#### 2. **Crear un archivo `.env`**
Guarda tus credenciales y configuraciones en un archivo `.env` en el directorio ra칤z de tu proyecto:

```plaintext
DB_HOST=localhost
DB_PORT=5432
DB_NAME=my_database
DB_USER=admin
DB_PASSWORD=securepassword
```

---

#### 3. **Cargar y usar las variables en Python**

Carga las variables en tu c칩digo utilizando `python-dotenv` y el m칩dulo `os`:

```python
from dotenv import load_dotenv
import os

# Cargar las variables del archivo .env
load_dotenv()

# Obtener las variables
db_host = os.getenv('DB_HOST')
db_port = os.getenv('DB_PORT')
db_name = os.getenv('DB_NAME')
db_user = os.getenv('DB_USER')
db_password = os.getenv('DB_PASSWORD')

# Usar las variables (ejemplo con SQLAlchemy)
from sqlalchemy import create_engine

engine = create_engine(f"postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}")
print("Conexi칩n exitosa.")
```

---

### Ventajas de usar `.env`:
1. **Seguridad**: Evita exponer credenciales sensibles en el c칩digo fuente.
2. **Flexibilidad**: Facilita la configuraci칩n de entornos diferentes (desarrollo, pruebas, producci칩n).
3. **Mantenibilidad**: Centraliza las configuraciones del proyecto.

---

### Buenas pr치cticas:
1. **No incluir `.env` en el repositorio**: Agrega el archivo `.env` a tu `.gitignore` para evitar subirlo a GitHub o a cualquier otro sistema de control de versiones:
   ```plaintext
   # .gitignore
   .env
   ```
2. **Usar variables descriptivas**: Nombres claros como `DB_HOST` o `API_KEY` ayudan a entender su prop칩sito.

Con estas configuraciones, puedes manejar datos sensibles de manera segura y profesional en tus proyectos Python.

## Estructurar sentence para carga de datos de S3 a Redshift

춰Hola! En la clase anterior definimos esta funci칩n de carga de archivos en nuestra notebook de Python:

![sentence](images/sentence.png)

游눠Dentro de esta funci칩n esta la variable sentence, que es una secuencia de texto con una instrucci칩n para copiar los registros de los archivos CSV, que est치n dentro del bucket de S3, hacia la base de datos dev que est치 en un schema.

Esta sentencia debe configurarse espec칤ficamente para el schema de tu base de datos y la direcci칩n de tu bucket en S3.

Revisitemos tres componentes principales dentro de la sentencia:

![sentence partes](images/sentence_partes.png)

- **Schema en DB en Redshift**: es el schema de nuestra base de datos donde est치n las tablas a donde se copiar치n los registros de los archivos CSV. Este debe ser remplazado por ese schema, que en tu caso deber칤a ser public en lugar de `etl_test`

- **Nombre de bucket de S3**: direcci칩n del bucket donde se guardan temporalmente los archivos CSV. En tu caso debe remplazarse `s3://platzi-etl/course_etl_target/{}` por el nombre 칰nico de tu bucket. Si tu bucket se llamara **nombre-unico-bucket-curso-platzi** en la sentencia deber칤a estar `s3://nombre-unico-bucket-curso-platzi{}`

- **Regi칩n elegida de AWS**: es esa regi칩n donde creaste el cl칰ster de Redshift. En caso de que no lo hayas hecho en la regi칩n `us-west-2` escribe el c칩digo de la regi칩n que elegiste.

Tu sentence deber칤a verse de forma similar a la siguiente, pero recuerda que no ser치 id칠ntica porque el nombre de tu bucket es 칰nico en el mundo:

```python
sentence = '''copy public.{} from 's3://nombre-unico-bucket-curso-platzi/{}' credentials 'aws_access_key_id={};aws_secret_access_key={}' csv delimiter '|' region 'us-west-2' ignoreheader 1'''.format(table_name, file_name, os.environ.get('AWS_ACCESS_KEY_ID'), os.environ.get('AWS_SECRET_ACCESS_KEY'))
```

춰Listo! Aplica estos ajustes a tu sentence y 칰salos durante la pr칩xima clase donde usaremos la funci칩n load_file() para cargar los registros en Redshift. 丘뙖잺游녪游낗

## Carga de datos: subida de archivos a AWS Redshift

Subir datos a **AWS Redshift** implica un proceso que usualmente consta de los siguientes pasos:  

1. **Preparar los datos**: Aseg칰rate de que los datos est치n en un formato compatible, como archivos CSV, JSON, Parquet, o Avro.  
2. **Subir los datos a S3**: AWS Redshift carga datos desde Amazon S3, as칤 que primero necesitas colocar tus archivos en un bucket de S3.  
3. **Cargar datos a Redshift**: Usa la instrucci칩n `COPY` para transferir los datos desde S3 a tu tabla de Redshift.

---

### Prerrequisitos
1. **Crear una base de datos y tablas en Redshift**:
   - Necesitas un cl칰ster configurado y una tabla creada en Redshift donde cargar los datos.
2. **Configurar un bucket en S3**:
   - Define un bucket donde alojar치s los datos que ser치n importados.
3. **Credenciales AWS**:
   - Necesitar치s una IAM Role o Access Key/Secret Access Key con permisos para acceder a S3 y Redshift.

---

### Proceso detallado

#### 1. **Preparar los datos y subirlos a S3**

Puedes usar **AWS CLI** para subir datos a tu bucket de S3:
```bash
aws s3 cp /path/to/your/data.csv s3://your-bucket-name/data.csv
```

O, si est치s usando Python, puedes emplear la librer칤a `boto3`:
```python
import boto3

# Configurar el cliente de S3
s3 = boto3.client('s3')

# Subir archivo
s3.upload_file('data.csv', 'your-bucket-name', 'data.csv')
print("Archivo subido exitosamente a S3.")
```

---

#### 2. **Configurar conexi칩n a Redshift**

Usa `psycopg2` o SQLAlchemy para conectarte al cl칰ster de Redshift:
```python
from sqlalchemy import create_engine

# Datos de conexi칩n
DATABASE = "your_database"
USER = "your_user"
PASSWORD = "your_password"
HOST = "your-cluster-endpoint"
PORT = "5439"

# Crear la conexi칩n
engine = create_engine(f"postgresql+psycopg2://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}")
connection = engine.connect()
print("Conexi칩n exitosa a Redshift.")
```

---

#### 3. **Crear la tabla en Redshift**

Define la estructura de tu tabla para que coincida con los datos que cargar치s:
```sql
CREATE TABLE your_table (
    column1 VARCHAR(50),
    column2 INT,
    column3 DATE
);
```

---

#### 4. **Cargar datos desde S3 a Redshift**

Usa la instrucci칩n `COPY` para transferir datos desde S3 a tu tabla en Redshift:
```python
# Comando SQL para copiar los datos
copy_command = """
    COPY your_table
    FROM 's3://your-bucket-name/data.csv'
    IAM_ROLE 'arn:aws:iam::your-account-id:role/your-redshift-role'
    FORMAT AS CSV
    IGNOREHEADER 1;
"""

# Ejecutar el comando
connection.execute(copy_command)
print("Datos cargados exitosamente en Redshift.")
```

---

### Consideraciones importantes
1. **Rol IAM**:
   - El rol asociado a tu cl칰ster Redshift debe tener permisos para acceder al bucket S3.
   - Pol칤tica m칤nima necesaria:
     ```json
     {
       "Version": "2012-10-17",
       "Statement": [
         {
           "Effect": "Allow",
           "Action": "s3:*",
           "Resource": "arn:aws:s3:::your-bucket-name/*"
         }
       ]
     }
     ```

2. **Validaci칩n**:
   - Despu칠s de cargar los datos, verifica que se cargaron correctamente:
     ```sql
     SELECT COUNT(*) FROM your_table;
     ```

3. **Formato del archivo**:
   - Aseg칰rate de que el archivo en S3 est치 correctamente formateado y coincide con las columnas de la tabla.

4. **Errores comunes**:
   - **Permisos insuficientes**: Verifica que el rol IAM tiene acceso al bucket S3 y permisos COPY en Redshift.
   - **Formato incorrecto**: Si el archivo CSV tiene delimitadores inconsistentes, podr칤as recibir errores.

---

### Alternativa: Usar AWS Data Wrangler

La librer칤a `awswrangler` simplifica el proceso de carga desde Pandas DataFrame a Redshift:
```python
import awswrangler as wr
import pandas as pd

# Crear DataFrame
df = pd.read_csv('data.csv')

# Cargar el DataFrame a Redshift
wr.redshift.copy_from_files(
    paths=["s3://your-bucket-name/data.csv"],
    con=engine.raw_connection(),
    schema="public",
    table="your_table",
    iam_role="arn:aws:iam::your-account-id:role/your-redshift-role",
    format="csv",
    mode="overwrite"
)
```

Con este enfoque, puedes integrar Redshift con tus flujos de datos en Python de manera m치s sencilla.

**Lecturas recomendadas**

[Examples of using the Amazon Redshift Python connector - Amazon Redshift](https://docs.aws.amazon.com/redshift/latest/mgmt/python-connect-examples.html)

[Curso de AWS Redshift para Manejo de Big Data - Platzi](https://platzi.com/cursos/redshift-big-data/)

[template_ETL_OEC.ipynb - Google Drive](https://drive.google.com/file/d/1P5kQo5_0bkzLNakbBwlf-9evT9PYtae_/view?usp=share_link)