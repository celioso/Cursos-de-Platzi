# Curso de Fundamentos de ETL con Python y Pentaho

## 쯈u칠 es un ETL en ingenier칤a de datos?

Un **ETL** (Extract, Transform, Load) es un proceso en la ingenier칤a de datos que se utiliza para mover y transformar datos desde diferentes fuentes hacia un destino centralizado, como un **data warehouse** o un **data lake**, para su an치lisis o procesamiento. Este proceso consta de tres fases principales:

### **1. Extract (Extracci칩n):**
- **Objetivo:** Obtener datos de una o m치s fuentes heterog칠neas.
- **Fuentes t칤picas:** Bases de datos SQL, archivos (CSV, JSON, XML), APIs, sistemas ERP, hojas de c치lculo, entre otros.
- **Desaf칤os comunes:** Manejo de datos inconsistentes, conexiones lentas y formatos variados.

### **2. Transform (Transformaci칩n):**
- **Objetivo:** Procesar y convertir los datos para que sean consistentes, limpios y adecuados para el an치lisis.
- **Ejemplos de transformaciones:**
  - Normalizaci칩n de formatos (ejemplo: fechas o monedas).
  - Enriquecimiento de datos (agregar datos adicionales desde otra fuente).
  - Limpieza de datos (eliminar duplicados, corregir errores).
  - Agregaciones o c치lculos (ejemplo: sumar ingresos mensuales).
- **Beneficio:** Garantiza que los datos sean 칰tiles y coherentes antes de su almacenamiento.

### **3. Load (Carga):**
- **Objetivo:** Transferir los datos transformados al sistema de destino.
- **Tipos de carga:**
  - **Carga completa:** Se transfiere todo el conjunto de datos.
  - **Carga incremental:** Solo se cargan los datos nuevos o modificados.
- **Destinos t칤picos:** Bases de datos anal칤ticas, herramientas de BI (Business Intelligence), o sistemas de almacenamiento en la nube.

### **Importancia del ETL:**
- Proporciona una base s칩lida para el an치lisis de datos y la toma de decisiones basada en datos confiables.
- Permite integrar informaci칩n de m칰ltiples fuentes en un 칰nico sistema.
- Mejora la calidad y accesibilidad de los datos.

### Herramientas ETL populares:
- **Open Source:** Apache Nifi, Talend Open Studio, Pentaho.
- **Comerciales:** Informatica, Microsoft SQL Server Integration Services (SSIS).
- **Basadas en la nube:** AWS Glue, Google Dataflow, Azure Data Factory.

Aqu칤 tienes un ejemplo b치sico de un proceso ETL en Python utilizando la biblioteca `pandas`. Este script leer치 datos de un archivo CSV, realizar치 algunas transformaciones b치sicas y luego los cargar치 en un archivo de salida:

### Ejemplo de ETL con Python y pandas

```python
import pandas as pd

# ETL Pipeline

# --- Extract ---
# Leer datos desde un archivo CSV
input_file = 'datos_entrada.csv'  # Archivo de entrada
try:
    data = pd.read_csv(input_file)
    print("Datos cargados exitosamente:")
    print(data.head())
except FileNotFoundError:
    print(f"El archivo {input_file} no existe. Por favor, verifica la ruta.")

# --- Transform ---
# Transformaciones b치sicas:
# 1. Renombrar columnas
data.rename(columns={"Nombre": "name", "Edad": "age", "Pa칤s": "country"}, inplace=True)

# 2. Convertir la edad a n칰meros enteros
data['age'] = pd.to_numeric(data['age'], errors='coerce')

# 3. Filtrar filas donde la edad no sea nula y mayores de 18 a침os
data = data[data['age'] >= 18]

# 4. Capitalizar nombres de pa칤s
data['country'] = data['country'].str.capitalize()

print("\nDatos despu칠s de la transformaci칩n:")
print(data.head())

# --- Load ---
# Guardar datos transformados en un nuevo archivo
output_file = 'datos_salida.csv'
data.to_csv(output_file, index=False)
print(f"\nDatos procesados guardados en {output_file}")
```

### 쯈u칠 hace este script?

1. **Extract (Extracci칩n):**
   - Lee un archivo CSV llamado `datos_entrada.csv`.
   - Muestra las primeras filas para verificar el contenido.

2. **Transform (Transformaci칩n):**
   - Renombra columnas para estandarizar los nombres.
   - Convierte la columna de edad (`Edad`) en n칰meros enteros.
   - Filtra registros donde la edad sea mayor o igual a 18.
   - Ajusta los nombres de los pa칤ses para tener formato capitalizado.

3. **Load (Carga):**
   - Guarda el resultado transformado en un nuevo archivo llamado `datos_salida.csv`.

### Estructura de entrada esperada (`datos_entrada.csv`):
```csv
Nombre,Edad,Pa칤s
Juan,25,Colombia
Maria,17,per칰
Carlos,30,Chile
Ana,,Brasil
```

### Resultado (`datos_salida.csv`):
```csv
name,age,country
Juan,25,Colombia
Carlos,30,Chile
```
### **ETL vs. ELT: Prop칩sito y Usos**

La elecci칩n entre **ETL (Extract, Transform, Load)** y **ELT (Extract, Load, Transform)** depende del caso de uso, la infraestructura y los requisitos del negocio. Ambos son enfoques utilizados para mover y procesar datos, pero tienen diferencias clave en c칩mo y d칩nde se realiza la transformaci칩n de los datos.

### **1. ETL (Extract, Transform, Load)**

#### **쯈u칠 es?**
- En ETL, los datos se extraen de las fuentes, se transforman en un entorno intermedio (como un servidor o herramienta dedicada), y luego se cargan en el sistema de destino.

#### **쯇ara qu칠 se utiliza?**
- **Sistemas tradicionales de an치lisis de datos** como **Data Warehouses** (almacenes de datos).
- Cuando el sistema de destino tiene capacidades limitadas para procesar datos o se requieren transformaciones complejas antes de la carga.
- **Casos donde se prioriza la calidad de los datos antes del an치lisis.**

#### **Ventajas:**
1. **Transformaci칩n temprana:** Los datos llegan al destino ya listos para el an치lisis.
2. **Control sobre la calidad de datos:** Permite realizar validaciones estrictas y limpiezas antes de la carga.
3. **Compatible con sistemas antiguos:** Ideal para bases de datos tradicionales que no manejan grandes vol칰menes de datos.

#### **Limitaciones:**
- Puede ser m치s lento, especialmente con vol칰menes grandes de datos.
- Requiere una infraestructura intermedia para realizar las transformaciones.

### **2. ELT (Extract, Load, Transform)**

#### **쯈u칠 es?**
- En ELT, los datos se extraen de las fuentes, se cargan directamente en el sistema de destino (por ejemplo, un Data Lake o un Data Warehouse moderno), y luego se transforman utilizando el poder computacional del sistema de destino.

#### **쯇ara qu칠 se utiliza?**
- **Sistemas modernos en la nube**, como **Data Lakes** (Google BigQuery, Amazon Redshift, Snowflake).
- Cuando los vol칰menes de datos son masivos y los sistemas de destino tienen alta capacidad de procesamiento.
- **An치lisis en tiempo real o cuasi-tiempo real**, donde los datos deben estar r치pidamente disponibles.

#### **Ventajas:**
1. **Velocidad de carga inicial:** Los datos se mueven r치pidamente al destino.
2. **Escalabilidad:** Aprovecha la potencia de procesamiento de sistemas en la nube.
3. **Flexibilidad:** Permite explorar y transformar los datos seg칰n las necesidades posteriores.

#### **Limitaciones:**
- Requiere un sistema de destino robusto con capacidades avanzadas.
- Puede cargar datos "sucios" inicialmente, lo que podr칤a generar problemas si no se transforman correctamente.

### **Comparaci칩n Directa**

| Caracter칤stica              | **ETL**                           | **ELT**                          |
|-----------------------------|------------------------------------|-----------------------------------|
| **Momento de transformaci칩n** | Antes de cargar los datos          | Despu칠s de cargar los datos       |
| **Sistema de destino**       | Data Warehouse tradicional         | Data Lake o Data Warehouse moderno |
| **Procesamiento de datos**   | Externo al destino                 | Interno en el destino            |
| **Vol칰menes de datos**       | Moderados                          | Grandes o masivos                |
| **Velocidad inicial**        | M치s lenta                          | M치s r치pida                       |
| **Limpieza y calidad inicial**| Alta calidad antes de la carga     | Requiere procesamiento posterior |

### **쮺u치ndo usar ETL?**
- Cuando los datos requieren transformaciones complejas o detalladas antes de ser 칰tiles.
- Cuando trabajas con sistemas antiguos o limitados en capacidad.
- Cuando los datos necesitan estar listos para an치lisis inmediatamente despu칠s de la carga.

### **쮺u치ndo usar ELT?**
- Cuando se manejan grandes vol칰menes de datos no estructurados.
- Si el sistema de destino tiene gran capacidad de procesamiento y escalabilidad (por ejemplo, BigQuery, Snowflake).
- Cuando se necesita flexibilidad para analizar o transformar datos en diferentes momentos.

## Conceptos base de ETL

### Conceptos Base de ETL (Extract, Transform, Load)

ETL (Extraer, Transformar, Cargar) es un proceso fundamental en la ingenier칤a de datos que permite trasladar y procesar datos desde m칰ltiples fuentes hacia un destino final para an치lisis o almacenamiento. A continuaci칩n, se describen los conceptos base:

### **1. Extracci칩n (Extract)**
**Definici칩n:** Es el proceso de recopilar datos desde una o varias fuentes heterog칠neas. Las fuentes pueden incluir bases de datos relacionales, archivos planos (CSV, JSON, XML), APIs, logs, o sistemas ERP.  

**Caracter칤sticas:**
- **Variedad de fuentes:** Datos estructurados (tablas SQL) y no estructurados (archivos de texto, im치genes).
- **Objetivo:** Obtener datos sin alterar su formato original.
- **Herramientas comunes:** Conectores de bases de datos, APIs REST, scripts personalizados.

**Ejemplo:**  
Conectar a una base de datos SQL para extraer una tabla de usuarios:
```sql
SELECT * FROM usuarios;
```

### **2. Transformaci칩n (Transform)**
**Definici칩n:** Es la etapa donde los datos se limpian, estandarizan, enriquecen o transforman para adaptarse a las necesidades del negocio o del sistema de destino.  

**Operaciones t칤picas:**
- **Limpieza:** Eliminar valores nulos, duplicados o inconsistentes.
- **Normalizaci칩n:** Cambiar formatos de fecha o convertir unidades de medida.
- **C치lculos:** Crear nuevas columnas (por ejemplo, calcular ingresos anuales a partir de ingresos mensuales).
- **Enriquecimiento:** Combinar datos de m칰ltiples fuentes.
- **Validaci칩n:** Asegurarse de que los datos cumplen con reglas de negocio espec칤ficas.

**Ejemplo:**  
Convertir un archivo CSV de ventas en un formato estandarizado:
```python
import pandas as pd

# Cargar datos
data = pd.read_csv("ventas.csv")

# Limpiar y transformar
data['fecha'] = pd.to_datetime(data['fecha'])
data['total'] = data['cantidad'] * data['precio_unitario']
data = data.dropna()  # Eliminar valores nulos
```

### **3. Carga (Load)**
**Definici칩n:** Es el proceso de mover los datos transformados al sistema de destino, como un almac칠n de datos (Data Warehouse), base de datos, o sistema de an치lisis.  

**Tipos de carga:**
- **Carga completa:** Sobrescribe los datos existentes en cada ejecuci칩n.
- **Carga incremental:** Solo se cargan los datos nuevos o modificados.
- **Carga en tiempo real:** Los datos se env칤an continuamente al destino.

**Herramientas comunes:** 
- SQL para bases de datos relacionales.
- APIs o conectores espec칤ficos para sistemas en la nube como Amazon S3 o Google BigQuery.

**Ejemplo:**  
Insertar los datos procesados en una tabla de SQL:
```sql
INSERT INTO ventas_procesadas (fecha, producto, cantidad, total)
VALUES ('2024-01-01', 'Laptop', 10, 15000);
```

### **Objetivo del Proceso ETL**
El prop칩sito principal de ETL es consolidar datos dispersos en un solo lugar, procesarlos para que sean 칰tiles y garantizar que est칠n listos para el an치lisis o la toma de decisiones. Esto incluye:
- **Integraci칩n:** Combinar datos de diferentes fuentes.
- **Consistencia:** Proveer datos limpios y estructurados.
- **Eficiencia:** Reducir la complejidad del acceso y an치lisis.

### **ETL vs. ELT**
Aunque ETL es el enfoque tradicional, **ELT (Extract, Load, Transform)** es una variaci칩n que carga los datos directamente en el almac칠n de datos antes de transformarlos. Esto se utiliza especialmente en sistemas modernos basados en la nube.

## Consideraciones de ETL

### **Consideraciones Clave en un Proceso ETL**

El 칠xito de un proyecto ETL (Extract, Transform, Load) depende de la planificaci칩n cuidadosa, la comprensi칩n de las necesidades del negocio y la calidad de la ejecuci칩n. Aqu칤 tienes las principales consideraciones al implementar un proceso ETL:

### **1. Comprensi칩n de los Requisitos**
- **Objetivos del negocio:** Define claramente qu칠 se espera lograr con el proceso ETL (reportes, an치lisis, monitoreo, etc.).
- **Volumen de datos:** Considera la cantidad de datos a procesar y su frecuencia (diaria, semanal, en tiempo real).
- **Fuente de datos:** Identifica todas las fuentes de datos (bases de datos relacionales, APIs, archivos CSV, etc.) y su formato.

### **2. Calidad de los Datos**
- **Integridad de datos:** Verifica que los datos de las fuentes sean completos y precisos.
- **Consistencia:** Aseg칰rate de que los datos tengan un formato est치ndar (por ejemplo, fechas y monedas).
- **Manejo de datos err칩neos:** Implementa estrategias para tratar datos faltantes, duplicados o corruptos.

### **3. Escalabilidad**
- **Crecimiento futuro:** Dise침a el sistema para manejar un incremento en el volumen y variedad de datos.
- **Escalabilidad horizontal:** Utiliza herramientas capaces de procesar datos en paralelo para mantener el rendimiento.

### **4. Rendimiento**
- **Tiempo de procesamiento:** Minimiza el tiempo necesario para extraer, transformar y cargar datos, especialmente para procesos cr칤ticos.
- **Optimizaci칩n de consultas:** Utiliza 칤ndices y otras t칠cnicas para acelerar las operaciones en bases de datos.

### **5. Seguridad**
- **Encriptaci칩n:** Protege los datos sensibles durante la transferencia (en tr치nsito) y el almacenamiento (en reposo).
- **Control de acceso:** Implementa pol칤ticas de seguridad para limitar qui칠n puede acceder a los datos y realizar cambios.
- **Regulaciones:** Cumple con normativas como GDPR, HIPAA o CCPA, seg칰n sea necesario.

### **6. Mantenimiento**
- **Monitoreo:** Configura alertas para detectar fallos en tiempo real.
- **Registro de errores:** Implementa un sistema de logging para rastrear problemas en el flujo de datos.
- **Actualizaciones:** Aseg칰rate de que las herramientas ETL puedan actualizarse sin interrupciones significativas.

### **7. Herramientas y Tecnolog칤a**
- **Elecci칩n de herramientas:** Decide entre herramientas comerciales (Informatica, Talend) o plataformas en la nube (AWS Glue, Google Dataflow).
- **Compatibilidad:** Aseg칰rate de que la herramienta seleccionada pueda conectarse a todas las fuentes de datos necesarias.

### **8. Transformaciones de Datos**
- **Complejidad:** Eval칰a qu칠 tan complejas son las transformaciones necesarias (filtros, agregaciones, cambios de formato).
- **Flexibilidad:** Dise침a transformaciones modulares y reutilizables.
- **Pruebas:** Verifica que las transformaciones produzcan resultados correctos antes de cargarlas en el destino.

### **9. Procesos de Carga**
- **Tipo de carga:** Define si ser치 completa o incremental (solo datos nuevos o modificados).
- **Manejo de fallos:** Implementa mecanismos para reiniciar cargas fallidas sin duplicar datos.
- **Orden de carga:** Aseg칰rate de que las dependencias entre tablas se respeten.

### **10. Documentaci칩n**
- **Mapeo de datos:** Documenta c칩mo se transforman los datos desde las fuentes hasta el destino.
- **Gu칤as operativas:** Proporciona instrucciones claras para administrar y solucionar problemas del flujo ETL.
- **Versionado:** Registra cambios en el dise침o del flujo para facilitar auditor칤as y mantenimiento.

### **11. Consideraciones Adicionales**
- **ETL vs. ELT:** Eval칰a si un enfoque ELT podr칤a ser m치s adecuado para el caso de uso espec칤fico.
- **Costos:** Considera los costos de licencias, infraestructura y mantenimiento de las herramientas ETL.
- **Pruebas:** Realiza pruebas exhaustivas antes de implementar en producci칩n.

Al abordar cada una de estas 치reas, puedes garantizar que el proceso ETL sea robusto, eficiente y alineado con las necesidades del negocio. 

## Servicios y herramientas para ETL

### **Servicios y Herramientas para ETL**

El 칠xito de los procesos de ETL depende en gran medida de las herramientas y servicios que facilitan la extracci칩n, transformaci칩n y carga de datos. A continuaci칩n, se describen las principales opciones divididas en categor칤as clave:

### **1. Herramientas ETL Tradicionales**
Estas herramientas est치n dise침adas espec칤ficamente para procesos ETL en entornos locales o h칤bridos.

- **Informatica PowerCenter**  
  Una de las herramientas m치s populares y robustas para ETL. Ofrece funciones avanzadas para transformar y gestionar grandes vol칰menes de datos.

- **Talend Data Integration**  
  Plataforma de c칩digo abierto que incluye conectores para diversas fuentes de datos y capacidades avanzadas de transformaci칩n.

- **IBM DataStage**  
  Herramienta empresarial para grandes proyectos ETL, ideal para integraciones complejas y procesamiento de big data.

- **Microsoft SQL Server Integration Services (SSIS)**  
  Parte de Microsoft SQL Server, ofrece capacidades ETL para usuarios que trabajan con bases de datos SQL.

### **2. Herramientas ETL en la Nube**
Dise침adas para aprovechar la escalabilidad y flexibilidad de la nube, estas herramientas integran flujos ETL con servicios en la nube.

- **AWS Glue**  
  Servicio ETL totalmente administrado en AWS que permite ejecutar transformaciones de datos basadas en Python (PySpark).

- **Google Cloud Dataflow**  
  Ofrece capacidades de procesamiento en tiempo real y por lotes para pipelines de datos en la nube de Google.

- **Azure Data Factory**  
  Soluci칩n de integraci칩n de datos de Microsoft Azure que permite mover y transformar datos entre m칰ltiples or칤genes y destinos.

- **Snowflake + Matillion**  
  Snowflake es un almac칠n de datos en la nube, y Matillion es una herramienta ETL dise침ada espec칤ficamente para integrarse con Snowflake.

### **3. Herramientas Open Source**
Opciones gratuitas que ofrecen flexibilidad y personalizaci칩n para desarrolladores y peque침os equipos.

- **Apache Nifi**  
  Herramienta de integraci칩n de datos visual para flujos ETL. Ideal para flujos en tiempo real y automatizaci칩n.

- **Apache Airflow**  
  Aunque no es una herramienta ETL tradicional, permite programar y orquestar pipelines ETL.

- **Pentaho Data Integration (PDI)**  
  Herramienta de c칩digo abierto que proporciona un enfoque visual para construir y ejecutar flujos ETL.

### **4. Herramientas de Orquestaci칩n de Datos**
Estas herramientas gestionan pipelines de datos m치s complejos, combinando ETL con otras funcionalidades.

- **Fivetran**  
  Automatiza la extracci칩n de datos y los carga en destinos populares como BigQuery, Snowflake o Redshift.

- **Stitch**  
  Una herramienta ligera para mover datos r치pidamente hacia almacenes de datos.

- **dbt (Data Build Tool)**  
  Aunque es m치s una herramienta ELT, ayuda a gestionar transformaciones SQL en almacenes de datos modernos.

### **5. Herramientas de Big Data y Procesamiento en Tiempo Real**
Dise침adas para manejar vol칰menes masivos de datos y ofrecer capacidades en tiempo real.

- **Apache Spark**  
  Plataforma de an치lisis distribuido que permite realizar ETL a gran escala con alta velocidad.

- **Kafka + Kafka Streams**  
  Para flujos ETL en tiempo real con mensajes entre sistemas distribuidos.

- **Databricks**  
  Plataforma basada en Apache Spark que permite construir pipelines ETL avanzados.

### **6. Servicios ETL Especializados**
Soluciones dise침adas para necesidades espec칤ficas de sectores o casos de uso.

- **Alteryx**  
  Enfocada en la integraci칩n y an치lisis de datos, ideal para usuarios que requieren an치lisis avanzado sin codificaci칩n extensa.

- **SAP Data Services**  
  Herramienta ETL orientada a la integraci칩n de datos empresariales en entornos SAP.

- **Boomi (Dell Boomi)**  
  Soluci칩n basada en la nube que facilita la integraci칩n de datos entre aplicaciones SaaS y locales.

### **7. Factores para Elegir una Herramienta ETL**
1. **Volumen de datos:** 쮼s un entorno peque침o o big data?  
2. **Compatibilidad:** 쯉e conecta f치cilmente a tus fuentes y destinos?  
3. **Rendimiento:** 쯇uede manejar la frecuencia y carga de datos?  
4. **Facilidad de uso:** 쮼s necesario programar o es m치s visual?  
5. **Costo:** Considera herramientas gratuitas frente a licencias comerciales.  
6. **Escalabilidad:** 쯉e adapta al crecimiento futuro de los datos?  

**Lecturas recomendadas**

[pandas - Python Data Analysis Library](https://pandas.pydata.org/)

[Curso B치sico de Manipulaci칩n y Transformaci칩n de Datos con Pandas y NumPy - Platzi](https://platzi.com/cursos/pandas-numpy/)

[Tutorial desde cero para dominar Pandas [Python]](https://platzi.com/blog/pandas/)

[Pentaho from Hitachi Vantara - Browse Files at SourceForge.net](https://sourceforge.net/projects/pentaho/files/)

[Pentaho Data Integration - Hitachi Vantara Lumada and Pentaho Documentation](https://help.hitachivantara.com/Documentation/Pentaho/7.0/0D0/Pentaho_Data_Integration)

[Descargar Pentaho](https://pentaho.com/pentaho-developer-edition/)

## Sources

El t칠rmino "sources" en el contexto de ETL y procesamiento de datos se refiere a las **fuentes de datos**. Estas fuentes son los or칤genes de la informaci칩n que se extrae para ser procesada y transformada dentro de los sistemas ETL. Pueden provenir de diferentes tipos de sistemas o bases de datos, tanto estructurados como no estructurados. A continuaci칩n, te explico algunos conceptos clave sobre las fuentes de datos en un proceso ETL, en espa침ol:

### Fuentes de Datos en un Proceso ETL:
1. **Bases de Datos Relacionales (RDBMS)**:
   Las bases de datos como **MySQL**, **PostgreSQL**, **Oracle** o **SQL Server** suelen ser fuentes comunes para los procesos ETL. Los datos extra칤dos de estas fuentes generalmente est치n estructurados y organizados en tablas, lo que facilita su extracci칩n.

2. **Archivos de Texto y CSV**:
   Archivos planos como **CSV**, **JSON**, **XML** o **TXT** son comunes en muchos procesos ETL. Estos archivos pueden contener datos en formato tabular o jer치rquico, pero requieren procesamiento para ser transformados en un formato adecuado para el an치lisis.

3. **APIs**:
   Las **APIs (Interfaces de Programaci칩n de Aplicaciones)** permiten acceder a datos de aplicaciones externas, como redes sociales, plataformas de comercio electr칩nico o sistemas de informaci칩n. Los datos extra칤dos a trav칠s de una API generalmente est치n en formato JSON o XML.

4. **Sistemas de Almacenamiento en la Nube**:
   Fuentes como **Amazon S3**, **Google Cloud Storage**, o **Azure Blob Storage** son muy utilizadas, ya que permiten almacenar grandes vol칰menes de datos no estructurados que se pueden extraer para su procesamiento ETL.

5. **Sistemas NoSQL**:
   Bases de datos NoSQL como **MongoDB**, **Cassandra**, o **CouchDB** son comunes cuando los datos no siguen una estructura r칤gida de tablas y relaciones. Estos sistemas pueden ser fuentes para datos semi-estructurados o no estructurados.

6. **Flujos de Datos en Tiempo Real**:
   Los sistemas que generan datos en tiempo real, como los sensores IoT, o las plataformas de streaming como **Apache Kafka**, pueden ser fuentes de datos para procesos ETL de transmisi칩n continua (streaming ETL), donde los datos son procesados en tiempo real en lugar de ser extra칤dos en lotes.

### Explicaci칩n de las Fuentes de Datos en el Contexto ETL:
Las **fuentes de datos** son un componente crucial en los procesos ETL (Extract, Transform, Load), ya que son el primer paso para obtener la informaci칩n necesaria para su an치lisis o almacenamiento. En este proceso:

- **Extract (Extracci칩n)**: Se extraen los datos de las fuentes. Aqu칤 es donde se encuentran las "sources" que alimentan el proceso.
- **Transform (Transformaci칩n)**: Los datos extra칤dos se limpian, se validan y se estructuran seg칰n sea necesario. 
- **Load (Carga)**: Finalmente, los datos transformados se cargan en un sistema de destino, como un **Data Warehouse** o un **Data Lake**, donde pueden ser utilizados para an치lisis y reportes.

### Ejemplos de Fuentes Comunes en la Industria:
- En la **industria financiera**, las fuentes de datos pueden incluir bases de datos de transacciones bancarias, sistemas de tarjetas de cr칠dito, APIs de pagos, etc.
- En la **industria de salud**, las fuentes pueden ser registros m칠dicos electr칩nicos, dispositivos m칠dicos, o incluso datos de investigaci칩n cient칤fica.
- En la **industria petrolera**, las fuentes de datos pueden ser sensores en los pozos de petr칩leo, sistemas de monitoreo de maquinaria, y bases de datos que contienen registros de producci칩n.

En resumen, las **sources** en un proceso ETL son todas aquellas plataformas, bases de datos, archivos, APIs o sistemas que proveen los datos que se van a extraer, transformar y cargar en el flujo de trabajo ETL. Estas fuentes son fundamentales porque determinan la calidad, cantidad y tipo de datos que se utilizar치n en los an치lisis posteriores.

## Configuraci칩n de base de datos source y entorno para ETL en Python

춰Hola! En esta clase conocer치s c칩mo configurar una base de datos con SQL, que ser치 una de las 3 fuentes para extraer datos en el proyecto de ETL. Las otras dos fuentes son un archivo JSON y otro CSV que conocer치s en clases posteriores.

Adem치s, conocer치s c칩mo conectarte a esta base de datos OLTP con un software de administraci칩n de bases de datos. Puede ser DataSpell, DBeaver o el de tu preferencia.

Te sugiero usar DataSpell. M치s adelante de este tutorial ver치s c칩mo configurarlo.

游눠Algo que tenemos que destacar es que la base de datos SQL source no se tendr칤a que crear en un proceso de ETL. Esta base de datos ya estar칤a creada en alg칰n lado de la infraestructura de los sistemas y aplicaciones de la empresa donde est칠s colaborando.

En este caso lo estamos haciendo por fines educativos para que tengas una base de datos de donde tomar datos y conozcas el proceso de extracci칩n.

Para la configuraci칩n de nuestra base de datos source usaremos PostgreSQL. Podemos utilizarlo de dos formas, una instalaci칩n local de PostgreSQL o una configuraci칩n por Docker. Te sugiero hacerlo por Docker.

### 1. Crear container en Docker

Recordemos que Docker es un entorno de gesti칩n de contenedores, de manera que usaremos una imagen base con toda la configuraci칩n que requerimos sin instalar necesariamente en nuestra m치quina. Solo utilizando los recursos del sistema para correr dicha imagen, algo similar a una m치quina virtual.

Por ahora, solo necesitas haber tomado el [Curso de Python: PIP y Entornos Virtuales](https://platzi.com/cursos/python-pip/ "Curso de Python: PIP y Entornos Virtuales") para conocer lo esencial de c칩mo usar esta herramienta con Python. En ese curso encontrar치s la[ clase para saber c칩mo instalarlo en tu computador](https://platzi.com/clases/4261-python-pip/55136-instalacion-de-docker/ " clase para saber c칩mo instalarlo en tu computador").

Una vez que tengas instalado Docker en tu computador, ejecuta este comando en tu terminal:

WSL 2, Linux o macOS

```bash
sudo docker run -d --name=postgres -p 5432:5432 -v postgres-volume:/var/lib/postgresql/data -e POSTGRES_PASSWORD=mysecretpass postgres
```

Windows

```bash
docker run -d --name=postgres -p 5432:5432 -v postgres-volume:/var/lib/postgresql/data -e POSTGRES_PASSWORD=mysecretpass postgres
```

Como podr치s notar, en este comando se espec칤fico lo siguiente para la creaci칩n de la base de datos con Docker:

- Nombre del container: `--name=postgres`
- Puerto a compartir con la m치quina local: `-p 5432:5432`
- Volumen para el manejo de disco e informaci칩n: `-v postgres-volume:/var/lib/postgresql/data`
- Password en PostgreSQL: `POSTGRES_PASSWORD=mysecretpass`

### 1.5 Instalaci칩n local de PostgreSQL (opcional)

De no usar Docker podr칤as ver la clase del curso de PostgreSQL en donde aprendes a instalarlo localmente, pero te sugiero intentarlo con Docker ya que puede agilizar tu flujo de trabajo. 游땔

### 2. Validar container creado

Una vez que hayas creado el container de Docker usa el comando `docker ps` en tu terminal. Podr치s ver todos los contenedores que se encuentran en ejecuci칩n actualmente y una descripci칩n.

Deber치s ver la IMAGE postgres.

![IMAGE postgres](images/etl01.png)

### 3. Configurar DataSpell

Para conectarte a la base de datos usar치s un software de administraci칩n de bases de datos. Existen varios que puedes utilizar. Para el seguimiento del curso te sugiero utilizar **DataSpell** o, en su defecto, **DBeaver**.

DataSpell es un **IDE** completo para ciencia de de datos donde, adem치s de conectarte y hacer consultas a bases de datos, podr치s ejecutar Jupyter Notebooks. 춰Todo en el mismo lugar! 游눩游낗

![DataSpell](images/dataspell.png)

游눠 Una de sus desventajas es que es de pago, pero tiene un per칤odo de prueba de 30 d칤as para que lo pruebes con este curso. Adem치s existen ciertas opciones para obtener [licencias para estudiantes de bachillerato y universidad](https://www.jetbrains.com/community/education/#students "licencias para estudiantes de bachillerato y universidad").

丘멆잺游붦 En caso de que decidas usar DBeaver en lugar de DataSpell, utiliza tu entorno local de Jupyter Notebooks con Anaconda para la ejecuci칩n del c칩digo Python de las siguientes clases. 游냀

### Instalaci칩n de DataSpell

1. Para instalar DataSpell ve a [su sitio web aqu칤](https://www.jetbrains.com/dataspell/ "su sitio web aqu칤") y descarga la versi칩n para tu sistema operativo.游닌

3. Inst치lalo siguiendo las instrucciones que te aparezcan en el instalador.

丘멆잺 Cuando te solicite actualizar PATH Variable acepta marcando la opci칩n que te indique. Esto es para evitar errores de ambientes en el futuro. En Windows se ve as칤:

![PATH](images/PATH.png)

Al finalizar te pedir치 reiniciar el computador:

![restar dataspell](images/restar_dataspell.png)

4. Abre DataSpell ya que se haya instalado. Al hacer esto por primera vez te pedir치 iniciar sesi칩n. Elige la versi칩n free trial registrando tu cuenta para ello.

5. Una vez que tengas tu cuenta configurada te pedir치 elegir un int칠rprete de Python 游냀.

Previamente deber치s tener instalado **Anaconda** en tu sistema operativo. Te recomiendo que crees un **ambiente de Anaconda** (**Conda environment**) 칰nico para el proyecto del curso. Llama al ambiente `fundamentos-etl`.

Elige el ambiente de Anaconda que usar치s para el proyecto y presiona el bot칩n Launch DataSpell.

![Welcome to DataSpel](images/WelcometoDataSpell.png)

Elegir un int칠rprete de Anaconda servir치 para ejecutar Jupyter Notebooks en DataSpell.

6. Crea un nuevo Workspace en **DataSpell**. Presiona el bot칩n File en la barra superior y luego elige la opci칩n New Workspace Directory.

![new workspace directory](images/new_workspace_directory.png)

Llama` fundamentos-etl` al workspace y presiona el bot칩n azul **Create**.

![new etl workspace](images/new_etl_workspace.png)

### Elegir ambiente de WSL2 (opcional si usas WSL)

Si quieres usar DataSpell con tu entorno en Windows con WSL 2, deber치s conectar DataSpell al ambiente de Anaconda que tenga tu WSL.游냀

0. Crea un ambiente de Anaconda en tu WSL dedicado al proyecto de tu curso si todav칤a no lo has hecho. Ll치malo fundamentos-etl

`conda create --name fundamentos-etl python=3.9`

1. Despu칠s ve a DataSpell en su parte inferior donde aparece el int칠rprete. Presiona la direcci칩n que aparece y elige la opci칩n **Interpreter Settings**.

![elegir interprete dataspell](images/elegir_interprete_dataspell.png)

2. Escoge el workspace `fundamentos-etl` creado anteriormente en DataSpell.

丘멆잺OJO: el workspace y el Anaconda Environment no son lo mismo. El Anaconda Environment lo vamos a cargar dentro del Workspace de DataSpell.

Despu칠s presiona el bot칩n **Add Interpreter** e inmediatamente selecciona la opci칩n **On WS**L.

![elegir_interprete_wsl_dataspell.png](images/elegir_interprete_wsl_dataspell.png)

3. Elige la distribuci칩n de Linux a usar y da clic en el bot칩n Next cuando aparezca el mensaje "Instrospection completed succesfully!

![ws instrospection](images/wsl instrospection.png)

4. Elige el int칠rprete a usar. Este puede ser un Virtualvenv Environment, el System Interpreter o un Conda Environment. Elige la opci칩n de Conda Environment.

![select interpreter.png](images/select_interpreter.png)

5. Mara la casilla **Use existing environment**. Elige el Conda Environment de WSL que usar치s para tu proyecto. Anteriormente debiste crearlo desde tu terminal en WSL y llamarlo `fundamentos-etl`.

Finalmente, presiona el bot칩n azul **Create**.

![conda fundamentos etl environment](images/conda-fundamentos-etl-environment.png)

6. Para terminar el proceso presiona el bot칩n azul OK en la parte inferior.

![workspace](images/workspace.png)

7. Listo, ya deber치 aparecer tu entorno de Anaconda en WSL cargado en la parte inferior de DataSpell.

![ambiente cargado](images/ambiente-cargado.png)

丘멆잺Si te aparece un error que indique que el ambiente no puede ser usado como el int칠rprete del workspace es porque est치s intentando cargar el ambiente en el workspace general y no en un workspace de DataSpell que creaste.

[Aqu칤](https://www.jetbrains.com/help/dataspell/using-wsl-as-a-remote-interpreter.html "Aqu칤") encuentras la gu칤a oficial de c칩mo conectar tu DataSpell al int칠rprete de Python o Anaconda en WSL, por si necesitas aprender a configurarlo a detalle.

Recuerda que otra alternativa en Windows es instalar [Anaconda para Windows](https://www.anaconda.com/products/distribution "Anaconda para Windows") y conectar DataSpell directamente a esta versi칩n.

### 4. Conexi칩n a la base de datos PostgreSQL

Sigue estos pasos para conectarte a la base de datos postgres desde DataSpell.

1. Abre DataSpell en tu computador.

![data spell](images/dataspells.png)

2. Ve a la pesta침a de **Database** y en ella da clic en el **bot칩n de signo de +**.

![database dataspell.png](images/database_dataspell.png)

3. Selecciona la opci칩n de **Data Source** y dentro del men칰 desplegable elige la opci칩n de **PostgreSQL**.

![workspace](images/workspace25.png)

4. Introduce los datos siguientes en la conexi칩n:

- **Name**: local_postgres
- **Host**: localhost
- **Port**: 5432
- **User**: postgres
- **Database**: postgres
- **Url (opcional)**: jdbc:postgresql://localhost:5432/postgres
- **Password**: mysecretpass

5. Da clic en el bot칩n de Test Connection para probar la conexi칩n. Puede que te solicite actualizar unos drivers, ac칠ptalos. Una vez que indique que la conexi칩n es exitosa, da clic en el bot칩n OK.

![etl 02](images/etl02.png)

6. Listo, ya tienes tu base de datos conectada en DataSpell.

![postgres data spell](images/postgresdataspell.png)

### 4. Cargar datos en la base de datos Postgres

Dentro de DataSpell, ya con la conexi칩n a la base de datos previamente creada, ejecutar치s el script ***postgres_public_trades.sql***.

Desc치rgalo [aqu칤 de Google Drive](https://drive.google.com/file/u/2/d/19U7l0kp3mEh8SYYG6BjoDp0kVPYWDsqI/view?usp=share_link "aqu칤 de Google Drive"). 游닌

丘멆잺Este archivo pesa cerca de 500 MB, por lo que puede demorar su descarga. Contiene la creaci칩n de una tabla llamada trades y los insert de registros de la tabla.

丘멆잺Es posible que al intentar correr este script en **DBeaver** no sea posible por falta de memoria. Te sugerimos cortarlo en varias partes y cargar cada script independientemente.

![etl 03](images/etl031.png)

Una vez descargado el archivo **postgres_public_trades.sql** sigue estos pasos para cargar los datos con DataSpell:

1. Da clic derecho sobre la base de datos de PostgreSQL.

![etl 04](images/etl04.png)

2. Posteriormente da clic en SQL Script y luego en Run SQL Scripts.

![dataspell run sql script](images/dataspell_run_sql_script.png)

3. Ubica el script descargado dentro de tu computador y da clic en OK.

![etl 06](images/etl06.png)

丘멆잺La creaci칩n de la tabla y la carga de datos puede demorar cerca de 15-20 minutos en DataSpell.

![script sql done](images/script_sql_done.png)

### 5. Prueba la tabla trades

Una vez terminada la ejecuci칩n del script, consulta la tabla Trades ya cargada. Abre el editor de queries desde tu base de datos en DataSpell e ingresa la siguiente consulta:

`SELECT * FROM trades;`

![etl 07](images/etl07.png)

춰Listo! Ya tienes lo esencial para comenzar a extraer datos de una base de datos OLTP y correr tus notebooks de Python.

Avanza a la siguiente clase. 丘뙖잺

## Extracci칩n de datos con Python y Pandas

La **extracci칩n de datos** con Python y Pandas es una pr치ctica com칰n en el an치lisis de datos y en procesos ETL (Extract, Transform, Load). **Pandas** es una biblioteca poderosa que permite manipular y analizar datos estructurados f치cilmente. A continuaci칩n, se describen los m칠todos m치s utilizados para extraer datos con Pandas:

---

## 1. **Extracci칩n desde archivos comunes**

### a) Archivos CSV
```python
import pandas as pd

# Cargar un archivo CSV
df = pd.read_csv('archivo.csv')

# Mostrar las primeras filas
print(df.head())
```

### b) Archivos Excel
```python
# Cargar un archivo Excel
df = pd.read_excel('archivo.xlsx', sheet_name='Hoja1')

# Mostrar resumen de datos
print(df.info())
```

### c) Archivos de texto delimitados
```python
# Cargar archivo con delimitadores personalizados (ejemplo: tabulaci칩n)
df = pd.read_csv('archivo.txt', delimiter='\t')

# Mostrar estad칤sticas descriptivas
print(df.describe())
```

---

## 2. **Extracci칩n desde Bases de Datos**

Pandas puede conectarse a bases de datos relacionales como **MySQL**, **PostgreSQL** y otras utilizando bibliotecas como `SQLAlchemy`.

### Ejemplo con SQLite
```python
import sqlite3

# Conectar a la base de datos
conn = sqlite3.connect('base_de_datos.db')

# Ejecutar una consulta y cargar los datos en un DataFrame
query = "SELECT * FROM tabla"
df = pd.read_sql_query(query, conn)

# Cerrar conexi칩n
conn.close()

print(df)
```

### Ejemplo con MySQL y SQLAlchemy
```python
from sqlalchemy import create_engine

# Crear conexi칩n
engine = create_engine('mysql+pymysql://usuario:contrase침a@host/nombre_base_datos')

# Ejecutar consulta
query = "SELECT * FROM tabla"
df = pd.read_sql(query, engine)

print(df)
```

---

## 3. **Extracci칩n desde APIs**

Pandas puede trabajar con datos obtenidos de APIs, que usualmente est치n en formato **JSON**.

### Ejemplo con `requests`
```python
import pandas as pd
import requests

# Realizar la solicitud
response = requests.get('https://api.ejemplo.com/data')
data = response.json()

# Convertir a DataFrame
df = pd.json_normalize(data)

print(df.head())
```

---

## 4. **Extracci칩n desde fuentes en la nube**

### a) Desde Amazon S3
```python
import boto3
import pandas as pd

# Configurar cliente S3
s3 = boto3.client('s3')

# Descargar archivo
s3.download_file('mi-bucket', 'ruta/al/archivo.csv', 'archivo_local.csv')

# Leer el archivo descargado
df = pd.read_csv('archivo_local.csv')

print(df)
```

### b) Desde Google Sheets
Utilizando la API de Google Sheets.
```python
import pandas as pd
import gspread
from oauth2client.service_account import ServiceAccountCredentials

# Configurar las credenciales
scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
credentials = ServiceAccountCredentials.from_json_keyfile_name('credenciales.json', scope)
client = gspread.authorize(credentials)

# Obtener hoja de c치lculo
sheet = client.open("nombre_hoja").sheet1

# Convertir a DataFrame
data = sheet.get_all_records()
df = pd.DataFrame(data)

print(df.head())
```

---

## 5. **Extracci칩n desde flujos en tiempo real**

Pandas no est치 dise침ado para datos en tiempo real, pero puede integrarse con herramientas como **Kafka** o **Spark** para leer datos en tiempo real y transformarlos.

---

## Recomendaciones:
1. **Validaci칩n de datos:** Verificar datos faltantes o inconsistencias despu칠s de la extracci칩n usando m칠todos como `df.isnull().sum()`.
2. **Eficiencia:** Si trabajas con datos grandes, considera usar `chunksize` al leer archivos o bases de datos.
   ```python
   for chunk in pd.read_csv('archivo_grande.csv', chunksize=1000):
       print(chunk.head())
   ```

Con estas t칠cnicas, puedes extraer datos desde diversas fuentes y trabajar con ellos eficientemente en tus proyectos de ingenier칤a de datos. 游땕