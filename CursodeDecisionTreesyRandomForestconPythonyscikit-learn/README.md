# Curso de Decision Trees y Random Forest con Python y scikit-learn

## Â¿QuÃ© son los Ã¡rboles de decisiÃ³n?

Los **Ã¡rboles de decisiÃ³n** son modelos de aprendizaje supervisado que se utilizan para resolver problemas de **clasificaciÃ³n** y **regresiÃ³n**. Su estructura se asemeja a un Ã¡rbol, donde cada **nodo interno** representa una pregunta o condiciÃ³n sobre una caracterÃ­stica (feature), cada **rama** representa el resultado de esa condiciÃ³n, y cada **hoja** representa una predicciÃ³n final (una clase o un valor numÃ©rico).

### ğŸ” Â¿CÃ³mo funcionan?

1. **DivisiÃ³n del conjunto de datos**:
   En cada nodo, el algoritmo selecciona la caracterÃ­stica que mejor divide los datos segÃºn algÃºn criterio (como Gini, EntropÃ­a o MSE).

2. **ConstrucciÃ³n del Ã¡rbol**:
   El proceso se repite de forma recursiva dividiendo el conjunto en subconjuntos hasta que:

   * Todos los datos en un nodo pertenecen a la misma clase.
   * Se alcanza una profundidad mÃ¡xima.
   * Otras condiciones de parada.

3. **PredicciÃ³n**:
   Para predecir con un Ã¡rbol, se sigue una ruta desde la raÃ­z hasta una hoja, tomando decisiones segÃºn los valores de entrada.

### âœ³ï¸ Ventajas

* FÃ¡cil de entender e interpretar.
* No requiere escalado de variables.
* Puede manejar datos tanto categÃ³ricos como numÃ©ricos.
* Permite visualizar cÃ³mo se toman las decisiones.

### âš ï¸ Desventajas

* Pueden **sobreajustarse** fÃ¡cilmente si no se podan.
* Sensibles a pequeÃ±as variaciones en los datos.
* No suelen ser tan precisos como modelos mÃ¡s complejos (aunque se pueden combinar en **Random Forests** o **Gradient Boosting**).

### ğŸ”§ Ejemplo en Python con Scikit-learn

```python
from sklearn.tree import DecisionTreeClassifier

modelo = DecisionTreeClassifier()
modelo.fit(X_train, y_train)
predicciones = modelo.predict(X_test)
```

**Archivos de la clase**

[decision-trees-random-forest-slides.pdf](https://static.platzi.com/media/public/uploads/decision-trees-random-forest-slides_a67df04d-0c2a-45ec-9ff0-68222424cc81.pdf)

## Tu primer Ã¡rbol de decisiÃ³n con scikit-learn

AquÃ­ tienes un ejemplo completo de **tu primer Ã¡rbol de decisiÃ³n con Scikit-learn**, usando el clÃ¡sico conjunto de datos **Iris**. Es ideal para aprender porque tiene 4 caracterÃ­sticas y 3 clases de flores.

### ğŸŒ± Paso a paso: Ãrbol de DecisiÃ³n en Scikit-learn

### 1. ğŸ“¦ Importar librerÃ­as necesarias

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
```

### 2. ğŸŒ¸ Cargar dataset Iris

```python
iris = load_iris()
X = iris.data
y = iris.target
```

### 3. âœ‚ï¸ Dividir en entrenamiento y prueba

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
```

### 4. ğŸŒ³ Crear y entrenar el Ã¡rbol

```python
modelo = DecisionTreeClassifier(random_state=42)
modelo.fit(X_train, y_train)
```

### 5. ğŸ”® Hacer predicciones

```python
y_pred = modelo.predict(X_test)
```

### 6. âœ… Evaluar el modelo

```python
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nReporte de clasificaciÃ³n:\n", classification_report(y_test, y_pred, target_names=iris.target_names))
```

### 7. ğŸ“ˆ Visualizar el Ã¡rbol de decisiÃ³n

```python
plt.figure(figsize=(12, 8))
plot_tree(modelo, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)
plt.show()
```

### ğŸ§  Â¿QuÃ© hace este Ã¡rbol?

* Separa las flores (Setosa, Versicolor, Virginica) segÃºn el largo y ancho del sÃ©palo y del pÃ©talo.
* En cada nodo, hace una pregunta como â€œÂ¿petal length (cm) <= 2.45?â€ para clasificar.

**Archivos de la clase**

[primer-arbol-decision-template.ipynb](https://static.platzi.com/media/public/uploads/primer_arbol_decision_template_84c03d57-915e-4533-852c-f596f47c81e8.ipynb)
[primer-arbol-decision-completed.ipynb](https://static.platzi.com/media/public/uploads/primer_arbol_decision_completed_c6330ed6-aa2e-4c32-b499-2480b6a5b78f.ipynb)

**Lecturas recomendadas**

[CS109](https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/problem12.html)

[primer_arbol_decision_completed.ipynb - Google Drive](https://drive.google.com/file/d/1t8iSZcNn5Th9I7B_FrzrVXh-68jPwsol/view?usp=sharing)

[primer_arbol_decision_template.ipynb - Google Drive](https://drive.google.com/file/d/1wsxzoUcyLHCJ55YrFW5wMHqYhthWuckR/view?usp=sharing)

## AnÃ¡lisis de datos para tu primer Ã¡rbol de decisiÃ³n

AquÃ­ tienes un anÃ¡lisis bÃ¡sico de los datos antes de construir tu primer Ã¡rbol de decisiÃ³n. Usaremos el dataset **Iris** como ejemplo, que es ideal para comenzar porque:

* Es pequeÃ±o y limpio.
* Tiene 150 observaciones.
* Tiene 4 caracterÃ­sticas numÃ©ricas.
* Su objetivo (target) es predecir el tipo de flor (Setosa, Versicolor o Virginica).

### ğŸ” 1. Cargar y entender los datos

```python
from sklearn.datasets import load_iris
import pandas as pd

iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target, name='species')

X['species'] = y
X.head()
```

**Salida esperada (primeras filas):**

| sepal length (cm) | sepal width (cm) | petal length (cm) | petal width (cm) | species |
| ----------------- | ---------------- | ----------------- | ---------------- | ------- |
| 5.1               | 3.5              | 1.4               | 0.2              | 0       |
| 4.9               | 3.0              | 1.4               | 0.2              | 0       |
| ...               | ...              | ...               | ...              | ...     |

* `species`: 0 = Setosa, 1 = Versicolor, 2 = Virginica

### ğŸ“Š 2. EstadÃ­sticas descriptivas

```python
X.describe()
```

Esto te darÃ¡ un resumen de los valores mÃ­nimos, mÃ¡ximos, medias, etc. Sirve para:

* Ver si hay valores extremos (outliers).
* Ver si las escalas son muy distintas.
* Observar la distribuciÃ³n de cada variable.

### ğŸ“ˆ 3. DistribuciÃ³n de clases

```python
import matplotlib.pyplot as plt
import seaborn as sns

sns.countplot(x='species', data=X)
plt.title('DistribuciÃ³n de clases')
plt.xticks([0, 1, 2], iris.target_names)
plt.show()
```

### ğŸ”— 4. Correlaciones

```python
sns.heatmap(X.drop('species', axis=1).corr(), annot=True, cmap='coolwarm')
plt.title('Matriz de correlaciÃ³n')
plt.show()
```

Esto ayuda a ver quÃ© caracterÃ­sticas estÃ¡n mÃ¡s relacionadas entre sÃ­.

### ğŸ§  5. Observaciones clave

* **Setosa** es fÃ¡cil de separar por su pÃ©talo corto y delgado.
* Las clases **Versicolor** y **Virginica** son mÃ¡s parecidas.
* Las caracterÃ­sticas mÃ¡s importantes suelen ser el **largo y ancho del pÃ©talo**.

**Archivos de la clase**

[primer-arbol-decision-template.ipynb](https://static.platzi.com/media/public/uploads/primer_arbol_decision_template_84c03d57-915e-4533-852c-f596f47c81e8.ipynb)
[primer-arbol-decision-completed.ipynb](https://static.platzi.com/media/public/uploads/primer_arbol_decision_completed_c6330ed6-aa2e-4c32-b499-2480b6a5b78f.ipynb)

**Lecturas recomendadas**

[CS109](https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/problem12.html)

[Curso de AnÃ¡lisis Exploratorio de Datos - Platzi](https://platzi.com/cursos/analisis-exploratorio-datos/)

[primer_arbol_decision_completed.ipynb - Google Drive](https://drive.google.com/file/d/1t8iSZcNn5Th9I7B_FrzrVXh-68jPwsol/view?usp=sharing)

[primer_arbol_decision_template.ipynb - Google Drive](https://drive.google.com/file/d/1wsxzoUcyLHCJ55YrFW5wMHqYhthWuckR/view?usp=sharing)

## Entrenamiento y evaluaciÃ³n de Ã¡rbol de decisiÃ³n con scikit-learn

AquÃ­ tienes un ejemplo **completo** y **explicado paso a paso** de cÃ³mo entrenar y evaluar un **Ã¡rbol de decisiÃ³n** usando `scikit-learn` con el dataset del **Titanic**:

### ğŸ§ª 1. Cargar y preparar los datos

```python
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Cargar el dataset (puede ser desde seaborn, kaggle o archivo local)
titanic = pd.read_csv('titanic.csv')  # o usa seaborn.load_dataset('titanic')

# SelecciÃ³n de variables
features = ['Pclass', 'Sex', 'Age', 'Fare']
target = 'Survived'

# Eliminar nulos simples (sÃ³lo para simplificar el ejemplo)
titanic = titanic.dropna(subset=features + [target])

# Convertir variables categÃ³ricas a numÃ©ricas
titanic = pd.get_dummies(titanic, columns=['Sex'], drop_first=True, dtype=int)

# Variables predictoras y variable objetivo
X = titanic[['Pclass', 'Age', 'Fare', 'Sex_male']]
y = titanic[target]
```

### ğŸ§  2. Dividir en conjunto de entrenamiento y prueba

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

### ğŸŒ² 3. Entrenar el Ã¡rbol de decisiÃ³n

```python
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)
```

### ğŸ“Š 4. EvaluaciÃ³n del modelo

```python
# Predicciones
y_pred = model.predict(X_test)

# MÃ©tricas
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
```

### ğŸŒ³ 5. (Opcional) Visualizar el Ã¡rbol

```python
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(model, feature_names=X.columns, class_names=['No', 'Yes'], filled=True)
plt.show()
```

### âœ… Resultado

Con esto obtendrÃ¡s:

* Un modelo de Ã¡rbol de decisiÃ³n entrenado.
* MÃ©tricas de precisiÃ³n (`accuracy`, `precision`, `recall`, `f1-score`).
* VisualizaciÃ³n clara de cÃ³mo el Ã¡rbol toma decisiones.

## Â¿CÃ³mo funcionan los Ã¡rboles de decisiÃ³n?

Los **Ã¡rboles de decisiÃ³n** son algoritmos de aprendizaje supervisado que se utilizan para **clasificaciÃ³n** o **regresiÃ³n**. Funcionan dividiendo los datos en ramas basadas en preguntas simples sobre las caracterÃ­sticas (features), hasta llegar a una predicciÃ³n.

### ğŸŒ³ Â¿CÃ³mo funciona un Ã¡rbol de decisiÃ³n?

1. **Inicio (nodo raÃ­z)**:
   El Ã¡rbol comienza con todos los datos en un nodo inicial.

2. **DivisiÃ³n (nodos internos)**:
   Se elige la **caracterÃ­stica (feature)** que mejor separa los datos segÃºn un criterio (por ejemplo, **entropÃ­a**, **Ã­ndice Gini** o **reducciÃ³n de varianza**).

3. **Ramas**:
   Se crean ramas segÃºn los valores de esa caracterÃ­stica. Cada rama lleva a un subconjunto del dataset.

4. **RepeticiÃ³n**:
   Este proceso se repite recursivamente en cada subconjunto, formando un Ã¡rbol.

5. **Fin (hojas)**:
   Cuando no se puede dividir mÃ¡s (por ejemplo, los datos estÃ¡n completamente separados o se llega a un lÃ­mite de profundidad), se hace una predicciÃ³n basada en la mayorÃ­a de clase (clasificaciÃ³n) o en el promedio (regresiÃ³n).

### ğŸ§  Ejemplo simple (ClasificaciÃ³n):

**Â¿SobreviviÃ³ una persona en el Titanic?**
Variables:

* Edad
* Sexo
* Clase del boleto

El Ã¡rbol podrÃ­a hacer:

* **Â¿Sex\_female == 1?**

  * SÃ­ â†’ Â¿Pclass <= 2?

    * SÃ­ â†’ Probabilidad alta de sobrevivir
    * No â†’ Probabilidad media
  * No â†’ Â¿Age <= 10?

    * SÃ­ â†’ Probabilidad media
    * No â†’ Probabilidad baja

### âš™ï¸ Criterios de divisiÃ³n comunes:

* **Gini** (por defecto en `sklearn`): mide impureza
* **EntropÃ­a**: mide la cantidad de informaciÃ³n necesaria para clasificar
* **MSE** (para regresiÃ³n): error cuadrÃ¡tico medio

### âœ… Ventajas:

* FÃ¡cil de entender e interpretar
* No requiere normalizaciÃ³n de datos
* Puede trabajar con variables categÃ³ricas y numÃ©ricas

### âŒ Desventajas:

* Puede sobreajustar (overfitting) si no se poda o se limita la profundidad
* Poca estabilidad: cambios pequeÃ±os en los datos pueden cambiar mucho el Ã¡rbol

## Â¿CuÃ¡ndo usar Ã¡rboles de decisiÃ³n?

Debes considerar usar **Ã¡rboles de decisiÃ³n** cuando:

### âœ… **1. Quieres interpretabilidad**

* Los Ã¡rboles son **fÃ¡ciles de visualizar y entender**. Puedes explicar decisiones con reglas simples del tipo â€œsi... entonces...â€.
* Ideal cuando necesitas **explicar el modelo a personas no tÃ©cnicas**.

### âœ… **2. Tus datos incluyen variables categÃ³ricas y numÃ©ricas**

* Los Ã¡rboles manejan bien **ambos tipos** sin necesidad de normalizaciÃ³n ni transformaciÃ³n compleja.

### âœ… **3. Tienes relaciones no lineales**

* A diferencia de la regresiÃ³n lineal, los Ã¡rboles **capturan interacciones y no linealidades** entre variables automÃ¡ticamente.

### âœ… **4. Quieres saber quÃ© variables son mÃ¡s importantes**

* El modelo calcula automÃ¡ticamente **importancia de caracterÃ­sticas**, lo cual es Ãºtil para **selecciÃ³n de variables** o interpretaciÃ³n.

### âœ… **5. Los datos tienen valores faltantes o estÃ¡n mal escalados**

* Los Ã¡rboles son **resistentes** a valores faltantes (algunos algoritmos los manejan bien) y **no necesitan normalizaciÃ³n**.

### âœ… **6. Tu problema es de clasificaciÃ³n o regresiÃ³n**

* Puedes usar Ã¡rboles para:

  * **ClasificaciÃ³n** (ej. detecciÃ³n de spam, predicciÃ³n de enfermedad)
  * **RegresiÃ³n** (ej. predicciÃ³n de precios, demanda, consumo)

### âŒ **Â¿CuÃ¡ndo evitar Ã¡rboles de decisiÃ³n?**

* Cuando necesitas **altÃ­sima precisiÃ³n**: suelen tener peor desempeÃ±o que mÃ©todos como Random Forest o XGBoost.
* Cuando el dataset es **muy pequeÃ±o y complejo**: puede sobreajustar.
* Cuando necesitas **predicciones muy estables**: los Ã¡rboles simples pueden variar bastante ante pequeÃ±os cambios en los datos.

### ğŸ“Œ En resumen:

Usa Ã¡rboles de decisiÃ³n cuando necesitas un modelo **rÃ¡pido, interpretable y versÃ¡til**, especialmente en las primeras etapas del anÃ¡lisis o cuando el entendimiento del modelo es prioritario.

**Lecturas recomendadas**

[Ãrbol de decisiÃ³n en valoraciÃ³n de inversiones | 2023 | Economipedia](https://economipedia.com/definiciones/arbol-de-decision-en-valoracion-de-inversiones.html)

## Conociendo problema a resolver y dataset de clasificaciÃ³n

Para **resolver un problema de clasificaciÃ³n** con **machine learning**, como identificar el tipo de automÃ³vil (por ejemplo: **fÃ³sil**, **elÃ©ctrico** o **hÃ­brido**), necesitas tener claro dos cosas fundamentales:

### âœ… 1. **Conocer el problema a resolver**

Esto implica entender:

* QuÃ© **queremos predecir** â†’ En este caso, el **tipo de automÃ³vil**.
* QuÃ© **tipo de problema es** â†’ Es un **problema de clasificaciÃ³n multiclase**.
* QuÃ© datos tenemos disponibles para predecir â†’ Variables como peso, potencia, tipo de motor, consumo, emisiones, etc.

### âœ… 2. **Conocer y preparar el dataset**

Esto incluye:

#### a. **Variable objetivo (target)**

Es la que vamos a predecir, en este caso:

```python
'y_tipo'  â†’ ['fÃ³sil', 'elÃ©ctrico', 'hÃ­brido']
```

Esta variable debe ser **convertida a valores numÃ©ricos**, por ejemplo usando `LabelEncoder`:

```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['tipo_auto'] = le.fit_transform(df['tipo_auto'])
```

Esto puede traducir:

* fÃ³sil â†’ 0
* elÃ©ctrico â†’ 1
* hÃ­brido â†’ 2

#### b. **Variables predictoras (features)**

Estas son las columnas que usarÃ¡ el modelo para aprender. Por ejemplo:

```python
X = df[['peso', 'potencia', 'consumo', 'emisiones']]
y = df['tipo_auto']  # variable objetivo codificada
```

### âœ… 3. **DivisiÃ³n del dataset**

Siempre se divide el dataset en datos de entrenamiento y prueba:

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### âœ… 4. **Entrenamiento del modelo**

Por ejemplo, con un Ã¡rbol de decisiÃ³n:

```python
from sklearn.tree import DecisionTreeClassifier

modelo = DecisionTreeClassifier()
modelo.fit(X_train, y_train)
```

### âœ… 5. **EvaluaciÃ³n**

Evaluamos quÃ© tan bien predice:

```python
from sklearn.metrics import accuracy_score

y_pred = modelo.predict(X_test)
print("PrecisiÃ³n:", accuracy_score(y_test, y_pred))
```

**Archivos de la clase**

[decision-tree-random-forest-project-template.ipynb](https://static.platzi.com/media/public/uploads/decision_tree_random_forest_project_template_fd59d141-d477-4f10-b39e-8127e0fbccb8.ipynb)
[decision-tree-random-forest-project-completed.ipynb](https://static.platzi.com/media/public/uploads/decision_tree_random_forest_project_completed_ede3a4ea-5f79-4ed7-9fbc-bdfd3b16b017.ipynb)

**Lecturas recomendadas**

[Car Evaluation Data Set | Kaggle](https://www.kaggle.com/datasets/elikplim/car-evaluation-data-set)

[decision_tree_random_forest_project_completed.ipynb - Google Drive](https://drive.google.com/file/d/1Ck8R2GXK_ZeW9oYRIdXgVhBqi3ibt_QJ/view?usp=sharing)

[decision_tree_random_forest_project_template.ipynb - Google Drive](https://drive.google.com/file/d/1PFP6e4YfAI8nXq31kzRC8NquONJqLqeK/view?usp=sharing)

## AnÃ¡lisis exploratorio de datos para Ã¡rbol de decisiÃ³n

El **anÃ¡lisis exploratorio de datos (EDA)** es un paso clave antes de aplicar un algoritmo como un **Ã¡rbol de decisiÃ³n**, ya que te permite:

* Entender la estructura y calidad del dataset.
* Detectar valores faltantes o atÃ­picos.
* Visualizar relaciones entre variables.
* Evaluar quÃ© variables podrÃ­an ser importantes para la predicciÃ³n.

### âœ… Pasos del AnÃ¡lisis Exploratorio de Datos (EDA) para Ãrbol de DecisiÃ³n

### 1. **Cargar los datos y revisar estructura**

```python
import pandas as pd

df = pd.read_csv('autos.csv')  # ejemplo
print(df.head())
print(df.info())
print(df.describe())
```

### 2. **Identificar la variable objetivo (target)**

Verifica si es una variable **categÃ³rica** (clasificaciÃ³n) o **numÃ©rica** (regresiÃ³n).

```python
print(df['tipo_auto'].value_counts())
```

Ejemplo de categorÃ­as: `['fÃ³sil', 'elÃ©ctrico', 'hÃ­brido']`

### 3. **Visualizar la distribuciÃ³n de la variable target**

```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.countplot(x='tipo_auto', data=df)
plt.title('DistribuciÃ³n del tipo de automÃ³vil')
plt.show()
```

### 4. **Revisar valores nulos**

```python
print(df.isnull().sum())
```

Soluciones:

* Imputar valores nulos (media, moda, etc.)
* Eliminar columnas o filas con muchos nulos

### 5. **Revisar correlaciones entre variables numÃ©ricas**

Aunque los Ã¡rboles no necesitan variables escaladas ni normalizadas, es Ãºtil conocer la relaciÃ³n entre variables.

```python
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
```

### 6. **Codificar variables categÃ³ricas**

Los Ã¡rboles pueden trabajar con etiquetas numÃ©ricas, asÃ­ que debes convertir las categorÃ­as:

```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['tipo_auto'] = le.fit_transform(df['tipo_auto'])
```

Aplica lo mismo a otras columnas categÃ³ricas si es necesario.

### 7. **Detectar outliers**

Puedes usar diagramas de caja (boxplots):

```python
sns.boxplot(x=df['potencia'])
plt.title('Potencia - detecciÃ³n de outliers')
plt.show()
```

### 8. **AnÃ¡lisis bivariado**

Estudia cÃ³mo se relacionan las variables predictoras con la variable objetivo:

```python
sns.boxplot(x='tipo_auto', y='consumo', data=df)
```

### 9. **Feature importance (opcional luego del modelo)**

Los Ã¡rboles te permiten saber quÃ© variables son mÃ¡s importantes despuÃ©s del entrenamiento:

```python
from sklearn.tree import DecisionTreeClassifier

X = df.drop('tipo_auto', axis=1)
y = df['tipo_auto']

modelo = DecisionTreeClassifier()
modelo.fit(X, y)

importances = modelo.feature_importances_
for col, imp in zip(X.columns, importances):
    print(f"{col}: {imp:.3f}")
```

## Procesamiento de datos para el entrenamiento de Ã¡rbol de decisiÃ³n

El **procesamiento de datos** para entrenar un **Ã¡rbol de decisiÃ³n** implica preparar tu dataset de forma que el algoritmo pueda aprender patrones de manera efectiva. Aunque los Ã¡rboles de decisiÃ³n son muy flexibles (no requieren escalado de variables, por ejemplo), **sÃ­ necesitan ciertos pasos clave** para funcionar correctamente.

### âœ… Pasos de Procesamiento de Datos para Ãrbol de DecisiÃ³n (ClasificaciÃ³n o RegresiÃ³n)

### 1. **Separar variables predictoras y objetivo**

```python
X = df.drop('target', axis=1)  # variables predictoras
y = df['target']               # variable objetivo
```

### 2. **Codificar variables categÃ³ricas**

Los Ã¡rboles requieren valores numÃ©ricos.

**OpciÃ³n A: Label Encoding (Ãºtil si hay orden implÃ­cito o pocas categorÃ­as)**

```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
X['transmision'] = le.fit_transform(X['transmision'])
```

**OpciÃ³n B: One-Hot Encoding (Ãºtil para categorÃ­as sin orden)**

```python
X = pd.get_dummies(X, columns=['marca', 'modelo'])
```

### 3. **Manejo de valores nulos**

Los Ã¡rboles no manejan valores faltantes por sÃ­ solos.

```python
X = X.fillna(X.median())  # o usar X.dropna() si es apropiado
```

### 4. **DivisiÃ³n de datos en entrenamiento y prueba**

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

### 5. **(Opcional) Balancear clases si es clasificaciÃ³n**

Si tienes un problema de clasificaciÃ³n desbalanceada:

```python
from sklearn.utils import resample

# Combinar X e y
df_train = pd.concat([X_train, y_train], axis=1)
minority = df_train[df_train['target'] == 'clase_rara']
majority = df_train[df_train['target'] == 'clase_comÃºn']

minority_upsampled = resample(minority,
                              replace=True,
                              n_samples=len(majority),
                              random_state=42)

df_balanced = pd.concat([majority, minority_upsampled])
X_train = df_balanced.drop('target', axis=1)
y_train = df_balanced['target']
```

### 6. **Entrenamiento del Ã¡rbol**

```python
from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(max_depth=5, random_state=42)
clf.fit(X_train, y_train)
```

### 7. **EvaluaciÃ³n**

```python
from sklearn.metrics import classification_report

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred))
```

### 8. **(Opcional) Visualizar el Ã¡rbol**

```python
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(16, 8))
plot_tree(clf, feature_names=X.columns, class_names=clf.classes_, filled=True)
plt.show()
```

## Entrenamiento de modelo de clasificaciÃ³n con Ã¡rbol de decisiÃ³n

AquÃ­ tienes un ejemplo completo y claro de **entrenamiento de un modelo de clasificaciÃ³n con Ã¡rbol de decisiÃ³n**, usando Python y `scikit-learn`.

### âœ… **Ejemplo paso a paso: ClasificaciÃ³n con Ãrbol de DecisiÃ³n**

Usaremos el dataset `Iris` como ejemplo.

#### **1. Importar librerÃ­as**

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
```

#### **2. Cargar datos**

```python
iris = load_iris()
X = iris.data                  # variables predictoras
y = iris.target                # variable objetivo (0, 1, 2)
```

#### **3. Dividir en entrenamiento y prueba**

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

#### **4. Entrenar el Ã¡rbol de decisiÃ³n**

```python
clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)
clf.fit(X_train, y_train)
```

#### **5. Predecir y evaluar**

```python
y_pred = clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nReporte de clasificaciÃ³n:\n", classification_report(y_test, y_pred))
```

#### **6. (Opcional) Visualizar el Ã¡rbol**

```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(12, 6))
plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)
plt.show()
```

### ğŸ” Â¿QuÃ© puedes modificar?

* Cambia `max_depth`, `criterion`, `min_samples_split`, etc. para ver cÃ³mo afecta al rendimiento.
* Usa tus propios datos (`pd.read_csv(...)`) y reemplaza `X` e `y`.

**Lecturas recomendadas**

[1.10. Decision Trees â€” scikit-learn 1.2.1 documentation](https://scikit-learn.org/stable/modules/tree.html)

[sklearn.tree.DecisionTreeClassifier â€” scikit-learn 1.2.1 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)

[decision_tree_random_forest_project_completed.ipynb - Google Drive](https://drive.google.com/file/d/1Ck8R2GXK_ZeW9oYRIdXgVhBqi3ibt_QJ/view?usp=sharing)

[decision_tree_random_forest_project_template.ipynb - Google Drive](https://drive.google.com/file/d/1PFP6e4YfAI8nXq31kzRC8NquONJqLqeK/view?usp=sharing)