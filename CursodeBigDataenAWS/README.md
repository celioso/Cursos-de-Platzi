# Curso de Big Data en AWS

## Iniciando con Big Data

Big Data es un campo orientado al anÃ¡lisis, procesamiento y almacenamiento de grandes cantidades de informaciÃ³n que permite mejorar el valor de tu negocio. Utilizando la informaciÃ³n mediante este esquema podemos detectar puntos de optimizaciÃ³n en diferentes Ã¡reas.

Actualmente encontramos diferentes proveedores de Cloud computing con soporte de Big Data, compitiendo entre sÃ­ por atraer la mayor cantidad de clientes a sus nubes, destacando Amazon Web Services por sus mÃºltiples servicios para manejo de grandes cantidades de informaciÃ³n.

En este curso aprenderÃ¡s:

- Â¿CÃ³mo tomar data desde el origen para llevarla a Cloud?
- Â¿CÃ³mo transformar la data?
- Â¿CÃ³mo visualizar la data?
- Â¿CÃ³mo proteger los datos?

Tomando en cuenta tres aspectos importantes:

1. Automatizar todos los procesos.
2. Orquestar las distintas tareas.
3. Involucrar un buen nivel de seguridad sobre nuestros datos.

Antes de comenzar este curso te sugerimos tomar previamente los cursos de :

[Amazon Web Services](https://platzi.com/aws/)
[Cloud Computing](https://platzi.com/clases/aws-computo/)
[Storage en AWS](https://platzi.com/clases/storage-aws/)
[Base de datos en AWS](https://platzi.com/clases/db-aws/)

Â¡Nunca pares de aprender!

## Cloud Computing en proyectos de BigData

El **Cloud Computing** ha revolucionado la manera en que se gestionan y procesan grandes volÃºmenes de datos en proyectos de **Big Data**. La escalabilidad, flexibilidad y el pago por uso de la nube permiten a las empresas analizar grandes cantidades de informaciÃ³n sin necesidad de invertir en infraestructura costosa.

### **ğŸš€ Beneficios del Cloud Computing en Big Data**  

1ï¸âƒ£ **Escalabilidad**  
   - Permite ajustar recursos (CPU, memoria, almacenamiento) de forma dinÃ¡mica segÃºn la demanda.  
   - Ejemplo: **AWS EMR, Google Dataproc, Azure HDInsight**.  

2ï¸âƒ£ **Procesamiento Distribuido**  
   - Uso de tecnologÃ­as como **Apache Hadoop, Apache Spark** para procesamiento masivo de datos en paralelo.  
   - Ejemplo: AWS Glue, Databricks en Azure.  

3ï¸âƒ£ **Almacenamiento Flexible y EconÃ³mico**  
   - Bases de datos optimizadas para Big Data como **Amazon S3, Google Cloud Storage, Azure Blob Storage**.  
   - Bases de datos NoSQL: **Amazon DynamoDB, Google Bigtable, MongoDB Atlas**.  

4ï¸âƒ£ **Bajo Costo**  
   - Pago por uso evita inversiones en hardware.  
   - Uso de instancias spot/preemptibles para reducir costos.  

5ï¸âƒ£ **Seguridad y Cumplimiento**  
   - Cifrado, control de acceso y auditorÃ­as en tiempo real.  
   - Normativas como **GDPR, HIPAA, SOC2**.  

6ï¸âƒ£ **IntegraciÃ³n con Machine Learning e IA**  
   - Servicios como **AWS SageMaker, Google Vertex AI, Azure ML** para entrenar modelos con grandes volÃºmenes de datos.

### **ğŸ”§ Herramientas Cloud para Big Data**  

âœ… **Almacenamiento:**  
   - **Amazon S3, Google Cloud Storage, Azure Data Lake**  

âœ… **Procesamiento de datos:**  
   - **Amazon EMR (Hadoop/Spark), Google Dataflow (Apache Beam), Azure HDInsight**  

âœ… **Bases de datos:**  
   - **Amazon Redshift, BigQuery, Snowflake, Azure Synapse Analytics**  

âœ… **Streaming de datos:**  
   - **Kafka en AWS MSK, Google Pub/Sub, Azure Event Hubs**  

âœ… **VisualizaciÃ³n de datos:**  
   - **Amazon QuickSight, Google Looker, Power BI, Tableau** 

### **ğŸ“Œ Casos de uso en Big Data con Cloud Computing**  

ğŸ”¹ **AnÃ¡lisis de redes sociales** ğŸ“Š â†’ Procesamiento en tiempo real de tweets con **Kafka + Spark en AWS**.  
ğŸ”¹ **Recomendaciones de productos** ğŸ›’ â†’ Uso de **BigQuery + TensorFlow en GCP** para anÃ¡lisis de comportamiento.  
ğŸ”¹ **DetecciÃ³n de fraudes financieros** ğŸ’³ â†’ Procesamiento en **Azure Synapse + ML** para detectar transacciones sospechosas.  
ğŸ”¹ **Salud y genÃ³mica** ğŸ§¬ â†’ AnÃ¡lisis de grandes volÃºmenes de datos genÃ©ticos con **AWS Lambda + S3 + SageMaker**.

### **ğŸš€ ConclusiÃ³n**  
El **Cloud Computing** permite manejar proyectos de **Big Data** de manera eficiente, escalable y econÃ³mica. Su integraciÃ³n con herramientas de **IA y Machine Learning** facilita la creaciÃ³n de soluciones avanzadas para distintos sectores.

**Resumen**

En este curso impartido por Carlos Zambrano, profesor con gran experiencia en el sector financiero y de transformaciÃ³n de data, vas a aprender a:

- CÃ³mo tomar data desde el origen para llevarla a Cloud.
- CÃ³mo transformar la data.
- CÃ³mo visualizar la data.

Tomando en cuenta tres aspectos importantes:

1. Automatizar todos los procesos.
2. Orquestar las distintas tareas.
3. Involucrar un buen nivel de seguridad sobre nuestros datos.

**Lecturas recomendadas**

[Map AWS services to Google Cloud Platform products  |  Google Cloud Platform Free Tier  |  Google Cloud](https://cloud.google.com/free/docs/map-aws-google-cloud-platform)

[Â¿QuÃ© son los big data? â€“ Amazon Web Services (AWS)](https://aws.amazon.com/es/big-data/what-is-big-data/)

[Big Data Analytics Solutions  |  Google Cloud](https://cloud.google.com/solutions/big-data/)

[Big Data and Advanced Analytics Solutions | Microsoft Azure](https://azure.microsoft.com/en-us/solutions/big-data/)

## IntroducciÃ³n al manejo de datos en Cloud

El **manejo de datos en la nube (Cloud Data Management)** es una estrategia clave para empresas y organizaciones que desean almacenar, procesar y analizar datos de manera eficiente y escalable. La computaciÃ³n en la nube ha transformado la forma en que los datos son gestionados, eliminando las limitaciones de la infraestructura local y ofreciendo soluciones flexibles y de pago por uso.

### **ğŸ”¹ Â¿QuÃ© es el Manejo de Datos en la Nube?**  

Es el conjunto de tÃ©cnicas y herramientas utilizadas para **almacenar, procesar, proteger y analizar datos** en plataformas de nube como **AWS, Google Cloud y Microsoft Azure**.  

Los datos pueden almacenarse en diferentes formatos y tipos de almacenamiento, dependiendo de su estructura y finalidad:  

âœ… **Estructurados** â†’ Bases de datos relacionales (SQL).  
âœ… **No estructurados** â†’ Archivos multimedia, documentos, correos electrÃ³nicos.  
âœ… **Semiestructurados** â†’ JSON, XML, logs de servidores.

### **ğŸš€ Beneficios del Manejo de Datos en la Nube**  

1ï¸âƒ£ **Escalabilidad**  
   - Capacidad de aumentar o reducir los recursos segÃºn la demanda.  
   - Ejemplo: **Google BigQuery escala automÃ¡ticamente** segÃºn las consultas.  

2ï¸âƒ£ **Costo-Eficiencia**  
   - Pago por uso, evitando inversiones en hardware.  
   - Opciones de almacenamiento econÃ³mico como **Amazon S3 Glacier** para datos archivados.  

3ï¸âƒ£ **Alta Disponibilidad y Resiliencia**  
   - ReplicaciÃ³n de datos en mÃºltiples regiones.  
   - Ejemplo: **Azure Storage** replica datos en diferentes ubicaciones.  

4ï¸âƒ£ **Seguridad y Cumplimiento**  
   - Cifrado de datos en trÃ¡nsito y en reposo.  
   - Cumplimiento con normativas como **GDPR, HIPAA, SOC2**.  

5ï¸âƒ£ **Accesibilidad Global**  
   - Datos accesibles desde cualquier parte del mundo con baja latencia.  
   - IntegraciÃ³n con **APIs y herramientas de analÃ­tica avanzada**.

### **ğŸ”§ Principales Servicios Cloud para el Manejo de Datos**  

ğŸ“Œ **Almacenamiento**  
   - **Amazon S3, Google Cloud Storage, Azure Blob Storage** â†’ Archivos y datos no estructurados.  
   - **Amazon EBS, Google Persistent Disk, Azure Managed Disks** â†’ Almacenamiento para mÃ¡quinas virtuales.  

ğŸ“Œ **Bases de Datos**  
   - **Relacionales:** Amazon RDS, Cloud SQL, Azure SQL Database.  
   - **NoSQL:** Amazon DynamoDB, Google Firestore, Azure Cosmos DB.  

ğŸ“Œ **Procesamiento de Datos**  
   - **Batch:** AWS Glue, Google Dataflow, Azure Data Factory.  
   - **Streaming:** AWS Kinesis, Google Pub/Sub, Azure Event Hubs.  

ğŸ“Œ **AnÃ¡lisis y VisualizaciÃ³n**  
   - **BigQuery (GCP), Amazon Redshift, Azure Synapse** â†’ AnalÃ­tica de datos a gran escala.  
   - **Amazon QuickSight, Google Looker, Power BI** â†’ Dashboards e informes.

### **ğŸ“Š Casos de Uso en la Nube**  

ğŸ”¹ **Empresas de Retail** ğŸ›’ â†’ Uso de **BigQuery** para anÃ¡lisis de tendencias de compra.  
ğŸ”¹ **Finanzas** ğŸ’³ â†’ **DynamoDB + SageMaker** para detecciÃ³n de fraudes en AWS.  
ğŸ”¹ **Salud** ğŸ¥ â†’ **FHIR en Google Cloud** para gestionar historiales clÃ­nicos.  
ğŸ”¹ **Streaming y Entretenimiento** ğŸ¬ â†’ Uso de **Azure Media Services** para distribuciÃ³n de contenido.

### **ğŸŒŸ ConclusiÃ³n**  
El manejo de datos en la nube permite a las organizaciones ser mÃ¡s Ã¡giles, reducir costos y aprovechar el poder del **Big Data y la Inteligencia Artificial**. La combinaciÃ³n de almacenamiento escalable, seguridad robusta y herramientas avanzadas de anÃ¡lisis hacen de la nube la mejor opciÃ³n para gestionar datos en la era digital.

### Resumen

Algunos puntos para tomar en cuenta al momento de iniciar en el manejo de datos en la nube, sin importar quÃ© servicio utilices, son:

- Cuando trabajas en la nube puedes tener un **crecimiento completamente escalable**, iniciando desde MB hasta EB.
- A medida que tu aplicaciÃ³n crezca puedes ir escalando el procesamiento de datos en la nube.
- En Cloud tienes acceso a un **gran nivel de eficiencia** a un bajo costo, solamente te van a cobrar mientras utilices las herramientas.
- Existen muchos servicios de procesamiento en la nube, escoge el que mÃ¡s se acomode a tus necesidades.

## Datos en Cloud

El almacenamiento y gestiÃ³n de datos en la nube ha revolucionado la forma en que las empresas y organizaciones manejan la informaciÃ³n. Con la llegada de tecnologÃ­as de **Cloud Computing**, los datos ahora pueden ser almacenados, procesados y analizados con mayor flexibilidad, escalabilidad y seguridad.

### **ğŸ“Œ Â¿QuÃ© son los Datos en la Nube?**  

Los **Datos en la Nube** son toda la informaciÃ³n que se almacena y procesa en servidores remotos gestionados por proveedores de servicios en la nube como **Amazon Web Services (AWS), Google Cloud Platform (GCP) y Microsoft Azure**. Estos datos pueden incluir:  

âœ… **Datos estructurados** â†’ Bases de datos relacionales (SQL).  
âœ… **Datos semiestructurados** â†’ JSON, XML, logs de servidores.  
âœ… **Datos no estructurados** â†’ Archivos multimedia, correos electrÃ³nicos, documentos.  

### **ğŸš€ Beneficios de Almacenar Datos en la Nube**  

ğŸ”¹ **Escalabilidad**: Se ajusta automÃ¡ticamente segÃºn la demanda.  
ğŸ”¹ **ReducciÃ³n de costos**: Pago por uso, eliminando infraestructura fÃ­sica.  
ğŸ”¹ **Accesibilidad global**: Disponibilidad en cualquier parte del mundo.  
ğŸ”¹ **Seguridad avanzada**: Cifrado, autenticaciÃ³n y cumplimiento normativo.  
ğŸ”¹ **Alta disponibilidad y recuperaciÃ³n ante desastres**: ReplicaciÃ³n en mÃºltiples regiones.

### **ğŸ”§ Tipos de Almacenamiento de Datos en la Nube**  

### **1ï¸âƒ£ Almacenamiento de Archivos**  
ğŸ“Œ **Ejemplos:**  
- **Amazon S3** (AWS)  
- **Google Cloud Storage** (GCP)  
- **Azure Blob Storage** (Microsoft)  
ğŸ“Œ **Usos:**  
- Almacenamiento de imÃ¡genes, videos y documentos.  
- Backup y recuperaciÃ³n de datos.  

### **2ï¸âƒ£ Bases de Datos Relacionales (SQL)**  
ğŸ“Œ **Ejemplos:**  
- **Amazon RDS** (AWS)  
- **Cloud SQL** (GCP)  
- **Azure SQL Database** (Microsoft)  
ğŸ“Œ **Usos:**  
- Aplicaciones empresariales y de e-commerce.  
- Sistemas de gestiÃ³n de clientes (CRM).  

### **3ï¸âƒ£ Bases de Datos NoSQL**  
ğŸ“Œ **Ejemplos:**  
- **Amazon DynamoDB** (AWS)  
- **Google Firestore** (GCP)  
- **Azure Cosmos DB** (Microsoft)  
ğŸ“Œ **Usos:**  
- Aplicaciones con datos de rÃ¡pida lectura/escritura.  
- Almacenamiento de sesiones web y perfiles de usuario.  

### **4ï¸âƒ£ Almacenamiento de Datos para Big Data**  
ğŸ“Œ **Ejemplos:**  
- **Amazon Redshift** (AWS)  
- **Google BigQuery** (GCP)  
- **Azure Synapse Analytics** (Microsoft)  
ğŸ“Œ **Usos:**  
- Procesamiento y anÃ¡lisis de grandes volÃºmenes de datos.  
- Machine Learning y anÃ¡lisis predictivo. 

### **ğŸ“Š Casos de Uso de Datos en la Nube**  

ğŸ¥ **Salud:** Historias clÃ­nicas en la nube con **FHIR en Google Cloud**.  
ğŸ¦ **Finanzas:** AnÃ¡lisis de fraude en **AWS usando Machine Learning**.  
ğŸ›ï¸ **E-commerce:** PersonalizaciÃ³n de productos con **BigQuery en Google Cloud**.  
ğŸ¬ **Entretenimiento:** Streaming de contenido con **Azure Media Services**. 

### **ğŸŒŸ ConclusiÃ³n**  
El almacenamiento y manejo de **datos en la nube** permite a las organizaciones optimizar costos, mejorar la seguridad y potenciar el anÃ¡lisis de datos. Con opciones de almacenamiento escalables y herramientas avanzadas, la nube es la mejor opciÃ³n para gestionar datos en la era digital.

### Resumen

Hay algunos puntos importantes que debemos tener en cuenta al momento de manejar nuestra data en un servicio en la nube:

- Debemos seleccionar el servicio que mejor se ajuste a nuestras necesidades de almacenamiento.
- Lo primero que debemos hacer es extraer de otras fuentes la informaciÃ³n que vamos a necesitar.
- Debemos validar nuestra informaciÃ³n, verificar que sea consistente.
- Verificar los tipos de datos que vamos a extraer.
- Al momento de realizar pruebas a nuestra informaciÃ³n debemos utilizar un subset de la data.

## Â¿QuÃ© nube deberÃ­a utilizar en mi proyecto de Big Data?

Actualmente el mercado de Cloud Computing tiene varios actores compitiendo entre sÃ­ por atraer la mayor cantidad de clientes a sus nubes, encontramos MÃºltiples opciones como: Amazon Web Services, Azure, Alibaba Cloud, Google Cloud Platform, Oracle Cloud, Rackspace, Digital Ocean y Softlayer entre muchas otras.

Dentro de esta variedad de proveedores muchas veces es complejo tomar decisiones de cuÃ¡l utilizar, el criterio para esta decisiÃ³n puede estar dado por diferentes factores como:

1. **Costo**: Valor de los servicios que serÃ¡n utilizados en el proyecto.
2. **Tipo de pricing**: Por demanda (por hora, minuto o segundo), subasta, reservado.
3. **Servicios**: Variedad de servicios provistos por el cloud provider. Â¿CuÃ¡l servicio se ajusta mejor a mis necesidades?
4. **UbicaciÃ³n**: DistribuciÃ³n de las regiones/zonas donde el cloud provider preste servicios por temas de latencia y experiencia usuario esto puede ser decisivo.
5. **Niveles de Servicio**: Consultar la documentaciÃ³n por servicio y los niveles ofrecidos de disponibilidad.
6. **Soporte**: Tipos de soporte, costo, tiempos de respuesta y nivel de soporte (basic, business, enterprise).
7. **Estudios de mercado**: Revisar los diferentes estudios de mercado, por ejemplo: el cuadrante mÃ¡gico de Gartner, en los cuales se evalÃºan en diferentes aspectos los servicios provistos.
8. **DocumentaciÃ³n**: Consultar la documentaciÃ³n de los cloud provider, muchas veces no es muy clara o estÃ¡ incompleta referente a sus servicios.

![Infografia big data AWS v3 curso](images/Infografia-big-data-AWS-v3-curso.png)

DespuÃ©s de revisar las diferentes opciones que proveen los cloud providers encontramos variedad en servicios de acuerdo a su funcionalidad, otras nubes como Azure, Softlayer, Alibaba tambiÃ©n cuentan con servicios orientados al procesamiento de datos, sin embargo dentro de su ecosistema no es tan completo el set de servicios, por tal motivo siempre que pensemos en proyectos de BigData los mejores cloud provider serÃ¡n AWS y GCP que estudiaras en este curso.

## Arquitecturas Lambda

Las **arquitecturas Lambda** son un enfoque de diseÃ±o en **computaciÃ³n en la nube** que permite el procesamiento de datos en tiempo real con alta escalabilidad y eficiencia. Son ampliamente utilizadas en aplicaciones de **Big Data, anÃ¡lisis en tiempo real e Internet de las Cosas (IoT)**.

### **ğŸ“Œ Â¿QuÃ© es una Arquitectura Lambda?**  

Una **Arquitectura Lambda** es un modelo diseÃ±ado para **procesar y analizar grandes volÃºmenes de datos en tiempo real y en lotes** de manera eficiente. Se basa en dos capas principales:  

1ï¸âƒ£ **Capa de Velocidad (Speed Layer)** â†’ Procesa datos en tiempo real.  
2ï¸âƒ£ **Capa de Batch (Batch Layer)** â†’ Procesa datos histÃ³ricos o en lotes.  
3ï¸âƒ£ **Capa de Servicio (Serving Layer)** â†’ Combina los resultados de ambas capas y los expone a aplicaciones.  

ğŸ’¡ **Este enfoque combina lo mejor de los sistemas de procesamiento en tiempo real y en batch, garantizando baja latencia y datos precisos.**  


### **ğŸš€ Componentes de una Arquitectura Lambda**  

### **1ï¸âƒ£ Capa de Ingesta de Datos**  
ğŸ“Œ **Recibe y almacena datos desde mÃºltiples fuentes.**  
ğŸ”¹ **Ejemplos:**  
- Amazon Kinesis (AWS)  
- Google Pub/Sub (GCP)  
- Azure Event Hub (Microsoft)  

### **2ï¸âƒ£ Capa de Procesamiento en Batch**  
ğŸ“Œ **Procesa grandes volÃºmenes de datos con alta precisiÃ³n.**  
ğŸ”¹ **Ejemplos:**  
- Apache Hadoop  
- AWS Glue  
- Google Dataflow  

### **3ï¸âƒ£ Capa de Procesamiento en Tiempo Real (Stream Processing)**  
ğŸ“Œ **Analiza datos con baja latencia en tiempo real.**  
ğŸ”¹ **Ejemplos:**  
- Apache Kafka  
- AWS Lambda  
- Azure Stream Analytics  

### **4ï¸âƒ£ Capa de Almacenamiento**  
ğŸ“Œ **Guarda datos para anÃ¡lisis y consultas futuras.**  
ğŸ”¹ **Ejemplos:**  
- Amazon S3 (AWS)  
- Google BigQuery (GCP)  
- Azure Data Lake (Microsoft)  

### **5ï¸âƒ£ Capa de Servicio (Serving Layer)**  
ğŸ“Œ **Proporciona acceso a los datos para dashboards y aplicaciones.**  
ğŸ”¹ **Ejemplos:**  
- Amazon Redshift  
- Google Looker Studio  
- Azure Synapse Analytics

### **ğŸ› ï¸ Caso de Uso: Procesamiento de Datos en Streaming con AWS Lambda**  

ğŸ”¹ **Escenario:** Una empresa de e-commerce quiere analizar el comportamiento de los usuarios en tiempo real.  

ğŸ”¹ **SoluciÃ³n con Arquitectura Lambda en AWS:**  
1ï¸âƒ£ **AWS Kinesis** captura eventos de navegaciÃ³n web.  
2ï¸âƒ£ **AWS Lambda** procesa los eventos en tiempo real.  
3ï¸âƒ£ **Amazon DynamoDB** almacena la informaciÃ³n procesada.  
4ï¸âƒ£ **Amazon QuickSight** genera reportes y visualizaciones.

### **ğŸ”„ Diferencia entre Arquitectura Lambda y Kappa**  

| CaracterÃ­stica  | **Lambda**  | **Kappa**  |
|---------------|------------|------------|
| Procesamiento | Batch + Streaming | Solo Streaming |
| Complejidad  | Alta (doble pipeline) | Baja (un solo pipeline) |
| Uso comÃºn    | Big Data, IoT | Machine Learning, IoT |

ğŸ’¡ **Si tu aplicaciÃ³n requiere procesamiento en tiempo real y en batch, usa Lambda. Si solo necesitas datos en streaming, Kappa es mejor.**

### **ğŸŒŸ Beneficios de las Arquitecturas Lambda**  

âœ… **Escalabilidad automÃ¡tica** â†’ Se adapta a grandes volÃºmenes de datos.  
âœ… **Baja latencia** â†’ Procesa informaciÃ³n en tiempo real.  
âœ… **Flexibilidad** â†’ Compatible con mÃºltiples tecnologÃ­as.  
âœ… **Alta disponibilidad** â†’ ReplicaciÃ³n y tolerancia a fallos.  

### **ğŸ” ConclusiÃ³n**  
Las **Arquitecturas Lambda** son ideales para manejar **Big Data, anÃ¡lisis en tiempo real e IoT**. Su combinaciÃ³n de procesamiento batch y en streaming las hace versÃ¡tiles y poderosas en la era digital.

**Resumen**

La arquitectura Lambda es atribuida a Nathan Marz, diseÃ±ada para ser escalable, tolerante a fallos y de alto procesamiento de datos.

Tiene una gran robustez, puede procesar una alta cantidad de datos. EstÃ¡ compuesta por tres capas:

1. **Batch**: En esta capa vamos a procesar toda la informaciÃ³n almacenada con anterioridad, desde el dÃ­a anterior hasta meses.
2. **Serve**: Dentro de esta capa es posible visualizar la data procesada de la capa batch.
3. **Speed**: Conforme llega la data se va a ir procesando.

## Arquitectura Kappa

La **Arquitectura Kappa** es un enfoque de diseÃ±o para el procesamiento de datos **en tiempo real**, eliminando la necesidad de procesar datos en batch. Es ideal para aplicaciones de **Big Data, Machine Learning, IoT y anÃ¡lisis en streaming**.

### **ğŸ“Œ Â¿QuÃ© es una Arquitectura Kappa?**  

Es un modelo que **procesa datos exclusivamente en tiempo real** mediante **event streaming**. A diferencia de la **Arquitectura Lambda**, **no tiene una capa de batch**, lo que simplifica la infraestructura y reduce la latencia.  

ğŸ’¡ **Se basa en un Ãºnico pipeline de datos en streaming.**

### **ğŸš€ Componentes de una Arquitectura Kappa**  

### **1ï¸âƒ£ Capa de Ingesta de Datos**  
ğŸ“Œ **Recibe datos en tiempo real desde mÃºltiples fuentes.**  
ğŸ”¹ **Ejemplos:**  
- Apache Kafka  
- Amazon Kinesis  
- Google Pub/Sub  
- Azure Event Hub  

### **2ï¸âƒ£ Capa de Procesamiento en Streaming**  
ğŸ“Œ **Transforma y analiza datos en tiempo real.**  
ğŸ”¹ **Ejemplos:**  
- Apache Flink  
- Apache Spark Streaming  
- AWS Lambda  
- Google Dataflow  

### **3ï¸âƒ£ Capa de Almacenamiento**  
ğŸ“Œ **Guarda datos estructurados y no estructurados para consultas futuras.**  
ğŸ”¹ **Ejemplos:**  
- Amazon S3 (AWS)  
- Google BigQuery (GCP)  
- Azure Data Lake (Microsoft)  

### **4ï¸âƒ£ Capa de Consumo y VisualizaciÃ³n**  
ğŸ“Œ **Permite a los usuarios acceder a los datos procesados.**  
ğŸ”¹ **Ejemplos:**  
- Amazon QuickSight  
- Google Looker Studio  
- Power BI

### **ğŸ”„ Diferencia entre Arquitectura Kappa y Lambda**  

| CaracterÃ­stica  | **Lambda**  | **Kappa**  |
|---------------|------------|------------|
| Procesamiento | Batch + Streaming | Solo Streaming |
| Complejidad  | Alta (doble pipeline) | Baja (un solo pipeline) |
| Latencia     | Baja, pero con batch | MÃ­nima |
| Uso comÃºn    | Big Data, IoT | IoT, ML, Finanzas |

ğŸ’¡ **Si necesitas procesamiento en batch y en tiempo real, usa Lambda. Si solo necesitas tiempo real, Kappa es mejor.**

### **ğŸ› ï¸ Caso de Uso: AnÃ¡lisis de Sensores IoT con Kappa**  

ğŸ”¹ **Escenario:** Una empresa de manufactura quiere monitorear la temperatura de sus mÃ¡quinas en tiempo real.  

ğŸ”¹ **SoluciÃ³n con Arquitectura Kappa:**  
1ï¸âƒ£ **Kafka o Kinesis** recibe datos de sensores en streaming.  
2ï¸âƒ£ **Apache Flink** procesa y detecta anomalÃ­as en tiempo real.  
3ï¸âƒ£ **Amazon DynamoDB** almacena la informaciÃ³n relevante.  
4ï¸âƒ£ **Amazon QuickSight** visualiza datos en tiempo real.

### **ğŸŒŸ Beneficios de la Arquitectura Kappa**  

âœ… **Menos complejidad** â†’ Un solo pipeline de datos.  
âœ… **Menor latencia** â†’ Respuesta en tiempo real.  
âœ… **Mayor escalabilidad** â†’ Perfecto para datos de alto volumen.  
âœ… **Ideal para Machine Learning** â†’ Modelos entrenados en tiempo real.

### **ğŸ” ConclusiÃ³n**  
La **Arquitectura Kappa** es ideal para sistemas que requieren **procesamiento en tiempo real**, como **IoT, finanzas, seguridad y ML**. Si buscas baja latencia y simplicidad, Kappa es una excelente opciÃ³n. ğŸš€

### Resumen

Fue presentada por Jay Krepsen en el 2014 como una evoluciÃ³n de la arquitectura lambda. Elimina la capa batch haciendo que todo se procese en tiempo real.

La arquitectura Kappa sigue los siguientes pilares:

1. Todo es un stream.
2. InformaciÃ³n de origen no modificada.
3. Solo un flujo de procesamiento.
4. Capaz de reprocesar.

## Arquitectura Batch

La **Arquitectura Batch** es un enfoque tradicional para el procesamiento de datos en grandes volÃºmenes. Se basa en la ejecuciÃ³n de tareas en **lotes** o **bloques** de datos en un periodo determinado, en lugar de procesar cada evento individualmente en tiempo real.

### **ğŸ“Œ Â¿QuÃ© es una Arquitectura Batch?**  
Es un modelo de procesamiento de datos donde los datos se recopilan, almacenan y luego se procesan en **bloques o lotes** en un intervalo especÃ­fico.  

ğŸ’¡ **Ejemplo:** Un banco genera un informe de todas las transacciones realizadas en el dÃ­a y lo procesa cada noche.

### **ğŸš€ Componentes de una Arquitectura Batch**  

### **1ï¸âƒ£ Capa de Ingesta de Datos**  
ğŸ“Œ **Recopila datos de diversas fuentes y los almacena para su posterior procesamiento.**  
ğŸ”¹ **Ejemplos:**  
- Amazon S3  
- Google Cloud Storage  
- Azure Data Lake  

### **2ï¸âƒ£ Capa de Procesamiento Batch**  
ğŸ“Œ **Ejecuta trabajos en lotes periÃ³dicamente.**  
ğŸ”¹ **Ejemplos:**  
- Apache Hadoop  
- AWS Glue  
- Google Dataflow  
- Azure Data Factory  

### **3ï¸âƒ£ Capa de Almacenamiento**  
ğŸ“Œ **Guarda los datos procesados para su consulta y anÃ¡lisis.**  
ğŸ”¹ **Ejemplos:**  
- Amazon Redshift  
- Google BigQuery  
- Azure Synapse Analytics  

### **4ï¸âƒ£ Capa de Consumo y VisualizaciÃ³n**  
ğŸ“Œ **Proporciona acceso a los datos procesados.**  
ğŸ”¹ **Ejemplos:**  
- Tableau  
- Power BI  
- Amazon QuickSight

### **ğŸ”„ Â¿CÃ³mo funciona una Arquitectura Batch?**  

1ï¸âƒ£ **Se recopilan los datos** en intervalos especÃ­ficos.  
2ï¸âƒ£ **Se almacenan en un sistema de archivos o base de datos.**  
3ï¸âƒ£ **Se procesan en lotes** mediante herramientas como Apache Spark o AWS Glue.  
4ï¸âƒ£ **Se almacenan los resultados** en un Data Warehouse.  
5ï¸âƒ£ **Se consumen los datos** mediante dashboards o informes.

### **ğŸ”¹ Casos de Uso de la Arquitectura Batch**  

âœ… **Procesamiento de grandes volÃºmenes de datos histÃ³ricos.**  
âœ… **GeneraciÃ³n de reportes y anÃ¡lisis de tendencias.**  
âœ… **Carga de datos en Data Warehouses.**  
âœ… **Procesamiento de facturaciÃ³n y pagos en empresas.** 

### **âš–ï¸ Diferencia entre Batch y Streaming**  

| CaracterÃ­stica  | **Batch**  | **Streaming**  |
|---------------|------------|---------------|
| Procesamiento | Lotes de datos | Evento por evento |
| Latencia     | Alta (horas/dÃ­as) | Baja (segundos) |
| Complejidad  | Baja | Media/Alta |
| Uso comÃºn    | Reportes, anÃ¡lisis histÃ³ricos | IoT, Finanzas en tiempo real |

ğŸ’¡ **Si necesitas procesamiento periÃ³dico de grandes volÃºmenes de datos, Batch es ideal. Si necesitas datos en tiempo real, usa Streaming.**

### **ğŸŒŸ Beneficios de la Arquitectura Batch**  

âœ… **Alta eficiencia en grandes volÃºmenes de datos.**  
âœ… **OptimizaciÃ³n de recursos, ya que no requiere procesamiento continuo.**  
âœ… **Facilidad de mantenimiento y escalabilidad.**  
âœ… **Menor costo en comparaciÃ³n con arquitecturas en tiempo real.** 

### **ğŸ” ConclusiÃ³n**  
La **Arquitectura Batch** es ideal cuando se necesita procesar grandes volÃºmenes de datos de manera eficiente sin la necesidad de respuestas en tiempo real. Se usa en **anÃ¡lisis de datos histÃ³ricos, reportes y procesos de negocio periÃ³dicos**. ğŸš€

**Resumen**

Esta arquitectura es una parte especÃ­fica de la arquitectura Lambda.

Es importante comprender que en esta arquitectura vamos a tener un Input, un proceso y un Output, estos tres orquestados por un sistema donde el Input y Output deben de conectarse a una base de datos.

## Llevar tu informaciÃ³n al cloud

Migrar tu informaciÃ³n a la nube (Cloud) implica trasladar datos, aplicaciones y cargas de trabajo desde infraestructuras locales a servicios en la nube proporcionados por empresas como AWS, Google Cloud o Azure.  

### **Ventajas de mover tu informaciÃ³n a la nube**  
âœ… **Escalabilidad**: Puedes aumentar o reducir recursos segÃºn la demanda.  
âœ… **Alta disponibilidad**: Accede a tus datos desde cualquier lugar con conexiÃ³n a internet.  
âœ… **Seguridad**: La mayorÃ­a de los proveedores ofrecen cifrado y medidas de seguridad avanzadas.  
âœ… **ReducciÃ³n de costos**: Evita gastos en hardware y mantenimiento.  

### **Tipos de migraciÃ³n a la nube**  
â˜ **Lift and Shift**: Mover los datos y aplicaciones sin modificaciones.  
â˜ **Replataforming**: Adaptar aplicaciones para aprovechar mejor los servicios en la nube.  
â˜ **Refactoring**: RediseÃ±ar aplicaciones para optimizar su desempeÃ±o en la nube.  

### **Servicios clave en la nube para almacenar informaciÃ³n**  
ğŸ“Œ **Amazon S3**: Almacenamiento escalable y seguro.  
ğŸ“Œ **Google Cloud Storage**: AlmacÃ©n de objetos para cargas de trabajo empresariales.  
ğŸ“Œ **Azure Blob Storage**: SoluciÃ³n de almacenamiento de Microsoft.  
ğŸ“Œ **Bases de datos en la nube**: DynamoDB, Aurora, BigQuery, Firestore, entre otras.  

Si quieres mÃ¡s detalles sobre cÃ³mo migrar a la nube, dime quÃ© tipo de datos o cargas de trabajo deseas mover. ğŸš€

### Resumen

Existen distintas formas de conectar tu sistema en la nube con la fuente de origen de datos que necesitas, algunas opciones son:

- Utilizar el SDK de la nube utilizando el lenguaje de programaciÃ³n de tu preferencia.
- Usar el CLI para conectarse a la nube.
- Utilizar servicios especializados para la tarea.

Es posible combinar esas opciones para llevar la informaciÃ³n que necesites a tu cloud provider.

## Demo - Creando nuestro IDE en la nube con Python - Boto3

Para crear un **IDE en la nube con Python** utilizando **Boto3** (el SDK de AWS para Python), podemos aprovechar servicios de AWS como **EC2, S3, Cloud9 y Lambda**. AquÃ­ te dejo una guÃ­a paso a paso:

### **1ï¸âƒ£ Configurar Boto3 en tu entorno local**  
Antes de comenzar, instala **Boto3** y configura las credenciales de AWS en tu mÃ¡quina local:  

```bash
pip install boto3
aws configure
```
Ingresa tu **AWS Access Key**, **Secret Key**, regiÃ³n y formato de salida.

### **2ï¸âƒ£ Crear una instancia EC2 para alojar el IDE**  
Podemos lanzar una instancia EC2 que sirva como nuestro entorno de desarrollo.  

```python
import boto3

ec2 = boto3.resource('ec2')

# Lanzar una instancia EC2 con Ubuntu
instance = ec2.create_instances(
    ImageId='ami-0c55b159cbfafe1f0',  # ID de la AMI (Ubuntu)
    MinCount=1,
    MaxCount=1,
    InstanceType='t2.micro',
    KeyName='mi-clave-aws',  # Debes haber creado una clave SSH en AWS
    SecurityGroups=['default'],
    UserData="""#!/bin/bash
    sudo apt update -y
    sudo apt install -y python3 python3-pip
    pip3 install jupyter boto3
    """,
    TagSpecifications=[
        {
            'ResourceType': 'instance',
            'Tags': [{'Key': 'Name', 'Value': 'IDE-Python-Cloud'}]
        }
    ]
)

print(f"Instancia EC2 creada: {instance[0].id}")
```
Esto inicia un **servidor Ubuntu con Python y Jupyter**.

### **3ï¸âƒ£ Configurar un Bucket S3 para almacenamiento de proyectos**
Podemos crear un bucket S3 para guardar archivos y proyectos.

```python
s3 = boto3.client('s3')

bucket_name = 'mi-ide-python-cloud'

s3.create_bucket(
    Bucket=bucket_name,
    CreateBucketConfiguration={'LocationConstraint': 'us-east-1'}
)

print(f"Bucket {bucket_name} creado exitosamente")
```
Subir archivos al IDE en la nube:

```python
s3.upload_file('mi_script.py', bucket_name, 'mi_script.py')
print("Archivo subido a S3")
```

### **4ï¸âƒ£ OpciÃ³n avanzada: Usar AWS Cloud9**
AWS **Cloud9** es un IDE en la nube administrado por AWS que se puede lanzar con **Boto3**.

```python
cloud9 = boto3.client('cloud9')

response = cloud9.create_environment_ec2(
    name='MiIDECloud9',
    instanceType='t2.micro',
    automaticStopTimeMinutes=30
)

print(f"IDE Cloud9 creado: {response['environmentId']}")
```
Cloud9 permite codificar directamente en un navegador sin necesidad de configurar servidores.

### **5ï¸âƒ£ Acceder al IDE**
- Si usaste EC2, puedes conectarte con SSH:
  ```bash
  ssh -i mi-clave-aws.pem ubuntu@IP_PUBLICA
  ```
- Para Jupyter Notebook, abre en tu navegador:
  ```
  http://IP_PUBLICA:8888
  ```
- Si usaste Cloud9, abre la consola de AWS y accede desde la secciÃ³n **Cloud9**.

### ğŸš€ **ConclusiÃ³n**
Hemos creado un **IDE en la nube con Python** utilizando EC2, S3 y Cloud9 con Boto3. Puedes escalarlo agregando **Docker, VSCode Server, Lambda o API Gateway** para mÃ¡s funcionalidades.

**Resumen**

Vamos a utilizar el servicio de Cloud de Amazon para este curso, especÃ­ficamente para esta demo usaremos el SDK de AWS para Python.

Python es una gran opciÃ³n para procesamiento de datos ya que cuenta con librerÃ­as como Pandas, Anaconda PyBrain, NumPy.

**Lecturas recomendadas**

[AWS Cloud9 â€“ Amazon Web Services](https://aws.amazon.com/es/cloud9/)

[Boto 3 Documentation â€” Boto 3 Docs 1.9.106 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)

[GitHub - boto/boto3: AWS SDK for Python](https://github.com/boto/boto3)

## Â¿CÃ³mo usar Boto3?

### **ğŸ“Œ Â¿QuÃ© es Boto3?**  
Boto3 es el SDK de Python para interactuar con **AWS**. Permite gestionar servicios como **S3, EC2, DynamoDB, Lambda, RDS, entre otros**.

### **1ï¸âƒ£ InstalaciÃ³n de Boto3**
Para instalarlo en tu entorno de Python, usa:
```bash
pip install boto3
```

### **2ï¸âƒ£ ConfiguraciÃ³n de credenciales**  
Antes de usar Boto3, debes configurar tus credenciales de AWS. Puedes hacerlo de dos maneras:

### **ğŸ”¹ OpciÃ³n 1: Configurar usando AWS CLI**  
Si tienes instalado AWS CLI, ejecuta:
```bash
aws configure
```
Te pedirÃ¡:
- **AWS Access Key ID**  
- **AWS Secret Access Key**  
- **RegiÃ³n por defecto** (ejemplo: `us-east-1`)  
- **Formato de salida** (`json`, `text`, etc.)

Las credenciales se guardarÃ¡n en `~/.aws/credentials` (Linux/macOS) o `C:\Users\TU_USUARIO\.aws\credentials` (Windows).

### **ğŸ”¹ OpciÃ³n 2: Configurar manualmente**
Crea el archivo `~/.aws/credentials` y aÃ±ade:
```ini
[default]
aws_access_key_id = TU_ACCESS_KEY
aws_secret_access_key = TU_SECRET_KEY
region = us-east-1
```

### **3ï¸âƒ£ Uso de Boto3**
### **ğŸ”¹ Crear una sesiÃ³n en Boto3**
```python
import boto3

# Crear una sesiÃ³n con AWS
session = boto3.Session(
    aws_access_key_id="TU_ACCESS_KEY",
    aws_secret_access_key="TU_SECRET_KEY",
    region_name="us-east-1"
)
```
Si ya configuraste las credenciales con `aws configure`, puedes omitir los parÃ¡metros.

### **4ï¸âƒ£ Ejemplos de uso de Boto3**
### **ğŸ”¹ Listar buckets en S3**
```python
s3 = boto3.client("s3")

# Obtener lista de buckets
response = s3.list_buckets()
for bucket in response["Buckets"]:
    print(bucket["Name"])
```

### **ğŸ”¹ Subir un archivo a S3**
```python
s3.upload_file("mi_archivo.txt", "nombre-del-bucket", "carpeta/archivo.txt")
```

### **ğŸ”¹ Crear una instancia EC2**
```python
ec2 = boto3.resource("ec2")

# Crear una nueva instancia EC2
instances = ec2.create_instances(
    ImageId="ami-12345678", 
    MinCount=1,
    MaxCount=1,
    InstanceType="t2.micro"
)
print("Instancia creada:", instances[0].id)
```

### **ğŸ”¹ Leer datos de DynamoDB**
```python
dynamodb = boto3.resource("dynamodb")
table = dynamodb.Table("MiTabla")

# Obtener un elemento por su clave primaria
response = table.get_item(Key={"id": "123"})
print(response.get("Item"))
```

### **5ï¸âƒ£ Manejo de errores en Boto3**
Siempre es recomendable manejar excepciones para evitar fallos en el cÃ³digo:
```python
import botocore

try:
    s3.list_buckets()
except botocore.exceptions.NoCredentialsError:
    print("Credenciales no encontradas")
except botocore.exceptions.PartialCredentialsError:
    print("Credenciales incompletas")
```

### **ğŸš€ ConclusiÃ³n**
Boto3 es una herramienta poderosa para interactuar con AWS desde Python. Si necesitas ayuda con algÃºn servicio especÃ­fico, dime y te ayudo con cÃ³digo mÃ¡s detallado. ğŸ˜Š

### Â¿CÃ³mo usar Boto3?

Boto3 es el SDK de Python para interactuar con **AWS**. Permite gestionar servicios como **S3, EC2, DynamoDB, Lambda, RDS, entre otros**.

### **1ï¸âƒ£ InstalaciÃ³n de Boto3**
Para instalarlo en tu entorno de Python, usa:
```bash
pip install boto3
```

### **2ï¸âƒ£ ConfiguraciÃ³n de credenciales**  
Antes de usar Boto3, debes configurar tus credenciales de AWS. Puedes hacerlo de dos maneras:

### **ğŸ”¹ OpciÃ³n 1: Configurar usando AWS CLI**  
Si tienes instalado AWS CLI, ejecuta:
```bash
aws configure
```
Te pedirÃ¡:
- **AWS Access Key ID**  
- **AWS Secret Access Key**  
- **RegiÃ³n por defecto** (ejemplo: `us-east-1`)  
- **Formato de salida** (`json`, `text`, etc.)

Las credenciales se guardarÃ¡n en `~/.aws/credentials` (Linux/macOS) o `C:\Users\TU_USUARIO\.aws\credentials` (Windows).

### **ğŸ”¹ OpciÃ³n 2: Configurar manualmente**
Crea el archivo `~/.aws/credentials` y aÃ±ade:
```ini
[default]
aws_access_key_id = TU_ACCESS_KEY
aws_secret_access_key = TU_SECRET_KEY
region = us-east-1
```

### **3ï¸âƒ£ Uso de Boto3**
### **ğŸ”¹ Crear una sesiÃ³n en Boto3**
```python
import boto3

# Crear una sesiÃ³n con AWS
session = boto3.Session(
    aws_access_key_id="TU_ACCESS_KEY",
    aws_secret_access_key="TU_SECRET_KEY",
    region_name="us-east-1"
)
```
Si ya configuraste las credenciales con `aws configure`, puedes omitir los parÃ¡metros.

### **4ï¸âƒ£ Ejemplos de uso de Boto3**
### **ğŸ”¹ Listar buckets en S3**
```python
s3 = boto3.client("s3")

# Obtener lista de buckets
response = s3.list_buckets()
for bucket in response["Buckets"]:
    print(bucket["Name"])
```

### **ğŸ”¹ Subir un archivo a S3**
```python
s3.upload_file("mi_archivo.txt", "nombre-del-bucket", "carpeta/archivo.txt")
```

### **ğŸ”¹ Crear una instancia EC2**
```python
ec2 = boto3.resource("ec2")

# Crear una nueva instancia EC2
instances = ec2.create_instances(
    ImageId="ami-12345678", 
    MinCount=1,
    MaxCount=1,
    InstanceType="t2.micro"
)
print("Instancia creada:", instances[0].id)
```

### **ğŸ”¹ Leer datos de DynamoDB**
```python
dynamodb = boto3.resource("dynamodb")
table = dynamodb.Table("MiTabla")

# Obtener un elemento por su clave primaria
response = table.get_item(Key={"id": "123"})
print(response.get("Item"))
```

### **5ï¸âƒ£ Manejo de errores en Boto3**
Siempre es recomendable manejar excepciones para evitar fallos en el cÃ³digo:
```python
import botocore

try:
    s3.list_buckets()
except botocore.exceptions.NoCredentialsError:
    print("Credenciales no encontradas")
except botocore.exceptions.PartialCredentialsError:
    print("Credenciales incompletas")
```

### **ğŸš€ ConclusiÃ³n**
Boto3 es una herramienta poderosa para interactuar con AWS desde Python. Si necesitas ayuda con algÃºn servicio especÃ­fico, dime y te ayudo con cÃ³digo mÃ¡s detallado. ğŸ˜Š

### Resumen

### Â¿CÃ³mo utilizar Boto3 para interactuar con servicios de AWS?

Boto3 es la biblioteca de Python por excelencia para interactuar con los servicios de Amazon Web Services (AWS). Su comprensiÃ³n y uso efectivo son esenciales para cualquier profesional que trabaje con AWS, ya que permite gestionar diversos servicios desde un solo lugar.

### Â¿QuÃ© es Boto3?

Boto3 es el SDK (Software Development Kit) de Python para AWS. Con Ã©l puedes crear, configurar y gestionar servicios de AWS como S3 y Athena, entre otros. Ofrece una interfaz fÃ¡cil de usar para programadores, que facilita la automatizaciÃ³n de tareas repetitivas dentro del entorno de AWS.

### Â¿CÃ³mo acceder a la documentaciÃ³n de Boto3?

Para aprovechar al mÃ¡ximo Boto3, es crucial familiarizarse con la documentaciÃ³n proporcionada por AWS. Puedes encontrar informaciÃ³n detallada sobre cada servicio, ejemplos de cÃ³digo y explicaciones claras de los parÃ¡metros necesarios. AsÃ­ es como se accede a la documentaciÃ³n:

1. Abre una pestaÃ±a de tu navegador.
2. Busca "AWS Boto3 S3" o el servicio especÃ­fico que te interese.
3. Accede al primer enlace que normalmente te llevarÃ¡ a la documentaciÃ³n oficial.

### Â¿QuÃ© debemos tener en cuenta al utilizar Boto3 con AWS?

Al trabajar con Boto3, es vital inicializar el servicio especÃ­fico antes de realizar cualquier operaciÃ³n. Este proceso implica crear un cliente desde Boto3 y especificar el servicio dentro de los parÃ©ntesis y entre comillas sencillas. Veamos un ejemplo sencillo para inicializar el cliente de Athena:

```python
import boto3

client = boto3.client('athena')

# AquÃ­ harÃ­amos una consulta, iniciarÃ­amos su ejecuciÃ³n y obtendrÃ­amos la respuesta
```

### Â¿CÃ³mo interactuar con servicios como S3 y Athena?

Aparte de S3, otro servicio potente que puedes gestionar con Boto3 es Athena, el cual te permite lanzar consultas SQL sobre datos almacenados en S3. AquÃ­ hay un ejemplo prÃ¡ctico que muestra cÃ³mo inicializar y realizar operaciones con estos servicios:

1. S3: Puedes definir la regiÃ³n de operaciÃ³n si el script lo requiere.
2. Athena: Requiere la inicializaciÃ³n del servicio y luego puedes enviar consultas SQL para analizar los datos almacenados.

En general, un script tÃ­pico podrÃ­a verse asÃ­:

```python
import boto3

# Inicializar cliente de S3
s3 = boto3.client('s3', region_name='us-west-2')

# Listar buckets en S3
response = s3.list_buckets()
print(response['Buckets'])

# Inicializar cliente de Athena
athena = boto3.client('athena')

# Enviar una consulta a Athena
query_start = athena.start_query_execution(
    QueryString='SELECT * FROM database.table',
    QueryExecutionContext={'Database': 'my_database'},
    ResultConfiguration={
        'OutputLocation': 's3://my-athena-results-bucket/path/to/',
    }
)

print(query_start)
```

### Â¿Por quÃ© es esencial Boto3 para tus proyectos?

Boto3 es fundamental para cualquier proyecto de Big Data en AWS gracias a su capacidad para interconectar servicios de AWS mediante Python, el cual es un lenguaje robusto y ampliamente utilizado en anÃ¡lisis de datos. Con esta librerÃ­a, se combina la potencia de los servicios en la nube con la facilidad y versatilidad de Python, permitiendo desarrollar soluciones efectivas y eficientes.

Â¡AnÃ­mate a seguir explorando y utilizando Boto3 para automatizar tus procesos en AWS! La prÃ¡ctica y el conocimiento profundo de esta herramienta abrirÃ¡n muchas puertas en tu carrera como desarrollador o analista de datos.

**Lecturas recomendadas**

[Ejemplo de Python para AWS Cloud9 - AWS Cloud9](https://docs.aws.amazon.com/es_es/cloud9/latest/user-guide/sample-python.html)

[S3 â€” Boto 3 Docs 1.9.106 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html)

[https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html)

## API Gateway

Amazon **API Gateway** es un servicio totalmente administrado de AWS que permite a los desarrolladores **crear, publicar, mantener, monitorear y asegurar APIs** en cualquier escala. Se utiliza para conectar clientes con **servicios backend** como **Lambda, EC2, DynamoDB o cualquier otro servicio HTTP/HTTPS**.

### **ğŸ”¹ CaracterÃ­sticas Principales**
1. **Soporte para diferentes tipos de APIs**  
   - RESTful APIs  
   - WebSocket APIs  
   - HTTP APIs (mÃ¡s ligeras y econÃ³micas que REST)

2. **Escalabilidad AutomÃ¡tica**  
   - Maneja millones de solicitudes sin intervenciÃ³n manual.  

3. **AutenticaciÃ³n y AutorizaciÃ³n**  
   - Compatible con **IAM, Cognito y Lambda Authorizers**.  

4. **GestiÃ³n de trÃ¡fico y seguridad**  
   - Soporta **rate limiting** y protecciÃ³n contra ataques DDoS.  

5. **Monitoreo y Logging**  
   - IntegraciÃ³n con **CloudWatch** para logs, mÃ©tricas y alertas.  

6. **TransformaciÃ³n de datos**  
   - Permite **mapear, modificar y validar** peticiones/respuestas.  

7. **IntegraciÃ³n con mÃºltiples backends**  
   - AWS Lambda, EC2, S3, DynamoDB, servicios HTTP, etc.

### **ğŸ”¹ Casos de Uso ğŸ“Œ**
âœ… CreaciÃ³n de microservicios  
âœ… API Gateway para aplicaciones mÃ³viles/web  
âœ… IntegraciÃ³n con AWS Lambda para una arquitectura **serverless**  
âœ… ExposiciÃ³n segura de endpoints para terceros  
âœ… Proxy para servicios internos de AWS  

### **ğŸ”¹ Ejemplo: Creando un API Gateway con Lambda**
1ï¸âƒ£ **Crear una funciÃ³n Lambda en AWS Lambda**  
2ï¸âƒ£ **Configurar API Gateway para que llame a la funciÃ³n Lambda**  
3ï¸âƒ£ **Implementar una polÃ­tica de seguridad (IAM o Cognito)**  
4ï¸âƒ£ **Probar la API con Postman o cURL**  

### **ğŸ”¹ Ejemplo en Python (Lambda)**
```python
import json

def lambda_handler(event, context):
    return {
        "statusCode": 200,
        "body": json.dumps({"message": "Â¡Hola desde AWS Lambda con API Gateway!"})
    }
```

Con API Gateway puedes **convertir cualquier servicio backend en una API escalable y segura**. ğŸš€

**Resumen**

Este servicio nos va a servir como puerta de enlace entre la data que tenemos y la plataforma en la nube.

- Soporta cientos de miles de llamadas concurrentes.
- Previene ataques DDOS.

## Storage Gateway

**AWS Storage Gateway** es un servicio hÃ­brido que permite conectar entornos **on-premise** con el almacenamiento en la **nube de AWS**. Se utiliza para extender la capacidad de almacenamiento local utilizando **Amazon S3, Glacier, EBS y otros servicios de AWS**.

### **ğŸ”¹ Tipos de Storage Gateway**
AWS ofrece **tres tipos principales** de Storage Gateway, segÃºn el caso de uso:  

1ï¸âƒ£ **File Gateway (NFS/SMB)**  
   - Permite almacenar archivos en **Amazon S3** y acceder a ellos como si fueran locales.  
   - Compatible con protocolos **NFS y SMB**.  
   - Ideal para **archivos de backup, machine learning y anÃ¡lisis de datos**.  

2ï¸âƒ£ **Volume Gateway (iSCSI)**  
   - Crea volÃºmenes que pueden ser montados en servidores **on-premise** como unidades de disco.  
   - Soporta **modo almacenado** (copia local + respaldo en AWS) o **modo en cachÃ©** (copia en AWS + cachÃ© local).  
   - Se integra con **EBS y S3** para backup y restauraciÃ³n.  

3ï¸âƒ£ **Tape Gateway (Virtual Tape Library - VTL)**  
   - Emula una **librerÃ­a de cintas** para backup en la nube.  
   - Usa **Amazon S3 y Glacier** para almacenamiento a largo plazo.  
   - Compatible con **software de backup tradicional** como Veeam, Commvault y NetBackup.

### **ğŸ”¹ Beneficios de Storage Gateway**
âœ… **ExtensiÃ³n del almacenamiento local sin comprar hardware adicional**  
âœ… **Backups automÃ¡ticos y recuperaciÃ³n ante desastres**  
âœ… **Acceso rÃ¡pido y seguro a datos en la nube**  
âœ… **ReducciÃ³n de costos operativos** al eliminar infraestructura fÃ­sica  
âœ… **Soporte para aplicaciones empresariales** como SAP, bases de datos y archivos compartidos

### **ğŸ”¹ Casos de Uso ğŸ“Œ**
ğŸ”¹ Empresas que desean **migrar gradualmente** a la nube  
ğŸ”¹ **Respaldo y recuperaciÃ³n de datos** desde servidores locales a AWS  
ğŸ”¹ **Archivos compartidos** accesibles desde mÃºltiples ubicaciones  
ğŸ”¹ **RetenciÃ³n de datos a largo plazo** con Glacier  

### **ğŸ”¹ Ejemplo de ImplementaciÃ³n de Storage Gateway**
1ï¸âƒ£ **Configurar una mÃ¡quina virtual** con el software de Storage Gateway en un entorno on-premise  
2ï¸âƒ£ **Conectar la mÃ¡quina a AWS** usando la consola de AWS  
3ï¸âƒ£ **Seleccionar el tipo de gateway** (File, Volume o Tape)  
4ï¸âƒ£ **Sincronizar con AWS S3, EBS o Glacier** segÃºn la configuraciÃ³n  
5ï¸âƒ£ **Acceder a los datos como si estuvieran en almacenamiento local**  

AWS Storage Gateway es una soluciÃ³n **hÃ­brida, flexible y econÃ³mica** para integrar almacenamiento local con la nube de AWS. ğŸš€

**Resumen**

Tiene tres caracterÃ­sticas importantes:

1. Nos permite enviar informaciÃ³n desde nuestro datacenter on-premise a la nube.
2. Puedes enviar los logs de una aplicaciÃ³n on-premise para que sean procesados.
3. Funciona en una mÃ¡quina virtual que instalamos en nuestro datacenter.

## Kinesis Data Streams 

**AWS Kinesis Data Streams** es un servicio de AWS que permite la **ingestiÃ³n, procesamiento y anÃ¡lisis de datos en tiempo real**. Es ideal para manejar flujos de datos generados continuamente, como logs, eventos de IoT, mÃ©tricas de aplicaciones y transacciones financieras.  

### **ğŸ”¹ CaracterÃ­sticas Principales**
âœ… **Procesamiento en tiempo real** ğŸ“Š  
âœ… **Alta escalabilidad** âš¡ (permite manejar desde KB/s hasta TB/hora)  
âœ… **Latencia baja (milisegundos)** â³  
âœ… **Persistencia de datos hasta 7 dÃ­as** ğŸ•’  
âœ… **IntegraciÃ³n con AWS Lambda, S3, DynamoDB, Redshift y mÃ¡s** ğŸ”„

### **ğŸ”¹ Componentes Claves de Kinesis Data Streams**
1ï¸âƒ£ **Shards (Fragmentos)**  
   - Cada stream se divide en shards, que determinan la capacidad del flujo.  
   - Cada shard puede manejar hasta **1 MB/s de escritura y 2 MB/s de lectura**.  
   - Puedes **escalar horizontalmente** agregando mÃ¡s shards.  

2ï¸âƒ£ **Producers (Productores)**  
   - Fuentes que envÃ­an datos a Kinesis.  
   - Ejemplos: logs de servidores, eventos de IoT, transacciones de e-commerce.  

3ï¸âƒ£ **Consumers (Consumidores)**  
   - Aplicaciones que leen y procesan los datos en tiempo real.  
   - Se pueden conectar mÃºltiples consumidores como **AWS Lambda, Kinesis Data Firehose, Apache Flink, o EC2**.

### **ğŸ”¹ Casos de Uso ğŸ“Œ**
ğŸ”¹ **Monitoreo en tiempo real** de logs de servidores y aplicaciones  
ğŸ”¹ **AnÃ¡lisis de tendencias de redes sociales** en tiempo real  
ğŸ”¹ **DetecciÃ³n de fraudes en transacciones bancarias** ğŸ’³  
ğŸ”¹ **Procesamiento de datos de IoT** en fÃ¡bricas, dispositivos inteligentes, etc.  
ğŸ”¹ **Streaming de eventos de videojuegos** ğŸ®

### **ğŸ”¹ Ejemplo de Uso: EnvÃ­o de Datos a Kinesis**
Usando **Boto3 (SDK de Python)** para enviar datos a un stream:

```python
import boto3
import json
import time

# Crear cliente de Kinesis
kinesis_client = boto3.client('kinesis', region_name='us-east-1')

stream_name = 'mi-stream'

# Enviar datos a Kinesis
for i in range(10):
    data = {
        'event_id': i,
        'timestamp': time.time(),
        'message': 'Evento de prueba'
    }
    kinesis_client.put_record(
        StreamName=stream_name,
        Data=json.dumps(data),
        PartitionKey=str(i)  # Determina a quÃ© shard irÃ¡ el dato
    )
    print(f"Enviado evento {i}")
    time.sleep(1)  # SimulaciÃ³n de flujo de datos
```

### **ğŸ”¹ Integraciones Populares**
âœ… **AWS Lambda** para disparar funciones en tiempo real  
âœ… **Kinesis Data Firehose** para almacenar datos en S3, Redshift o OpenSearch  
âœ… **Kinesis Data Analytics** para anÃ¡lisis en tiempo real con SQL  
âœ… **DynamoDB, S3, ElasticSearch** para almacenamiento y anÃ¡lisis posterior  

**ğŸ”¥ AWS Kinesis Data Streams es una soluciÃ³n potente para manejar y procesar grandes volÃºmenes de datos en tiempo real, con alta escalabilidad y baja latencia.** ğŸš€

**Resumen**

- Tienes que pensar en procesar grandes cantidades de datos, desde TB hasta EB.
- Algunos casos de uso son para procesar informaciÃ³n de logs, social media, market data feeds y web clickstream.
- La unidad fundamental dentro de Kinesis se llama Data Record.
- La informaciÃ³n dentro de Kinesis por defecto solamente cuenta con un periodo de retenciÃ³n de 24 horas.
- El Shard es una secuencia de Data Records dentro de un stream.

**Lecturas recomendadas**

[Creating and Updating Data Streams - Amazon Kinesis Data Streams](https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-streams.html)

## Demo - Despegando Kinesis con Cloudformation

AquÃ­ tienes un ejemplo de cÃ³mo desplegar **Amazon Kinesis Data Streams** usando **AWS CloudFormation**.

### **CloudFormation Template para Kinesis Data Streams**
Este cÃ³digo crea un flujo de datos de Kinesis con una capacidad de **2 shards**, lo que permite manejar mÃ¡s datos en paralelo.

```yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: 'CloudFormation template para desplegar un Kinesis Data Stream'

Resources:
  MyKinesisStream:
    Type: AWS::Kinesis::Stream
    Properties:
      Name: MiKinesisStream
      ShardCount: 2  # NÃºmero de shards (ajÃºstalo segÃºn la carga esperada)
      RetentionPeriodHours: 24  # Tiempo de retenciÃ³n de datos en horas
      StreamModeDetails:
        StreamMode: PROVISIONED  # TambiÃ©n puedes usar ON_DEMAND para autoescalado

Outputs:
  StreamName:
    Description: "Nombre del flujo Kinesis creado"
    Value: !Ref MyKinesisStream
```

### **Despliegue en AWS**
1. Guarda el archivo como `kinesis-template.yaml`.
2. Usa AWS CLI para desplegarlo:
   ```sh
   aws cloudformation create-stack --stack-name MiKinesisStack --template-body file://kinesis-template.yaml
   ```
3. Verifica en la consola de AWS en **Kinesis > Data Streams**.

### **Extras: Â¿CÃ³mo leer datos del Stream?**
Si quieres consumir los datos en Python con **Boto3**, puedes hacerlo asÃ­:

```python
import boto3

kinesis_client = boto3.client('kinesis', region_name='us-east-1')

stream_name = "MiKinesisStream"

response = kinesis_client.describe_stream(StreamName=stream_name)
print(response)
```

ğŸ’¡ **Notas:**
- Si quieres **autoescalado**, usa `StreamMode: ON_DEMAND`.
- Puedes conectar este stream con **AWS Lambda** o **Amazon Kinesis Data Firehose** para procesar datos en tiempo real.

### Resumen

### Â¿CÃ³mo desplegar AWS Kinesis con CloudFormation?

Para quienes buscan automatizar y gestionar recursos en AWS de manera eficiente, utilizar CloudFormation es una estrategia ganadora. CloudFormation ofrece la posibilidad de manejar infraestructuras a travÃ©s de cÃ³digo, facilitando el despliegue de servicios como AWS Kinesis de forma automatizada. AquÃ­ exploraremos cÃ³mo se lleva a cabo este proceso paso a paso.

### Â¿QuÃ© es CloudFormation y cÃ³mo lo utilizamos para Kinesis?

CloudFormation es un servicio de AWS que permite crear y gestionar un conjunto de recursos en la nube mediante plantillas en formato YAML o JSON. Es un enfoque que trata la infraestructura como cÃ³digo, permitiendo configuraciones precisas y repetitivas en la nube. Utilizando CloudFormation, podemos definir y desplegar una arquitectura completa, como Kinesis, a partir de un solo script.

Para desplegar un Kinesis Stream, comenzamos con una plantilla maestra escrita en YAML, donde definimos los parÃ¡metros y recursos necesarios:

- **Environment Name**: Especifica el entorno donde se desplegarÃ¡ Kinesis.
- **Kinesis Shards Number**: Define la cantidad de shards que queremos crear; por defecto es uno, pero se puede ajustar segÃºn necesidades individuales.

### Â¿CÃ³mo estructuramos las plantillas de CloudFormation?

Las plantillas se dividen en dos partes principales: la plantilla maestra y la plantilla de distribuciÃ³n de Kinesis.

1. Plantilla maestra: Define los parÃ¡metros bÃ¡sicos como el environment y los shards. Esta plantilla enlaza con una URL que contiene otra plantilla YAML mÃ¡s especÃ­fica, dedicada al despliegue de Kinesis.

3. Plantilla de Kinesis Distribution: Recibe parÃ¡metros de la plantilla maestra y los utiliza para configurar y crear el Kinesis Stream con las propiedades siguientes:

 - **Nombre del entorno**: AÃ±adido al final del recurso para facilitar su identificaciÃ³n.
 - **Tiempo de retenciÃ³n**: Por defecto, se establece en 24 horas.
 - **Shards**: Utiliza el nÃºmero especificado en la plantilla maestra.

### Â¿CÃ³mo almacenamos y desplegamos las plantillas?

Para manejar efectivamente las plantillas, las almacenamos en un bucket de S3, desde donde CloudFormation las accede durante el proceso de despliegue. El procedimiento es el siguiente:

1. **Crear un bucket en S3**: Este bucket almacenarÃ¡ las plantillas. Por ejemplo, un nombre para el bucket podrÃ­a ser CFN-Kinesis-Lab.

2. **Cargar las plantillas en el bucket**: Subimos la plantilla maestra y la de Kinesis Distribution al bucket reciÃ©n creado.

### Â¿CÃ³mo se ejecuta el despliegue desde CloudFormation?

Una vez las plantillas estÃ¡n en S3, procedemos al despliegue mediante CloudFormation:

1. Buscamos el servicio de CloudFormation en la consola de AWS.
2. Creamos un nuevo stack, especificando la ruta de la plantilla maestra almacenada en S3.
3. Configuramos el stack: Asignamos un nombre (por ejemplo, `Kinesis Lab Platzi`), seleccionamos el environment (como staging) y el nÃºmero de shards.
4. Tras completar el proceso, CloudFormation empieza a crear los recursos, y podemos monitorizar el progreso hasta que el estatus indica "create complete".

### Â¿QuÃ© ventajas ofrece este mÃ©todo de despliegue?

El uso de CloudFormation para desplegar servicios como Kinesis tiene mÃºltiples beneficios:

- **Control de versiones**: Las plantillas pueden estar versionadas y controladas en sistemas como GitHub.
- **ReutilizaciÃ³n**: Las plantillas pueden reutilizarse mediante nested stacks, permitiendo la creaciÃ³n modular de infraestructuras.
- **Portabilidad**: Las arquitecturas definidas pueden desplegarse fÃ¡cilmente en diferentes regiones o cuentas de AWS simplemente replicando las plantillas.

Este enfoque permite gestionar arquitecturas complejas y optimiza la manera en que se realizan los despliegues, alineando la infraestructura con las prÃ¡cticas de DevOps al tratarla como cÃ³digo. AsÃ­, te invitamos a seguir explorando y desarrollando tus habilidades con AWS para maximizar el potencial de tus proyectos.

## Kinesis Firehose

Amazon **Kinesis Data Firehose** es un servicio que permite cargar datos en tiempo real a destinos como **S3, Redshift, Elasticsearch, Splunk, y OpenSearch** sin necesidad de administrar servidores.

## **ğŸ“Œ Pasos para Crear un Firehose con CloudFormation**
1ï¸âƒ£ Crear una **fuente de datos** (Kinesis Data Stream o Direct PUT).  
2ï¸âƒ£ Configurar **Kinesis Firehose** para procesar y enviar los datos.  
3ï¸âƒ£ Definir un **destino** (Ejemplo: Amazon S3).  
4ï¸âƒ£ Implementar la plantilla con **AWS CLI o la consola de AWS**.  

### **ğŸŒŸ Ejemplo de Plantilla CloudFormation para Kinesis Firehose + S3**
Este ejemplo crea:  
âœ… Un **bucket S3** para almacenar los datos.  
âœ… Un **Kinesis Firehose** que envÃ­a datos al S3.  
âœ… Un **rol IAM** con permisos para escribir en S3.  

```yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: "Kinesis Firehose enviando datos a S3"

Resources:
  # ğŸ“Œ Bucket S3 donde Firehose almacenarÃ¡ los datos
  MiBucketS3:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: mi-bucket-firehose-ejemplo

  # ğŸ“Œ Rol IAM con permisos para Kinesis Firehose
  FirehoseIAMRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: FirehoseS3Policy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource: !Sub "arn:aws:s3:::${MiBucketS3}/*"

  # ğŸ“Œ Kinesis Firehose entregando datos a S3
  MiFirehose:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties:
      DeliveryStreamType: DirectPut  # Puedes usar tambiÃ©n KinesisStreamAsSource
      S3DestinationConfiguration:
        BucketARN: !Sub "arn:aws:s3:::${MiBucketS3}"
        RoleARN: !GetAtt FirehoseIAMRole.Arn
        Prefix: "datos/"  # Carpeta donde se almacenarÃ¡n los archivos

Outputs:
  FirehoseStreamName:
    Description: "Nombre del Kinesis Firehose creado"
    Value: !Ref MiFirehose
  S3BucketName:
    Description: "Nombre del bucket S3 de destino"
    Value: !Ref MiBucketS3
```

### **ğŸ“Œ Desplegar la Plantilla en AWS**
1ï¸âƒ£ Guarda el archivo como `firehose.yaml`.  
2ï¸âƒ£ Usa el siguiente comando en la **CLI de AWS**:  
   ```sh
   aws cloudformation create-stack --stack-name MiKinesisFirehose --template-body file://firehose.yaml
   ```
3ï¸âƒ£ Verifica en la consola de **CloudFormation** que la pila se creÃ³ correctamente.  
4ï¸âƒ£ EnvÃ­a datos al Firehose con el siguiente comando:  
   ```sh
   aws firehose put-record --delivery-stream-name MiFirehose --record='{"Data":"Hola Mundo\n"}'
   ```
5ï¸âƒ£ Confirma que los datos aparezcan en **S3** en la carpeta `datos/`.

### **ğŸš€ Beneficios de Kinesis Firehose**
âœ… **Serverless** â†’ No necesitas administrar servidores.  
âœ… **Escalable** â†’ Se ajusta automÃ¡ticamente al trÃ¡fico.  
âœ… **IntegraciÃ³n con AWS** â†’ Compatible con **S3, Redshift, OpenSearch, Splunk**.  
âœ… **TransformaciÃ³n de datos** â†’ Puede utilizar **AWS Lambda** para procesar antes de almacenar.

**Resumen**

1. Completamente administrado para la entrega de datos en tiempo real.
2. Permite usar una lambda para transformar la data.
3. Puede alimentar a diferentes sistemas de almacenamiento.

## Demo - ConfiguraciÃ³n de Kinesis Firehose

Amazon **Kinesis Data Firehose** es un servicio completamente administrado que permite **cargar datos en tiempo real** a destinos como **S3, Redshift, OpenSearch, y Splunk**.

### **ğŸ“Œ Pasos para Configurar Kinesis Firehose**  
1ï¸âƒ£ **Elegir la fuente de datos**:  
   - **Direct PUT** (envÃ­o directo sin Kinesis Streams).  
   - **Kinesis Data Streams** (para procesar antes de Firehose).  

2ï¸âƒ£ **Seleccionar un destino**:  
   - **Amazon S3** (almacenamiento en bruto).  
   - **Amazon Redshift** (para anÃ¡lisis).  
   - **Amazon OpenSearch** (bÃºsqueda en tiempo real).  
   - **Splunk** (para monitoreo de logs).  

3ï¸âƒ£ **Configurar transformaciÃ³n de datos** *(opcional)*:  
   - AWS Lambda para **formatear, filtrar o enriquecer** los datos antes de enviarlos.  

4ï¸âƒ£ **Configurar compresiÃ³n y cifrado** *(opcional)*:  
   - **CompresiÃ³n**: GZIP, ZIP, Snappy.  
   - **Cifrado**: Amazon S3 Server-Side Encryption o KMS.  

5ï¸âƒ£ **Definir permisos con IAM**:  
   - Firehose necesita permisos para escribir en el destino.  
   - Se debe crear un **rol IAM** adecuado.

### **ğŸ“Œ Ejemplo de ConfiguraciÃ³n: Kinesis Firehose â†’ S3**
Puedes configurar Kinesis Firehose usando **CloudFormation** o desde la **consola de AWS**.  

ğŸ”¹ **Ejemplo con AWS CLI** para crear un Firehose que envÃ­a datos a S3:  
```sh
aws firehose create-delivery-stream \
    --delivery-stream-name MiFirehoseStream \
    --delivery-stream-type DirectPut \
    --s3-destination-configuration file://s3_config.json
```

ğŸ”¹ **Archivo `s3_config.json`**:  
```json
{
    "BucketARN": "arn:aws:s3:::mi-bucket-firehose",
    "RoleARN": "arn:aws:iam::123456789012:role/KinesisFirehoseRole",
    "Prefix": "logs/",
    "BufferingHints": {
        "SizeInMBs": 5,
        "IntervalInSeconds": 300
    },
    "CompressionFormat": "GZIP",
    "EncryptionConfiguration": {
        "NoEncryptionConfig": "NoEncryption"
    }
}
```

### **ğŸ“Œ Monitoreo y Escalabilidad**
ğŸ”¹ Usa **Amazon CloudWatch** para monitorear mÃ©tricas como:  
   âœ… Cantidad de datos procesados.  
   âœ… Errores de entrega.  
   âœ… Retrasos en la ingesta de datos.  

ğŸ”¹ **Ajuste automÃ¡tico**:  
   - Firehose escala automÃ¡ticamente sin necesidad de administrar instancias.  
   - Se recomienda **optimizar buffers** para reducir costos de almacenamiento.

### **ğŸš€ ConclusiÃ³n**  
âœ… **Firehose es ideal para ingesta de datos en tiempo real sin administraciÃ³n de infraestructura.**  
âœ… **Se integra fÃ¡cilmente con S3, Redshift, OpenSearch y Splunk.**  
âœ… **Permite transformaciÃ³n de datos con Lambda y ofrece compresiÃ³n y cifrado opcionales.**

## Reto - Configurando Kinesis Firehose

Â¡Es hora de poner en practica lo aprendido! Para ello lo primero que debemos hacer es ir a nuestra consola de Amazon Web Services (AWS) y buscar el servicio de Kinesis.

![Buscando Kinesis en la consola de AWS](images/BuscandoKinesisenlaconsoladeAWS.png)

Si es la primera vez que visitas la herramienta de Kinesis debes dar click en el botÃ³n de Get Started para empezar.

![Get Started de Kinesis](images/GetStarteddeKinesis.png)

Dentro del dashboard de Kinesis debemos seleccionar la opciÃ³n *Create delivery stream*.

![Dashboard Kinesis](images/DashboardKinesis.png)

Lo primero que debemos hacer es ponerle un nombre a nuestro delivery stream, en este caso lo llamaremos *firehoseplatzi*.

![Creando un delivery stream llamado firehoseplatzi](images/Creandoundeliverystreamllamadofirehoseplatzi.png) 

Dentro de esa misma pantalla encontraremos dos opciones para alimentar nuestro delivery stream:

- Directamente por PUT u otros recursos.
- A travÃ©s de un stream de kinesis.

La opciÃ³n Direct PUT or other sources permite alimentar nuestro delivery stream mediante:

- La API de Firehose.
- Un Amazon Kinesis Agent.
- AWS IoT.
- CloudWatch Logs.
- CloudWatch Events.

Para fines de esta demo vamos a seleccionar **Direct PUT or other sources**. Damos click en *Next*

![Opciones para alimentar nuestro delivery stream](images/Opcionesparaalimentarnuestrodeliverystream.png)

Nuestro siguiente paso es elegir la forma en que nuestro delivery stream va a procesar la informaciÃ³n. Podemos modificar la informaciÃ³n de dos formas:

1. Mediante una funciÃ³n lambda, en caso de no tener ninguna podemos crearla al momento.
2. Convertir la informaciÃ³n a un formato mÃ¡s eficiente como Apache ORC o Apache parquet.

Para fines de esta demo no vamos a modificar nuestra informaciÃ³n, simplemente nos aseguramos que se encuentren ambas opciones deshabilitadas y damos click en Next.

![Configurar el procesamiento de los datos](images/Configurarelprocesamientodelosdatos.png)

Ya configuramos el origen de los datos y su procesamiento, es momento de elegir a dÃ³nde se va a mandar la informaciÃ³n:

- Amazon S3
- Amazon Redshift
- Amazon Elasticsearch Service
- Splunk

![Configurar destino de los datos](images/Configurardestinodelosdatos.png)

Dependiendo de quÃ© opciÃ³n elijamos vamos a tener que configurar su acceso. Para esta demo vamos a mandar nuestra informaciÃ³n a S3, puedes darle al botÃ³n Create new para generar un nuevo Bucket de S3.

Puedes configurar el prefijo donde Kinesis va a almacenar la informaciÃ³n, asÃ­ como el prefijo donde almacene los errores.

Una vez hayamos elegido nuestro bucket damos click en *Next*.

![ConfiguraciÃ³n de S3 para nuestra informaciÃ³n](images/ConfiguraciondeS3paranuestrainformacion.png)

Nuestro Ãºltimo paso para terminar la configuraciÃ³n de nuestro delivery stream nos va a pedir configurar:

- Condiciones del Buffer para S3
- La compresiÃ³n y encriptaciÃ³n de la informaciÃ³n
- Activar o desactivar el error logging
- Asignar un rol IAM por seguridad

Lo Ãºnico que vamos a modificar serÃ¡ el IAM role, esto con el fin de que Kinesis pueda interactuar con todos los demÃ¡s servicios de AWS. Vamos a dar click al botÃ³n *Create new or choose*.

![Crear nuevo IAM Role](images/CrearnuevoIAMRole.png)

Nos va a saltar una nueva ventana que nos pedirÃ¡ permiso para crear el nuevo IAM role, simplemente le damos al botÃ³n *Permitir*.

![Permitir nuevo rol](images/Permitirnuevorol.png)

Nos debe quedar un IAM role de la siguiente manera, ahora damos click a *Next*.

![IAM role asignado](images/IAMroleasignado.png)

Nos aparecerÃ¡ una review de la configuraciÃ³n del delivery stream, solamente damos click en el botÃ³n Create delivery stream.

Esperamos a que termine de crearse nuestro delivery stream y vamos a proceder a probar que nuestro delivery stream funcione correctamente. Damos click en el nombre de nuestro delivery.

![Delivery Stream Sucess](images/DeliveryStreamSucess.png)

Nos encontraremos en una pÃ¡gina con toda la informaciÃ³n acerca de nuestro stream. Abajo del nombre veremos un texto que dice **Test with demo data**, le vamos a dar click y nos va a desplegar informaciÃ³n sobre la prueba que va a realizar.

La prueba consta de mandar un simple archivo a nuestro S3. Debemos dar click en el botÃ³n *Start sending data* para iniciar la transmisiÃ³n de informaciÃ³n.

![Iniciar prueba con informaciÃ³n demo](images/Iniciarpruebaconinformaciondemo.png)

Este proceso puede llegar a tardar unos minutos, aproximadamente 3-5 minutos despuÃ©s vamos a dar click al enlace de nuestro bucket S3.

![Enlace al bucket S3](images/EnlacealbucketS3.png)

Si aÃºn no encuentras nada de informaciÃ³n dentro de tu bucket recuerda ser paciente, la velocidad de transmisiÃ³n depende de la configuraciÃ³n que hicimos al buffer, en este caso dejamos la configuraciÃ³n por defecto de 5MB.

Tras unos minutos debe aparecerte una carpeta dentro de otra y otra, separando la informaciÃ³n transmitida por su fecha. Dentro encontrarÃ¡s el archivo que mandaste, solamente queda regresar a la prueba de nuestro stream y apretar el botÃ³n *Stop sending demo data*.

[InformaciÃ³n dentro del Bucket](images/InformaciondentrodelBucket.png)

Si haz llegado hasta este parte con exito, agrega en los comentarios: â€œReto superadoâ€ y continua aprendiendo mÃ¡s.

## AWS - MSK

**AWS MSK (Amazon Managed Streaming for Apache Kafka)** es un servicio totalmente administrado que permite **ejecutar y operar Apache Kafka** en AWS sin preocuparse por la administraciÃ³n de infraestructura.

### **ğŸ“Œ Â¿Por quÃ© usar AWS MSK?**  
âœ… **AdministraciÃ³n automÃ¡tica**: AWS se encarga de la configuraciÃ³n, aprovisionamiento y mantenimiento.  
âœ… **Alta disponibilidad**: ReplicaciÃ³n en mÃºltiples zonas de disponibilidad (AZs).  
âœ… **IntegraciÃ³n con servicios AWS**: Kinesis, Lambda, IAM, CloudWatch, S3, Redshift, etc.  
âœ… **Escalabilidad automÃ¡tica**: Ajusta automÃ¡ticamente la capacidad del clÃºster segÃºn la carga de trabajo.

### **ğŸ“Œ Componentes de AWS MSK**  
1ï¸âƒ£ **Cluster de Kafka**: Conjunto de brokers administrados.  
2ï¸âƒ£ **Brokers**: Instancias que manejan la comunicaciÃ³n y el almacenamiento de mensajes.  
3ï¸âƒ£ **Zookeeper**: Administra metadatos y la coordinaciÃ³n de Kafka.  
4ï¸âƒ£ **Temas (Topics)**: Canales donde se publican y consumen los mensajes.  
5ï¸âƒ£ **Productores y consumidores**: Aplicaciones que envÃ­an y reciben datos desde Kafka.

### **ğŸ“Œ Pasos para Configurar AWS MSK**  

### **1ï¸âƒ£ Crear un clÃºster de Kafka en AWS MSK**  
Desde la consola de AWS:  
1. Ir a **Amazon MSK**.  
2. Hacer clic en **Crear clÃºster**.  
3. Elegir **MSK estÃ¡ndar** o **MSK Serverless**.  
4. Configurar el **nÃºmero de brokers**, tipo de instancia y almacenamiento.  
5. Definir **redes y permisos de seguridad (VPC, subnets, SG, IAM, etc.)**.  
6. **Crear y lanzar el clÃºster**.  

ğŸ”¹ **Ejemplo con AWS CLI**:  
```sh
aws kafka create-cluster --cluster-name MiKafkaCluster \
    --kafka-version 2.8.1 \
    --number-of-broker-nodes 3 \
    --broker-node-group-info file://broker-config.json
```

### **2ï¸âƒ£ Crear un topic en MSK**  
Puedes hacerlo usando `kafka-topics.sh` desde una mÃ¡quina con acceso al clÃºster:  

```sh
kafka-topics.sh --create --topic MiTopic --bootstrap-server <MSK_BROKER_URL> --partitions 3 --replication-factor 2
```

### **3ï¸âƒ£ Conectar productores y consumidores**  
#### **ğŸ”¹ Ejemplo de productor en Python (usando kafka-python)**
```python
from kafka import KafkaProducer

producer = KafkaProducer(bootstrap_servers=['MSK_BROKER_URL'])
producer.send('MiTopic', b'Hola desde AWS MSK!')
producer.flush()
```

#### **ğŸ”¹ Ejemplo de consumidor en Python**
```python
from kafka import KafkaConsumer

consumer = KafkaConsumer('MiTopic', bootstrap_servers=['MSK_BROKER_URL'])
for message in consumer:
    print(f'Recibido: {message.value}')
```

### **ğŸ“Œ Monitoreo y Seguridad en AWS MSK**  
ğŸ”¹ **CloudWatch**: Monitorea mÃ©tricas de uso, latencia y errores.  
ğŸ”¹ **IAM & VPC**: Controla accesos con polÃ­ticas y redes privadas.  
ğŸ”¹ **Cifrado**: Soporta TLS para datos en trÃ¡nsito y en reposo.

### **ğŸ“Œ Â¿MSK Serverless o MSK Standard?**  
ğŸ”¹ **MSK Serverless** â†’ Sin administraciÃ³n de brokers, pago por uso.  
ğŸ”¹ **MSK Standard** â†’ Personalizable, ideal para grandes cargas.

### **ğŸš€ ConclusiÃ³n**  
âœ… **AWS MSK simplifica la ejecuciÃ³n de Kafka sin gestionar servidores.**  
âœ… **Alta disponibilidad, escalabilidad automÃ¡tica y seguridad avanzada.**  
âœ… **Ideal para streaming de datos en tiempo real, IoT, logs y anÃ¡lisis.**

**Resumen**

Es un nuevo servicio lanzado en el 2018 cuyas caracterÃ­sticas son:

- Es un servicio que te permite tener Apache Kafka administrado en la nube.
- Se despliega en un clÃºster.
- Viene con la versiÃ³n de Apache Kafka 1.1.1
- Debemos especificar la cantidad de nodos que va a crear.
- Por defecto va a desplegar un nodo de Zookeeper para mantener la configuraciÃ³n.

**Lecturas recomendadas**

[Listing All Clusters in an Account - Amazon Managed Streaming for Kafka](https://docs.aws.amazon.com/msk/latest/developerguide/cli-list-clusters.html)

[Apache Kafka](https://kafka.apache.org/)

[Introduction to Amazon Managed Streaming for Kafka (MSK) | AWS Online Tech Talks](https://pages.awscloud.com/Introduction-to-Amazon-Managed-Streaming-for-Kafka-MSK_1208-ABD_OD.html)

[Get the ZooKeeper Connection String for an Amazon MSK Cluster - Amazon Managed Streaming for Apache Kafka](https://docs.aws.amazon.com/es_es/msk/latest/developerguide/msk-get-connection-string.html)

## Demo - Despliegue de un clÃºster con MSK

AquÃ­ tienes una guÃ­a paso a paso para desplegar un clÃºster de **AWS MSK (Managed Streaming for Apache Kafka)**.

### **ğŸ“Œ Pasos para Desplegar un ClÃºster MSK en AWS ğŸš€**  

### **1ï¸âƒ£ Configurar una VPC para MSK**  
MSK requiere que los brokers estÃ©n en una **VPC privada** con **subnets en mÃºltiples zonas de disponibilidad (AZs)**.  

1. Ve a la **Consola de AWS** > **VPC** > **Crear VPC**.  
2. Crea **3 subnets privadas** en diferentes AZs.  
3. Crea un **Security Group (SG)** con acceso en los puertos:  
   - **9092** (para comunicaciÃ³n sin TLS).  
   - **9094** (para comunicaciÃ³n con TLS).  
4. Configura un **IAM Role** con permisos de acceso a MSK.

### **2ï¸âƒ£ Crear el ClÃºster MSK**  

#### **ğŸ”¹ Desde la Consola de AWS**  
1. Ve a **Amazon MSK** > **Crear clÃºster**.  
2. Elige entre **MSK estÃ¡ndar** o **MSK Serverless**.  
3. Selecciona la versiÃ³n de Kafka (recomendado: `2.8.1` o superior).  
4. Especifica la cantidad de **brokers** y el tipo de instancia (ejemplo: `kafka.m5.large`).  
5. Configura el almacenamiento (ejemplo: `100 GiB` por broker).  
6. Asigna la **VPC, subnets y Security Group** creados en el paso 1.  
7. **Habilita CloudWatch Logs** y mÃ©tricas opcionales.  
8. **Crear clÃºster** y esperar a que estÃ© **Activo**.  

#### **ğŸ”¹ CreaciÃ³n con AWS CLI**  
Ejecuta el siguiente comando:  
```sh
aws kafka create-cluster --cluster-name MiKafkaCluster \
    --kafka-version 2.8.1 \
    --number-of-broker-nodes 3 \
    --broker-node-group-info file://broker-config.json
```

ğŸ“Œ **Ejemplo del archivo `broker-config.json`**:  
```json
{
    "InstanceType": "kafka.m5.large",
    "ClientSubnets": ["subnet-xxxxx", "subnet-yyyyy", "subnet-zzzzz"],
    "SecurityGroups": ["sg-xxxxxxx"],
    "StorageInfo": { "EbsStorageInfo": { "VolumeSize": 100 } }
}
```

### **3ï¸âƒ£ Crear un Topic en MSK**  
Para **crear un topic**, usa `kafka-topics.sh`:  
```sh
kafka-topics.sh --create --topic MiTopic \
  --bootstrap-server <BROKER_MSK> \
  --partitions 3 --replication-factor 2
```

### **4ï¸âƒ£ Conectar Productores y Consumidores**  
#### **ğŸ”¹ Productor en Python (kafka-python)**
```python
from kafka import KafkaProducer

producer = KafkaProducer(bootstrap_servers=['BROKER_MSK'])
producer.send('MiTopic', b'Hola desde MSK!')
producer.flush()
```

#### **ğŸ”¹ Consumidor en Python**
```python
from kafka import KafkaConsumer

consumer = KafkaConsumer('MiTopic', bootstrap_servers=['BROKER_MSK'])
for message in consumer:
    print(f'Recibido: {message.value}')
```

### **5ï¸âƒ£ Monitoreo y Seguridad**  
âœ… **CloudWatch** â†’ Monitorea mÃ©tricas de Kafka.  
âœ… **IAM & Security Groups** â†’ Controla accesos.  
âœ… **Cifrado TLS** â†’ Protege datos en trÃ¡nsito.

### **ğŸš€ ConclusiÃ³n**  
Desplegar un **clÃºster MSK** en AWS es simple con **CloudFormation, AWS CLI o la consola**.

### Resumen

Para esta demo vas a necesitar los siguientes comandos:

```bash
aws kafka list-clusters --region us-east-1
aws kafka describe-cluster --region us-east-1 --cluster-arn
aws kafka get-bootstrap-brokers --region us-east-1 --cluster-arn [clÃºsterARN]
```

**Lecturas recomendadas**

[List All Amazon MSK Clusters in an Account - Amazon Managed Streaming for Apache Kafka](https://docs.aws.amazon.com/es_es/msk/latest/developerguide/msk-list-clusters.html)

[Get the ZooKeeper Connection String for an Amazon MSK Cluster - Amazon Managed Streaming for Apache Kafka](https://docs.aws.amazon.com/es_es/msk/latest/developerguide/msk-get-connection-string.html)

## AWS - Glue

**AWS Glue** es un servicio **serverless** de AWS que facilita la **extracciÃ³n, transformaciÃ³n y carga (ETL)** de datos en entornos de **Big Data**. Permite integrar fuentes como **S3, RDS, Redshift, DynamoDB, y mÃ¡s**, sin necesidad de administrar infraestructura.

### **ğŸš€ Componentes Principales de AWS Glue**  

### **1ï¸âƒ£ CatÃ¡logo de Datos (Glue Data Catalog)**  
Un **repositorio centralizado** donde se almacenan metadatos sobre fuentes de datos.  
ğŸ“Œ Compatible con **Athena, Redshift, EMR y otros servicios de AWS**.

### **2ï¸âƒ£ Crawlers (Rastreador de Datos)**  
Detectan automÃ¡ticamente **esquemas** y **estructura** de datos almacenados en S3, RDS, DynamoDB, etc.  

ğŸ”¹ **Ejemplo de ejecuciÃ³n en AWS CLI**  
```sh
aws glue start-crawler --name MiCrawler
```

### **3ï¸âƒ£ Jobs de ETL**  
Permiten escribir **scripts en Python o Scala** para transformar y mover datos.  

ğŸ”¹ **Ejemplo de un Job en PySpark**
```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from pyspark.context import SparkContext

sc = SparkContext()
glueContext = GlueContext(sc)

datasource = glueContext.create_dynamic_frame.from_catalog(
    database="mi_base", table_name="mi_tabla"
)

transformed_df = datasource.toDF().filter("columna1 IS NOT NULL")

glueContext.write_dynamic_frame.from_options(
    frame=transformed_df, connection_type="s3",
    connection_options={"path": "s3://mi-bucket/output/"},
    format="parquet"
)
```

### **4ï¸âƒ£ Triggers y Workflows**  
Permiten **automatizar** la ejecuciÃ³n de Glue Jobs en base a eventos o programaciÃ³n.

### **ğŸ”¥ Casos de Uso**  
âœ… **Data Lakes** â†’ IntegraciÃ³n con **S3, Redshift, Athena**.  
âœ… **ETL con Big Data** â†’ TransformaciÃ³n de datos en **Spark** sin infraestructura.  
âœ… **MigraciÃ³n de Bases de Datos** â†’ Mover datos entre **DynamoDB, RDS, S3, Redshift**.  
âœ… **Procesamiento en Streaming** â†’ Con **Kinesis Data Streams**.

### **ğŸ“Œ IntegraciÃ³n con otros Servicios de AWS**  
ğŸ”¹ **Amazon S3** â†’ Almacena datasets y resultados de ETL.  
ğŸ”¹ **AWS Athena** â†’ Permite consultas SQL sobre datos procesados.  
ğŸ”¹ **AWS Redshift** â†’ Carga datos transformados en un DWH.  
ğŸ”¹ **Amazon RDS/DynamoDB** â†’ Extrae y carga datos desde bases de datos relacionales o NoSQL.

### Resumen

Dentro de este mÃ³dulo vamos a ver las herramientas que proporciona la nube de AWS para poder transformar nuestra data.

Algunas caracterÃ­sticas de Glue:

- Servicio totalmente administrado para implementar ETL (Extract, Transform, Load).
- Provee un contexto para trabajar basados en Python, Spark y Scala.
- Se encarga de crear catÃ¡logos de metadatos para que otros servicios puedan consultar la informaciÃ³n.
- Utiliza unidades de procesamiento llamadas DPU equivalente a 4 vCPU y 16GB RAM.
- Los Crawler van a escanear e identificar la informaciÃ³n para ponerla en el catÃ¡logo.
- aws s3 Los Classifier van a clasificar la data para ponerla en el catÃ¡logo.

## Demo - Instalando Apache Zeppelin

Apache Zeppelin es un cuaderno de datos interactivo que admite mÃºltiples intÃ©rpretes, como **Spark, Hive, Presto, Flink y mÃ¡s**. 

### ğŸ› ï¸ **1. Requisitos previos**  
Antes de instalar, asegÃºrate de tener:  
âœ… **Java 8+** (preferiblemente OpenJDK 8 o 11)  
âœ… **Python** (si vas a usar PySpark)  
âœ… **Git** (opcional, para clonar repositorios)

### ğŸ—ï¸ **2. Descarga de Apache Zeppelin**  

Ejecuta los siguientes comandos en Linux/macOS:  
```bash
wget https://dlcdn.apache.org/zeppelin/0.10.1/zeppelin-0.10.1-bin-netinst.tgz
tar -xvzf zeppelin-0.10.1-bin-netinst.tgz
cd zeppelin-0.10.1-bin-netinst
```
ğŸ“Œ Para Windows, descarga el **ZIP** desde [la pÃ¡gina oficial](https://zeppelin.apache.org/download.html) y extrae los archivos.

### ğŸš€ **3. Iniciar Zeppelin**  
Ejecuta:  
```bash
bin/zeppelin-daemon.sh start
```
Esto iniciarÃ¡ el servidor en `http://localhost:8080/`.

Si necesitas detener Zeppelin:  
```bash
bin/zeppelin-daemon.sh stop
```

### ğŸ”§ **4. ConfiguraciÃ³n (Opcional)**
Si necesitas configurar intÃ©rpretes o cambiar el puerto:  
```bash
nano conf/zeppelin-site.xml
```
TambiÃ©n puedes modificar `conf/zeppelin-env.sh` para definir variables de entorno, como **Java, Python o Spark**.

### ğŸ¯ **5. Acceder a Zeppelin**  
Abre en tu navegador:  
â¡ï¸ `http://localhost:8080/`  

Desde aquÃ­ puedes crear cuadernos y ejecutar cÃ³digo en diferentes intÃ©rpretes.

### âš¡ **IntegraciÃ³n con Spark**  
Si usas **Apache Spark**, asegÃºrate de definir `SPARK_HOME`:  
```bash
export SPARK_HOME=/ruta/a/spark
export PATH=$SPARK_HOME/bin:$PATH
```
Luego, en Zeppelin, en **Interpreter > Spark**, verifica que detecta Spark correctamente.

**Resumen**

Apache Zeppelin es un servicio que permite la ejecuciÃ³n de notebooks para anÃ¡lisis de datos utilizando SQL, Scala o Python. Tiene integraciÃ³n con los servicios de AWS.

**Usando la imagen oficial de Docker**

AsegÃºrese de que Docker estÃ© instalado en su mÃ¡quina local.

Utilice este comando para iniciar Apache Zeppelin en un contenedor.

`docker run -p 8080:8080 --rm --name zeppelin apache/zeppelin:0.12.0`

**Lecturas recomendadas**

[Apache Zeppelin 0.8.1 Documentation: Install](https://zeppelin.apache.org/docs/0.8.1/quickstart/install.html#starting-apache-zeppelin)

## CreaciÃ³n del Developer Endpoint

AWS Glue **Developer Endpoint** te permite ejecutar cÃ³digo interactivo para desarrollar, depurar y probar transformaciones de datos en AWS Glue usando **notebooks como Jupyter o Zeppelin**.

### ğŸ—ï¸ **1. Prerrequisitos**
Antes de crear un **Developer Endpoint**, asegÃºrate de:  
âœ… Tener una **VPC** y **subredes** configuradas.  
âœ… Contar con un **rol de IAM** con permisos para AWS Glue y S3.  
âœ… Tener configurado un **Security Group** con acceso a los servicios que usarÃ¡s.  
âœ… Haber creado una **clave SSH** para acceder al endpoint.

### ğŸš€ **2. CreaciÃ³n del Developer Endpoint en AWS Glue**
Puedes hacerlo desde la **Consola de AWS** o con **AWS CLI**.

### **ğŸ“Œ Desde la Consola de AWS**
1. Ve a **AWS Glue** en la Consola de AWS.  
2. En el panel de navegaciÃ³n, selecciona **Developer Endpoints**.  
3. Haz clic en **Create endpoint**.  
4. Configura los siguientes parÃ¡metros:
   - **Nombre** del Developer Endpoint.
   - **IAM Role** con permisos para Glue, S3 y acceso a logs.
   - **VPC y subred** donde se crearÃ¡ el endpoint.
   - **Security Groups** para controlar el acceso.
   - **Public Key SSH** para conectarte desde un cliente.
   - **Notebook Integration** (opcional, si quieres usar SageMaker o Jupyter).
5. Haz clic en **Create** y espera a que el endpoint se aprovisione.

### **ğŸ“Œ Desde AWS CLI**
Si prefieres crear el Developer Endpoint con **AWS CLI**, usa el siguiente comando:
```bash
aws glue create-dev-endpoint \
    --endpoint-name my-glue-dev-endpoint \
    --role-arn arn:aws:iam::123456789012:role/AWSGlueServiceRole \
    --public-key "ssh-rsa AAAAB3..." \
    --security-group-ids sg-0abc123456789 \
    --subnet-id subnet-0abc123456789 \
    --extra-python-libs-s3-path s3://my-bucket/libraries/ \
    --extra-jars-s3-path s3://my-bucket/jars/
```
ğŸ“Œ Reemplaza los valores segÃºn tu configuraciÃ³n.

### ğŸ”Œ **3. ConexiÃ³n al Developer Endpoint**
### **ğŸ“ ConexiÃ³n SSH**
Si agregaste una clave SSH, conÃ©ctate al endpoint con:
```bash
ssh -i my-key.pem glue@<DeveloperEndpointDNS>
```

### **ğŸ“ ConexiÃ³n desde Jupyter Notebook**
Si habilitaste la integraciÃ³n con **SageMaker**, abre el **AWS SageMaker Notebook** asignado al Developer Endpoint y empieza a ejecutar cÃ³digo interactivo.

### ğŸ¯ **4. VerificaciÃ³n**
Para verificar que el endpoint estÃ¡ activo:
```bash
aws glue get-dev-endpoint --endpoint-name my-glue-dev-endpoint
```
Si el estado es `READY`, ya puedes usarlo.

## ğŸ‰ **Â¡Listo! Tu AWS Glue Developer Endpoint estÃ¡ configurado y listo para usarse. ğŸš€**  

### Resumen

### Â¿CÃ³mo crear un endpoint de desarrollador en AWS Glue? 

Configurar un endpoint de desarrollador en AWS Glue es un paso esencial para aquellos que buscan integrar Apache Zeppelin y ejecutar consultas dentro de la plataforma de AWS. Este proceso le permitirÃ¡ realizar consultas de datos con eficiencia veamos cÃ³mo configurarlo correctamente.

### Â¿QuÃ© opciones ofrece la consola de AWS para notebooks?

Dentro de la consola de AWS, al explorar el servicio de Glue, encontrarÃ¡ la secciÃ³n de notebooks. AquÃ­, AWS ofrece dos opciones principales:

- **SageMaker Notebooks**: Orientados al servicio de Machine Learning de AWS, SageMaker.
- **Zeppelin Notebooks**: Ideales para quienes desean utilizar Apache Zeppelin para sus consultas y anÃ¡lisis de datos.

Para nuestro propÃ³sito de conectar un Apache Zeppelin local, optaremos por crear un notebook basado en Zeppelin.

### Â¿CÃ³mo crear un endpoint de desarrollador en AWS Glue?

Para comenzar, deberÃ¡ acceder a la secciÃ³n de Developer Endpoints dentro del servicio Glue. AquÃ­ es donde inicia la creaciÃ³n de un nuevo endpoint siguiendo estos pasos:

1. **Agregar un nuevo endpoint**: Al hacer clic en "Add Endpoint", se abre un formulario para configurar un nuevo endpoint. Le daremos el nombre "DevPlatziM" a nuestro endpoint.

2. **Seleccionar y configurar un rol de IAM**:

 - Debe contar con un rol de IAM que tenga permisos completos sobre los servicios de AWS Glue y acceso a S3, donde residirÃ¡ su data.
 - Si no posee un rol, AWS le ofrece crear uno directamente desde la interfaz.

3. Configurar los parÃ¡metros de cÃ³mputo:

 - Es recomendable especificar una capacidad de cÃ³mputo baja al trabajar con endpoints de desarrollador, para optimizar recursos. En este ejemplo, utilizaremos cinco DPUs.
 
4. **Ajustes de seguridad y librerÃ­as**:

Puede especificar librerÃ­as de Python y JAR necesarias para el developer endpoint, cargÃ¡ndolas desde S3.

### Â¿CÃ³mo configurar las opciones de red y seguridad del endpoint?

La configuraciÃ³n de networking es crucial para garantizar una conexiÃ³n segura y eficiente. Estas son algunas consideraciones:

- Desplegar dentro de una VPC: Aunque puede optar por una red genÃ©rica, si cuenta con VPCs privadas o pÃºblicas, puede seleccionar cuÃ¡l desea utilizar.

- GeneraciÃ³n de llaves SSH: Para conectar su Apache Zeppelin, necesitarÃ¡ llaves SSH:

 1. Acceda a su consola de comandos, regrese un directorio desde el de Zeppelin.
 2. Genere las llaves con el comando:
 
`ssh-keygen -t rsa -b 2048 -f platzi-llave`

 3. Se generarÃ¡n dos archivos: platzi-llave (privada) y platzi-llave.pub (pÃºblica).
 
- Carga de la llave pÃºblica al endpoint:

 - Copie el contenido de platzi-llave.pub.
 - PÃ©guelo en el campo correspondiente del endpoint para autorizar la conexiÃ³n a la plataforma AWS.
 
Siguiendo estos pasos, tiene la base para integrar Apache Zeppelin con AWS Glue, potenciando su capacidad de anÃ¡lisis de datos. Este conocimiento no solo amplÃ­a sus habilidades tÃ©cnicas, sino que tambiÃ©n le posiciona para seguir explorando el fascinante mundo de la gestiÃ³n de datos en la nube. Â¡ContinÃºe aprendiendo y explorando!

**Lecturas recomendadas**

[Trabajo con puntos de enlace de desarrollo en la consola de AWS Glue - AWS Glue](https://docs.aws.amazon.com/es_es/glue/latest/dg/console-development-endpoint.html)

[Tutorial: Set Up a Local Apache Zeppelin Notebook to Test and Debug ETL Scripts - AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-local-notebook.html)

## Demo - Conectando nuestro developer Endpoint a nuestro Zeppelin Edpoint

Para conectar tu **Developer Endpoint** de AWS Glue con un **Zeppelin Endpoint**, sigue estos pasos:

### **1. Crear un Developer Endpoint en AWS Glue**
Antes de conectar Apache Zeppelin, asegÃºrate de que tienes un **Developer Endpoint** de AWS Glue activo.

1. **Accede a la consola de AWS Glue**.
2. Ve a **"Developer Endpoints"** en el menÃº lateral.
3. **Crea un nuevo Developer Endpoint**, asegurÃ¡ndote de:
   - Seleccionar la versiÃ³n correcta de Glue.
   - Especificar una VPC, subred y grupo de seguridad compatibles con tu Apache Zeppelin.
   - Activar el acceso SSH si necesitas conexiÃ³n remota.
4. Una vez creado, copia la **direcciÃ³n del endpoint** para usarla mÃ¡s adelante.

### **2. Configurar Apache Zeppelin en EC2**
Si no tienes un servidor Zeppelin configurado, puedes lanzar una instancia EC2 y configurarlo manualmente o utilizar EMR:

### **OpciÃ³n 1: Instalar Zeppelin en una EC2 manualmente**
1. Lanza una **instancia EC2** (Amazon Linux o Ubuntu recomendado).
2. ConÃ©ctate a la instancia y ejecuta:
   ```bash
   sudo yum update -y
   wget https://downloads.apache.org/zeppelin/zeppelin-0.10.1-bin-netinst.tgz
   tar -xvzf zeppelin-*.tgz
   cd zeppelin-*
   ./bin/zeppelin-daemon.sh start
   ```
3. AsegÃºrate de abrir el puerto **8080** en los **grupos de seguridad** para acceder a Zeppelin en tu navegador.

### **OpciÃ³n 2: Configurar Zeppelin en un clÃºster EMR**
1. En la consola de **Amazon EMR**, crea un nuevo clÃºster con:
   - Apache Spark habilitado.
   - **Apache Zeppelin activado** en la configuraciÃ³n.
2. Espera a que el clÃºster se inicialice y obtÃ©n la **URL de Zeppelin** desde la consola de EMR.

### **3. Conectar AWS Glue con Zeppelin**
Ahora que tienes **Zeppelin** y tu **Developer Endpoint**, sigue estos pasos:

1. Abre **Apache Zeppelin** en tu navegador.
2. Ve a **"Interpreter"** en la barra de configuraciÃ³n.
3. Agrega un nuevo **interpreter para PySpark** y configÃºralo con:
   - **Master**: `yarn`
   - **glue.endpoint**: La direcciÃ³n de tu **Developer Endpoint** en AWS Glue.
   - **AWS Credentials**: Configura tus credenciales si es necesario.

4. Guarda los cambios y **reinicia Zeppelin**.

### **4. Validar la ConexiÃ³n**
Ejecuta el siguiente cÃ³digo en un **nuevo notebook** en Zeppelin para probar la conexiÃ³n con AWS Glue:

```python
sc.listFiles()
```
Si todo estÃ¡ bien, deberÃ­as ver una lista de archivos accesibles desde tu **Glue Developer Endpoint**.

### NOTA:

AWS **eliminÃ³ los Developer Endpoints en AWS Glue** a partir de **noviembre de 2023** y recomienda usar **AWS Glue Interactive Sessions** como alternativa.  

### **Â¿QuÃ© hacer ahora?**  
Si antes usabas un **Developer Endpoint** para conectarte a Zeppelin, ahora puedes hacerlo de dos maneras:  

### **1. Usar AWS Glue Interactive Sessions** (Recomendado)  
AWS Glue ahora permite sesiones interactivas en **Notebooks Jupyter y Zeppelin** sin necesidad de un Developer Endpoint.  

#### **Pasos para usar Glue Interactive Sessions en Zeppelin**  

1. **Configura Zeppelin en una instancia EC2 o Amazon EMR**  
   - Si no tienes Zeppelin instalado, sigue los pasos en la [documentaciÃ³n oficial de Zeppelin](https://zeppelin.apache.org/docs/latest/).  
   - Si usas **Amazon EMR**, habilita **Zeppelin** en la configuraciÃ³n del clÃºster.  

2. **Habilita AWS Glue Interactive Sessions en Zeppelin**  
   - Abre Zeppelin y ve a **Interpreter**.  
   - Crea un nuevo intÃ©rprete con la siguiente configuraciÃ³n:  

     ```
     Name: glue
     Interpreter Group: spark
     zeppelin.spark.useHiveContext: true
     zeppelin.pyspark.python: python3
     spark.hadoop.fs.s3.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
     ```

   - Guarda y reinicia Zeppelin.  

3. **ConÃ©ctate a Glue y prueba la sesiÃ³n**  
   - Abre un nuevo **Notebook** en Zeppelin.  
   - Ejecuta el siguiente cÃ³digo para probar la conexiÃ³n con AWS Glue:  

     ```python
     import sys
     from awsglue.context import GlueContext
     from pyspark.context import SparkContext

     sc = SparkContext()
     glueContext = GlueContext(sc)

     print(glueContext)
     ```

   - Si todo funciona correctamente, podrÃ¡s ejecutar transformaciones en AWS Glue desde Zeppelin.

### **2. Usar AWS Glue Studio Notebooks**  
Si solo necesitas un entorno interactivo para procesar datos con Glue, ahora AWS recomienda usar **Glue Studio Notebooks** en la consola de AWS.  

#### **CÃ³mo usar Glue Studio Notebooks**  
1. Ve a la **consola de AWS Glue**.  
2. Crea un nuevo **Job** y elige la opciÃ³n **Notebook** en vez de Script.  
3. Ejecuta cÃ³digo en tiempo real usando AWS Glue sin necesidad de Zeppelin o Developer Endpoints.

### **ConclusiÃ³n**  
- **Si usabas Developer Endpoints** â†’ Usa **AWS Glue Interactive Sessions** en Zeppelin.  
- **Si quieres un entorno nativo de AWS** â†’ Usa **AWS Glue Studio Notebooks**.  

AsÃ­ puedes seguir trabajando con AWS Glue sin necesidad de los Developer Endpoints. ğŸš€

### Resumen

### Â¿CÃ³mo configurar Apache Zeppelin para un developer endpoint?

Conectar tu entorno de desarrollo local a un Developer Endpoint en AWS puede ser un poco desafiante si no se tiene experiencia previa, pero con estos pasos podrÃ¡s configurarlo adecuadamente y sacar el mÃ¡ximo provecho a la capacidad de procesamiento de datos en la nube.

Primero, debes asegurarte de que tu developer endpoint estÃ© en estado "ready". Desde el panel de conexiÃ³n en AWS, toma nota de la direcciÃ³n y detalles que necesitarÃ¡s para hacer la conexiÃ³n desde tu entorno local, Apache Zeppelin.

### Â¿CÃ³mo configurar el interpreter de Spark en Zeppelin?

Para trabajar con Spark desde Apache Zeppelin, lo que necesitas hacer es muy sencillo. Sigue estos pasos:

1. Abre Apache Zeppelin y dirÃ­gete a la secciÃ³n de `Interpreters`.
2. Busca el interpreter para Spark y selecciona `Edit`.
3. Configura la propiedad `master` en `yarn client`. Si en las propiedades aparece `Spark executor memory` o `Spark driver memory`, elimÃ­nalas.
4. Configura un redireccionamiento de puerto al 9007 de localhost. Selecciona `connect to existing process` y establece `localhost` como host por el puerto `9007`.

### Â¿CÃ³mo conectar Apache Zeppelin local con el developer endpoint?

El siguiente paso es establecer la conexiÃ³n SSH. Esto implica abrir un tÃºnel entre tu mÃ¡quina local y el developer endpoint. AquÃ­ cÃ³mo:

1. DirÃ­gete a la consola de AWS Glue y copia el comando SSH proporcionado.
2. Reemplaza la llave por tu llave privada local, asegurÃ¡ndote de especificar la ruta si no estÃ¡s en el directorio correcto.
3. Ejecuta el comando en tu consola. Esto iniciarÃ¡ una conexiÃ³n que permitirÃ¡ ejecutar comandos en el endpoint.
4. Abre un nuevo comando SSH para trabajar con Scala. La mecÃ¡nica es igual que el anterior, asegurÃ¡ndote de usar la llave correcta.

### Â¿CÃ³mo usar Apache Zeppelin para visualizar y analizar datos?

DespuÃ©s de establecer la conexiÃ³n y verificar la ejecuciÃ³n de comandos bÃ¡sicos, puedes comenzar a utilizar la poderosa interfaz de Apache Zeppelin para realizar anÃ¡lisis mÃ¡s complejos.

1. Crea un nuevo notebook en Zeppelin y selecciona Spark como interpreter.
2. Ejecuta comandos y observa el resultado grÃ¡fico directamente desde tu notebook, apalancÃ¡ndote en la infraestructura en la nube para consultar y analizar la data en AWS.

Con este proceso, bÃ¡sicamente estÃ¡s armando un entorno de trabajo robusto donde pueden analizarse grandes volÃºmenes de datos, permitiendo un anÃ¡lisis en tiempo real y un testing eficaz antes de pasar cualquier desarrollo a producciÃ³n.

Â¿Listo para experimentar mÃ¡s con tus datos en la nube? Â¡ContinÃºa explorando y no dejes de aprender! El mundo de la transformaciÃ³n de datos es vasto y siempre hay algo nuevo por descubrir.

**Lecturas recomendadas**

[Tutorial: Set Up a Local Apache Zeppelin Notebook to Test and Debug ETL Scripts - AWS Glue](https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-tutorial-local-notebook.html)

## Demo - Creando nuestro primer ETL - Crawling

Para crear nuestro primer **ETL (Extract, Transform, Load) con Crawling** en AWS Glue, sigue estos pasos:  

### **1. Configurar AWS Glue y Crear un Crawler**
### **Paso 1: Crear una Base de Datos en Glue**
1. Inicia sesiÃ³n en la consola de AWS.
2. Ve a **AWS Glue** > **CatÃ¡logo de Datos** > **Bases de datos**.
3. Haz clic en **Agregar Base de Datos**.
4. Ingresa un nombre, por ejemplo: `mi_etl_db`, y guarda los cambios.

### **Paso 2: Crear un Crawler**
1. Ve a **AWS Glue** > **Crawlers**.
2. Haz clic en **Crear Crawler**.
3. Ingresa un nombre, por ejemplo: `crawler_etl`.
4. En **OrÃ­genes de datos**, selecciona:
   - **S3** si tienes archivos en un bucket.
   - **JDBC** si vas a extraer datos desde una base de datos relacional.
5. Agrega la ruta del bucket de S3 o la conexiÃ³n JDBC.
6. En **Rol de IAM**, selecciona un rol con permisos de AWS Glue y S3.
7. En **Destino**, elige la base de datos `mi_etl_db`.
8. Configura la frecuencia de ejecuciÃ³n (manual o automÃ¡tica).
9. Guarda y **ejecuta el Crawler**.

### **2. TransformaciÃ³n de Datos con AWS Glue ETL**
### **Paso 3: Crear un Trabajo ETL**
1. Ve a **AWS Glue** > **Trabajos**.
2. Haz clic en **Agregar trabajo**.
3. En **Nombre**, coloca `etl_transformacion`.
4. En **Tipo**, selecciona `ETL con script de Python o Spark`.
5. En **Rol de IAM**, selecciona el mismo rol del Crawler.
6. En **Origen de datos**, elige la tabla creada por el Crawler.
7. En **Destino**, selecciona:
   - **S3** (para almacenar datos transformados).
   - **JDBC** (si deseas cargar en una base de datos relacional).
8. Guarda los cambios y edita el script de transformaciÃ³n.

### **3. CÃ³digo de TransformaciÃ³n en PySpark**
Ejemplo de transformaciÃ³n con PySpark:
```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from pyspark.context import SparkContext
from awsglue.dynamicframe import DynamicFrame

# Crear contexto de Glue
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# Cargar datos desde Glue Data Catalog
datasource = glueContext.create_dynamic_frame.from_catalog(database="mi_etl_db", table_name="mi_tabla")

# TransformaciÃ³n: Filtrar datos nulos
transformed_data = DropNullFields.apply(frame=datasource)

# Guardar en S3 en formato Parquet
glueContext.write_dynamic_frame.from_options(
    frame=transformed_data,
    connection_type="s3",
    connection_options={"path": "s3://mi-bucket-transformado/"},
    format="parquet"
)
```

### **4. Cargar los Datos en un Destino**
DespuÃ©s de ejecutar el **trabajo ETL**, los datos transformados estarÃ¡n en el destino configurado (S3, RDS, Redshift, etc.).

### **5. Ejecutar y Monitorear el ETL**
1. Ve a **AWS Glue** > **Trabajos**.
2. Selecciona `etl_transformacion` y haz clic en **Ejecutar**.
3. Monitorea el estado en **Historial de EjecuciÃ³n**.

### Resumen

### Â¿CÃ³mo crear tu primer ETL con Apache Zeppelin y AWS Glue?

Iniciar en el mundo de la integraciÃ³n de datos y ETL (Extract, Transform, Load) puede ser abrumador, pero con herramientas como Apache Zeppelin y AWS Glue, este proceso se vuelve mucho mÃ¡s manejable. En este contenido, exploraremos cÃ³mo crear tu primer ETL y los pasos para craulear la data usando estos potentes servicios.

### Â¿CÃ³mo iniciar con AWS Glue y Apache Zeppelin?

Antes de comenzar, necesitas tener Apache Zeppelin instalado localmente y conectado a tu desarrollador de AWS Glue. Una vez que esto estÃ¡ configurado, el siguiente paso es aprender a crear tu primer ETL en fases. En esta guÃ­a, nos enfocaremos en cÃ³mo craulear la data inicial para transformaciÃ³n, comenzando por AWS Glue.

### Â¿QuÃ© es un crawler en AWS Glue y cÃ³mo se usa?

Un crawler en AWS Glue es una tarea automÃ¡tica que permite identificar y catalogar la data desde un bucket en S3. El proceso es el siguiente:

1. **Crea tu bucket de origen en S3**:

 - DirÃ­gete a Servicios en la consola de AWS.
 - Crea un bucket llamado origen-platzi.
 - Sube los archivos descargados desde el repositorio de GitHub en la carpeta JSON.

2. **Configura un nuevo crawler en AWS Glue**:

 - Accede al servicio Glue en AWS.
 - En Crawlers, agrega un nuevo crawler llamado `Platzi Crawler`.
 - Especifica el origen como el bucket de S3 que creaste (`origen-platzi`).
 - Configura el crawler para ejecutarse bajo demanda.

3. **Â¿CÃ³mo se realiza el proceso de crauleo?**

 - Ejecuta el crawler para que identifique y catalogue la data JSON en el Glue Catalog.
 - El Glue Catalog almacena los metadatos, permitiendo su uso por otros servicios como Athena.

### Â¿CÃ³mo se gestiona el Glue Catalog para tu data?

Una vez que se completa la operaciÃ³n del crawler, este crea tablas a partir de tus archivos JSON en una base de datos dentro del Glue Catalog. Todo el proceso se refleja de la siguiente manera:

- Revisa las tablas creadas en el Glue Catalog bajo tu base de datos PlatziDB.
- Usa servicios como Athena para consultar estas tablas y entender la estructura y contenido de tu data.

### Â¿CÃ³mo continuar con la transformaciÃ³n de data?

Una vez cargada la data en el Glue Catalog, el siguiente paso es transformarla y visualizarla nuevamente. Este proceso puede realizarse con ETL scripts mÃ¡s avanzados junto con herramientas de consulta como Athena.

### Recomendaciones PrÃ¡cticas

- **Revisa la documentaciÃ³n de AWS Glue y Apache Zeppelin regularmente**, ya que ambos servicios son actualizados frecuentemente con nuevas caracterÃ­sticas.
- **Explora ejemplos prÃ¡cticos en GitHub**: utilizar repositorios con ejemplos listos puede ayudarte a comprender el proceso completo de ETL.
- **Ejecuta pruebas en un entorno sandbox para familiarizarte con los errores comunes** y comprender cÃ³mo solucionarlos antes de implementarlo en producciÃ³n.

Con estas guÃ­as y herramientas, estarÃ¡s en camino de dominar la ingenierÃ­a de datos y explorar las posibilidades vastas que AWS y Apache Zeppelin ofrecen. Â¡Sigue profundizando en cada paso, mantÃ©n tu curiosidad y expande tus habilidades!

**Lecturas recomendadas**

[CÃ³digo de ejemplo: union de los datos y establecimiento de relaciones entre ellos - AWS Glue](https://docs.aws.amazon.com/es_es/glue/latest/dg/aws-glue-programming-python-samples-legislators.html)

[GitHub - czam01/glue-examples](https://github.com/czam01/glue-examples)

## Demo - Creando nuestro primer ETL - EjecuciÃ³n

Para ejecutar tu primer **ETL (Extract, Transform, Load)** en **AWS Glue**, sigue estos pasos:

### **1ï¸âƒ£ Crear un Job en AWS Glue**
1. **Accede a AWS Glue** desde la consola de AWS.
2. En el menÃº izquierdo, selecciona **"Jobs"** y haz clic en **"Create job"**.
3. Configura:
   - **Nombre del Job**: Un nombre descriptivo.
   - **IAM Role**: Un rol con permisos para acceder a S3 y Glue.
   - **Tipo de script**: Python o Scala.
   - **UbicaciÃ³n del cÃ³digo**: Puedes escribirlo en la consola o almacenarlo en S3.

### **2ï¸âƒ£ Escribir el cÃ³digo del ETL**
Ejemplo en **Python** usando **AWS Glue DynamicFrames**:

```python
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

# Inicializar contexto de Glue
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Extraer datos desde S3
datasource = glueContext.create_dynamic_frame.from_options(
    format_options={"multiline": True},
    connection_type="s3",
    format="json",
    connection_options={"paths": ["s3://mi-bucket/input/"], "recurse": True},
)

# Transformar datos
transformed_df = datasource.toDF()
transformed_df = transformed_df.withColumnRenamed("old_column", "new_column")

# Cargar datos a S3
output_dyf = DynamicFrame.fromDF(transformed_df, glueContext, "output_dyf")
glueContext.write_dynamic_frame.from_options(
    frame=output_dyf,
    connection_type="s3",
    connection_options={"path": "s3://mi-bucket/output/"},
    format="parquet"
)

job.commit()
```

### **3ï¸âƒ£ Ejecutar el Job**
1. Guarda el script en **S3** o en el editor de Glue.
2. En la consola de **AWS Glue**, selecciona el **Job** creado.
3. Haz clic en **"Run Job"** y monitorea su ejecuciÃ³n en **"Runs"**.

### **4ï¸âƒ£ Monitorear la EjecuciÃ³n**
- Ve a **AWS Glue â†’ Jobs â†’ Runs** para ver el estado del Job.
- Revisa **AWS CloudWatch Logs** para depurar errores.

ğŸ“Œ **Opcional:** Automatiza la ejecuciÃ³n con **AWS Lambda** o **AWS Step Functions** si deseas que el ETL corra periÃ³dicamente. ğŸš€

### Resumen

### Â¿CÃ³mo conectarse a AWS Glue para ejecutar tareas ETL?

Amazon Glue es un servicio de ETL (extracciÃ³n, transformaciÃ³n y carga) totalmente administrado que permite a los usuarios preparar y cargar sus datos para anÃ¡lisis. A lo largo de este artÃ­culo, exploraremos un escenario prÃ¡ctico sobre cÃ³mo configurar un entorno de desarrollo, verificar conexiones y ejecutar tareas de transformaciÃ³n en AWS Glue. AquÃ­ vamos.

### Â¿CÃ³mo configurar un entorno de desarrollo para AWS Glue?

En este caso, el primer paso es asegurarse de que su entorno de desarrollo estÃ© correctamente conectado. Para lograrlo, debemos establecer un tÃºnel SSH al endpoint de desarrollo. Estos son los pasos bÃ¡sicos:

1. **Configurar el tÃºnel SSH**: Utilice la llave privada y el endpoint de desarrollo proporcionado por AWS Glue. Abra la consola, seleccione el endpoint de desarrollo y copie el comando para iniciar el tÃºnel. AsegÃºrese de que la llave privada estÃ© disponible y ejecÃºtelo.

3. **Verificar la conexiÃ³n desde Zeppelin**: Una vez configurado el tÃºnel, es crucial verificar que tienes sincronizaciÃ³n con los datos almacenados en S3. Actualiza tu instancia de Zeppelin y confirma que estÃ¡ conectada con S3.

### Â¿CÃ³mo crear y ejecutar un contexto de Glue?

El siguiente paso despuÃ©s de la configuraciÃ³n del entorno de desarrollo es crear el contexto de Glue, el cual establece la conexiÃ³n y carga las librerÃ­as necesarias para la ejecuciÃ³n de las tareas ETL. AquÃ­ te explicamos cÃ³mo hacerlo:

1. **Crear el Glue Context**: Copia el comando para crear el Glue Context desde los ejemplos proporcionados en los enlaces del curso. Ve a tu interfaz de Zeppelin, pega la informaciÃ³n y ejecÃºtala.

3. **Crear un Dynamic Frame**: Con el Glue Context en marcha, el siguiente comando es crear un Dynamic Frame, que permitirÃ¡ identificar y manipular los datos desde la base de datos PlatziDB y la tabla 'persons-json'. Esto implica realizar la conexiÃ³n al catÃ¡logo Glue y verificar la cantidad de registros disponibles y su esquema.

### Â¿CÃ³mo corregir errores durante la ejecuciÃ³n de comandos?

Mientras se ejecutan comandos, es posible que te encuentres con errores. A continuaciÃ³n te mostramos cÃ³mo abordarlos:

1. **Agregar comandos que faltan**: Si un comando no se ejecuta correctamente, verifica que todas las librerÃ­as necesarias estÃ©n importadas. Puedes crear un nuevo bloque en Zeppelin donde especifiques que estÃ¡s trabajando con PySpark y luego ejecutar el comando completo con todas las librerÃ­as. 
2. **Usar la consola SSH para debuggin**g: Si los errores persisten, puedes intentar conectarte a PySpark a travÃ©s de SSH. Ejecuta los comandos lÃ­nea por lÃ­nea a travÃ©s de la CLI, lo cual puede facilitar la identificaciÃ³n de errores y otros problemas.

### Â¿CÃ³mo realizar consultÃ¡s directas para depuraciÃ³n?

Exploremos la opciÃ³n de conectarse al developer endpoint a travÃ©s de SSH para hacer un troubleshooting mÃ¡s detallado:

- **ConexiÃ³n vÃ­a SSH**: En caso de que encuentres errores al usar Zeppelin, puedes optar por conectarte directamente a PySpark a travÃ©s de SSH. Una vez conectado, ejecuta las consultas directamente desde la CLI.

- **Validar resultados en la CLI**: Ejecutando las consultas de esta manera, tendrÃ¡s oportunidad de ver los resultados en tiempo real, incluyendo errores o advertencias que puedan surgir durante la ejecuciÃ³n.

Este enfoque te permitirÃ¡ ejecutar anÃ¡lisis, solucionar errores y planificar los siguientes pasos en la arquitectura de datos de AWS Glue. Sigue explorando e integrando tus datos para maximizar su valor y seguir aprendiendo en el emocionante mundo de la ingenierÃ­a de datos.

**Lecturas recomendadas**

[aws-glue-samples/join_and_relationalize.md at master Â· aws-samples/aws-glue-samples Â· GitHub](https://github.com/aws-samples/aws-glue-samples/blob/master/examples/join_and_relationalize.md)

[GitHub - czam01/glue-examples](https://github.com/czam01/glue-examples)

## Demo - Creando nuestro primer ETL - Carga

La fase de **carga** en un proceso ETL consiste en almacenar los datos transformados en un destino adecuado, como Amazon S3, Amazon Redshift, DynamoDB, o una base de datos relacional en RDS. 

A continuaciÃ³n, te muestro cÃ³mo realizar la **carga de datos en S3** y otras opciones:

### **1ï¸âƒ£ Cargar los datos transformados en S3 (Parquet, CSV, JSON)**  
Si en la fase de transformaciÃ³n trabajaste con un **DynamicFrame** en AWS Glue, puedes cargarlo a S3 usando:

```python
glueContext.write_dynamic_frame.from_options(
    frame=output_dyf,  # DynamicFrame transformado
    connection_type="s3",
    connection_options={"path": "s3://mi-bucket/output/"},
    format="parquet"  # TambiÃ©n puedes usar "csv" o "json"
)
```
ğŸ“Œ **Opciones de formato:**
- `"parquet"` (Recomendado para analÃ­tica)
- `"json"`
- `"csv"` (Puede requerir opciones adicionales como delimitador)

### **2ï¸âƒ£ Cargar los datos en Amazon Redshift**
Si deseas cargar los datos en **Amazon Redshift**, usa **JDBC** con `connection_options`:

```python
glueContext.write_dynamic_frame.from_jdbc_conf(
    frame=output_dyf,
    catalog_connection="redshift-connection",
    connection_options={
        "database": "mi_base",
        "dbtable": "mi_tabla",
        "redshiftTmpDir": "s3://mi-bucket/temp-dir/"
    },
    redshift_tmp_dir="s3://mi-bucket/temp-dir/",
    transformation_ctx="datasink4"
)
```
âœ… **Requisitos para Redshift:**
- Un **Cluster Redshift** con una tabla creada.
- ConexiÃ³n JDBC en **AWS Glue Connections**.
- Permisos en el **IAM Role** para escribir en Redshift.

### **3ï¸âƒ£ Cargar los datos en Amazon DynamoDB**
Para almacenar datos en una **tabla de DynamoDB**, usa:

```python
glueContext.write_dynamic_frame.from_options(
    frame=output_dyf,
    connection_type="dynamodb",
    connection_options={"dynamodb.output.tableName": "mi_tabla_dynamodb"}
)
```
ğŸ“Œ **Consideraciones:**
- Asegurar que la tabla en DynamoDB ya exista.
- Configurar permisos de escritura en DynamoDB.

### **4ï¸âƒ£ Ejecutar y Monitorear la Carga**
DespuÃ©s de agregar la carga en el **script de AWS Glue**, sigue estos pasos:
1. **Guarda el script** en la consola de AWS Glue o en S3.
2. **Ejecuta el Job** desde la consola de Glue.
3. **Monitorea la ejecuciÃ³n** en **AWS Glue > Jobs > Runs**.
4. **Revisa logs en CloudWatch** en caso de errores.

### **ğŸš€ ConclusiÃ³n**
- **S3:** Ideal para almacenamiento escalable en formatos Parquet, JSON o CSV.
- **Redshift:** Para cargas analÃ­ticas y BI.
- **DynamoDB:** Para almacenamiento en NoSQL con alta disponibilidad.

Si necesitas optimizar la carga, considera **particionar los datos en S3** o **usar COPY en Redshift**.

### Resumen

### Â¿CÃ³mo crear y gestionar un Job en AWS Glue?

Al trabajar con grandes volÃºmenes de datos, la transformaciÃ³n de los mismos es clave. Amazon Web Services (AWS) Glue permite automatizar y escalar estas tareas de transformaciÃ³n mediante sus Jobs. Descubre cÃ³mo configurar y ejecutar un Job de ETL usando Glue y sigue paso a paso para optimizar tus datos.

### Â¿QuÃ© es un Job en AWS Glue?

Un Job en AWS Glue es un componente encargado de transformar datos. Son procesos que consisten en cÃ³digo de transformaciÃ³n que manipula los datos segÃºn se requiera. Estos son algunos pasos clave para configurar y utilizar un Job de AWS Glue:

1. **CreaciÃ³n del Job**:

 - Accede a la consola de AWS Glue y selecciona "Jobs".
 - Haz clic en "Agregar Job" y asigna un nombre, por ejemplo, "Platzi ETL".
 - Define el rol de IAM utilizado, como "AWS Glue Service Role Platzi".
 - Elige el tipo de ejecuciÃ³n, ya sea Spark o Python Shell. En este caso, seleccionamos Spark.

2. ConfiguraciÃ³n de Script:

 - Proporciona un nuevo script Python para el ETL.
 - Aprovecha las propiedades avanzadas para ajustar configuraciones como marcas de trabajo o tiempos de espera.

### Transformaciones ETL:

 - Edita el script Python para definir el Glue Context y especificar detalles del ETL.
 - Cambia el nombre de la base de datos y especifica directorios de salida en S3.

### Â¿CÃ³mo realizar transformaciones con Glue?
Para comenzar la transformaciÃ³n de datos, es importante definir al menos tres operaciones bÃ¡sicas:

1. **CreaciÃ³n de Dynamic Frames**: Son estructuras que permiten a Glue identificar y gestionar orÃ­genes de datos para transformaciones dinÃ¡micas.

2. **Operaciones de datos**:

 - Eliminar o renombrar columnas que no son necesarias.
 - Uniones (Joins) entre tablas para consolidar informaciÃ³n de distintas fuentes.
 
```python
# Ejemplo de un Join en el script Python de Glue
joined_data = DynamicFrame.fromDF(
    memberships_df.join(persons_df, memberships_df.id == persons_df.personid),
    glueContext,
    "joined_data"
)
```

2. **Escritura y Formato**:

 - Escribe los datos procesados en formato Parquet, que optimiza espacio y rendimiento.
 - Define el destino de escritura en S3 y particiona archivos segÃºn sea necesario.

### Â¿CÃ³mo automatizar y optimizar la ejecuciÃ³n de Jobs?

Automatizar la ejecuciÃ³n de Jobs es fundamental para optimizar el tiempo y recursos en un entorno profesional:

 - **Uso de Crawler**: DespuÃ©s de transformar los datos, un Crawler actualiza la metadata en AWS Glue Data Catalog.
 - **AutomatizaciÃ³n con SDK de AWS**: Orquesta ejecuciones automÃ¡ticamente, maneja errores y asegura reintentos en caso de fallas.

### Â¿QuÃ© sucederÃ¡ una vez finalizado el Job?
Una vez ejecutado el Job exitosamente, puedas verificar en S3 que los archivos se actualicen segÃºn lo previsto.

 - Comprueba el estado del job en la consola de AWS Glue. Debe mostrar 'succeeded' si se completÃ³ con Ã©xito.
 - Verifica la estructuraciÃ³n y calidad de los datos transformados usando servicios como AWS Atena.

Las operaciones realizadas a travÃ©s de AWS Glue son una vÃ­a poderosa y automatizada para manejar grandes volÃºmenes de datos. Estos pasos proporcionan un marco bÃ¡sico que se puede expandir y personalizar segÃºn las caracterÃ­sticas y necesidades de cada proyecto de Big Data. Â¡ContinÃºa explorando y aprendiendo para maximizar el potencial de tus proyectos con AWS Glue y mÃ¡s!

## AWS - EMR

AWS **Elastic MapReduce (EMR)** es un servicio en la nube que facilita el procesamiento de **grandes volÃºmenes de datos** utilizando **frameworks de Big Data** como **Apache Spark, Hadoop, Hive, Presto, HBase y Flink**. Se usa principalmente para anÃ¡lisis de datos, machine learning y procesamiento ETL.

### **ğŸš€ CaracterÃ­sticas clave de AWS EMR**
1. **Escalabilidad automÃ¡tica**: Ajusta la capacidad de los clusters segÃºn la demanda.
2. **Soporte para mÃºltiples frameworks**: Hadoop, Spark, Hive, Presto, Flink, etc.
3. **IntegraciÃ³n con AWS**: Se conecta fÃ¡cilmente con S3, RDS, DynamoDB, Redshift y otros servicios.
4. **AdministraciÃ³n simplificada**: AWS gestiona la configuraciÃ³n y el mantenimiento de los clusters.
5. **Pago por uso**: Paga solo por los recursos utilizados.

### **ğŸ“Œ Arquitectura de EMR**
Un clÃºster de EMR estÃ¡ compuesto por **tres tipos de nodos**:

1. **Master Node**: Controla la ejecuciÃ³n del clÃºster y distribuye tareas.
2. **Core Nodes**: Procesan los datos y almacenan informaciÃ³n en HDFS.
3. **Task Nodes** *(opcional)*: Solo ejecutan tareas sin almacenar datos.

### **ğŸ’» Pasos para crear un clÃºster EMR en AWS**
### **1ï¸âƒ£ ConfiguraciÃ³n inicial**
- Ve a la **consola de AWS** y busca **Amazon EMR**.
- Haz clic en **"Create cluster"**.

### **2ï¸âƒ£ Elegir un mÃ©todo de despliegue**
- **EMR on EC2**: Cluster en instancias EC2.
- **EMR on EKS**: Cluster gestionado en Kubernetes.
- **EMR Serverless**: EjecuciÃ³n sin necesidad de administrar servidores.

### **3ï¸âƒ£ Configurar el clÃºster**
- **Nombre del clÃºster**: Ej. `mi-cluster-emr`.
- **VersiÃ³n de EMR**: Ej. `6.9.0` (con Apache Spark, Hadoop, etc.).
- **Aplicaciones**: Selecciona los frameworks que necesitas (Spark, Hive, HBase, Presto, etc.).
- **Tipo de instancias EC2**: Ej. `m5.xlarge` (depende de la carga de trabajo).
- **Cantidad de nodos**:
  - 1 Master Node
  - 2+ Core Nodes (segÃºn el tamaÃ±o del clÃºster)

### **4ï¸âƒ£ Configurar almacenamiento**
- **S3**: Para almacenar los datos de entrada y salida.
- **HDFS**: Sistema de archivos distribuido dentro del clÃºster.

### **5ï¸âƒ£ Configurar networking**
- Selecciona la **VPC**, **subred** y **grupo de seguridad**.

### **6ï¸âƒ£ Opciones avanzadas**
- **Auto-terminaciÃ³n**: Configura si el clÃºster debe apagarse tras completar la tarea.
- **Spot Instances**: Reduce costos usando instancias Spot.
- **IAM Roles**: Define permisos para acceder a S3, Glue, DynamoDB, etc.

### **7ï¸âƒ£ Lanzar el clÃºster**
- Revisa la configuraciÃ³n y haz clic en **"Create cluster"**.

### **ğŸ“Š Ejecutar trabajos en EMR**
### **1ï¸âƒ£ Desde la consola AWS**
- Ve a **Clusters EMR** > **Actions** > **Submit Step**.
- Elige el tipo de trabajo (`Spark`, `Hive`, `Hadoop`, etc.).
- Sube el cÃ³digo o referencia un archivo en **S3**.

### **2ï¸âƒ£ Desde AWS CLI**
Ejecutar un script PySpark en EMR:

```bash
aws emr add-steps --cluster-id j-XXXX \
    --steps Type=Spark,Name="MySparkJob",ActionOnFailure=CONTINUE,Args=[--deploy-mode,client,--master,yarn,s3://mi-bucket/scripts/job.py]
```

### **3ï¸âƒ£ Con Jupyter Notebook en EMR**
- Activa **EMR Notebooks** para ejecutar cÃ³digo interactivo en un **Notebook Jupyter**.

### **ğŸ›  Integraciones con otros servicios de AWS**
âœ… **S3**: Almacenamiento de datos de entrada y salida.  
âœ… **Glue**: CatÃ¡logo de datos y transformaciÃ³n ETL.  
âœ… **Athena**: Consulta de datos sin servidores en S3.  
âœ… **Redshift**: IntegraciÃ³n con data warehouses.  
âœ… **CloudWatch**: Monitoreo y logs del clÃºster.  

### **ğŸ“Œ Casos de uso de EMR**
âœ… **AnÃ¡lisis de datos masivos**: Logs, sensores IoT, datos de redes sociales.  
âœ… **ETL**: ExtracciÃ³n, transformaciÃ³n y carga de grandes volÃºmenes de datos.  
âœ… **Machine Learning**: Entrenamiento de modelos con Spark MLlib.  
âœ… **Procesamiento en tiempo real**: Con Apache Flink o Spark Streaming.

### **ğŸ’° Costos de AWS EMR**
- Basado en **pago por hora** segÃºn el tipo de instancia EC2 y el nÃºmero de nodos.
- Puedes ahorrar usando **Spot Instances** o **EMR Serverless**.

ğŸ’¡ **Simula los costos en la calculadora de AWS**:  
[ğŸ”— AWS Pricing Calculator](https://calculator.aws/#/)

### **ğŸ¯ ConclusiÃ³n**
AWS EMR es una **soluciÃ³n poderosa y escalable** para procesar **Big Data** en la nube con **Spark, Hadoop y otros frameworks**. Su facilidad de integraciÃ³n con S3, Glue, Redshift y otros servicios lo hace ideal para empresas que manejan grandes volÃºmenes de informaciÃ³n.

**Resumen**

Elastic MapReduce o EMR es un clÃºster en el cual podemos correr cargas muy grandes de trabajo.

- Estos clusters son instancias de EC2 basadas en Hadoop.
- Provee interacciÃ³n con otros servicios de AWS como S3, RedShift, DynamoDB y Kinesis.
- Contamos con acciones Bootstrap, estos son scripts que se van a ejecutar al iniciar un clÃºster.
- Podemos ejecutar de manera ordenada distintos scripts utilizando Step.

## Demo - Desplegando nuestro primer clÃºster con EMR

AquÃ­ tienes una guÃ­a paso a paso para **desplegar tu primer clÃºster en AWS EMR**.

### **ğŸš€ Desplegar un ClÃºster EMR en AWS**
Un clÃºster de **Elastic MapReduce (EMR)** en AWS permite ejecutar frameworks de **Big Data** como **Apache Spark, Hadoop, Hive y Presto** para procesamiento de datos a gran escala.

### **1ï¸âƒ£ Configurar el ClÃºster desde la Consola AWS**
### **1.1 Ir a Amazon EMR**
1. Accede a la consola de AWS.
2. Busca "EMR" en la barra de bÃºsqueda y selecciona **Amazon EMR**.
3. Haz clic en **"Crear clÃºster"**.

### **1.2 Elegir el Tipo de Despliegue**
AWS EMR ofrece tres opciones:
- **EMR en EC2**: ClÃºster con instancias EC2 (recomendado para control total).
- **EMR en EKS**: Para ejecutar EMR sobre Kubernetes.
- **EMR Serverless**: Sin necesidad de gestionar servidores.

Para este caso, elegiremos **EMR en EC2**.

### **1.3 Configurar el ClÃºster**
#### **ğŸ”¹ ConfiguraciÃ³n bÃ¡sica**
- **Nombre del clÃºster**: `mi-cluster-emr`
- **VersiÃ³n de EMR**: Se recomienda la Ãºltima estable (ej. `6.9.0`).
- **Aplicaciones**:
  - Apache Spark (si trabajas con anÃ¡lisis de datos y ML)
  - Hadoop, Hive, Presto (para procesamiento ETL)
- **Modo de despliegue**: `ClÃºster estÃ¡ndar` (para un entorno persistente).

#### **ğŸ”¹ ConfiguraciÃ³n de la Red**
- **VPC**: Selecciona la VPC donde correrÃ¡ el clÃºster.
- **Subred**: Elige una subred disponible.
- **Grupo de seguridad**: Usa los predeterminados o crea uno personalizado.

#### **ğŸ”¹ Configurar los Nodos del ClÃºster**
Un clÃºster EMR tiene tres tipos de nodos:

| Tipo de Nodo  | FunciÃ³n  | Cantidad recomendada  |
|--------------|---------|----------------------|
| **Master** | Coordina las tareas del clÃºster. | `1` |
| **Core** | Procesa datos y almacena en HDFS. | `2+` |
| **Task (Opcional)** | Solo ejecuta tareas, no almacena datos. | `0+` |

**Ejemplo de configuraciÃ³n**:
- **Master Node**: `m5.xlarge`
- **Core Nodes**: `2` x `m5.xlarge`
- **Task Nodes**: Opcional

#### **ğŸ”¹ ConfiguraciÃ³n de Almacenamiento**
- **S3 Bucket**: Para almacenar logs y resultados de procesamiento.
- **HDFS**: Para almacenamiento distribuido dentro del clÃºster.

#### **ğŸ”¹ ConfiguraciÃ³n Avanzada**
- **IAM Roles**: `EMR_DefaultRole` (asegÃºrate de que tenga permisos adecuados para acceder a S3, DynamoDB, etc.).
- **Auto-terminaciÃ³n**: Habilitar si solo necesitas el clÃºster temporalmente.
- **Spot Instances**: Reducir costos usando instancias Spot para los nodos de cÃ³mputo.

### **1.4 Crear y Lanzar el ClÃºster**
- **Revisar configuraciÃ³n** y hacer clic en **"Crear clÃºster"**.
- El clÃºster tardarÃ¡ **de 5 a 15 minutos** en iniciarse.

### **2ï¸âƒ£ Ejecutar un Trabajo en el ClÃºster**
Una vez desplegado el clÃºster, puedes ejecutar trabajos de **Spark, Hadoop, Hive, Presto, etc.**.

### **2.1 Enviar una tarea desde la consola AWS**
1. Ve a **Clusters EMR**.
2. Selecciona tu clÃºster y haz clic en **"Submit Step"**.
3. Elige el tipo de tarea (Ejemplo: Spark, Hive, etc.).
4. Especifica la ubicaciÃ³n del script (ejemplo: `s3://mi-bucket/scripts/job.py`).
5. Ejecuta el trabajo.

### **2.2 Ejecutar un Trabajo con AWS CLI**
Si prefieres la lÃ­nea de comandos:

```bash
aws emr add-steps --cluster-id j-XXXX \
    --steps Type=Spark,Name="MySparkJob",ActionOnFailure=CONTINUE,Args=[--deploy-mode,client,--master,yarn,s3://mi-bucket/scripts/job.py]
```

### **2.3 Conectar Jupyter Notebook a EMR**
Para ejecutar cÃ³digo interactivo:
1. En la consola AWS, ve a **EMR** > **Notebooks** > **Create Notebook**.
2. ConÃ©ctalo al clÃºster EMR.
3. Escribe cÃ³digo en PySpark dentro del Notebook.

### **3ï¸âƒ£ Monitorear y Detener el ClÃºster**
Para revisar logs y estado del clÃºster:
- **CloudWatch Logs**: Ver registros de ejecuciÃ³n.
- **Cluster UI**: Acceder a interfaces como Spark UI.

Para **detener el clÃºster** y evitar costos adicionales:
```bash
aws emr terminate-clusters --cluster-ids j-XXXX
```

### **ğŸ¯ ConclusiÃ³n**
AWS EMR es una soluciÃ³n poderosa para ejecutar **Big Data y procesamiento ETL** en la nube. Siguiendo estos pasos, puedes desplegar y ejecutar tu primer clÃºster de forma eficiente.

**Resumen**

EMR tiene una gran cantidad de versiones, lo que cambia entre una versiÃ³n y otra son las herramientas de software que contiene. Para esta demo utilizaremos la versiÃ³n 5.20.0.

Un clÃºster de EMR se compone de:

- Master Nodes
- Core Nodes
- Task Nodes

## Demo - ConectÃ¡ndonos a Apache Zeppelin en EMR

Apache Zeppelin es una interfaz basada en web que permite interactuar con **Spark, Hive y Presto** en **AWS EMR**. Sigue estos pasos para conectarte a Zeppelin en tu clÃºster EMR. 

### **1ï¸âƒ£ Configurar Zeppelin al Crear el ClÃºster EMR**  
Si aÃºn no has creado el clÃºster, asegÃºrate de **habilitar Apache Zeppelin** en la configuraciÃ³n.  

ğŸ”¹ **Desde la Consola AWS**:  
1. Ve a **Amazon EMR** y haz clic en **Crear ClÃºster**.  
2. En la secciÃ³n **Aplicaciones**, selecciona:  
   - **Zeppelin** (para una interfaz interactiva).  
   - **Spark** (para ejecutar consultas).  
3. Configura el resto del clÃºster y lanza la instancia.  

Si el clÃºster ya estÃ¡ creado, puedes **instalar Zeppelin manualmente** con el siguiente comando:  
```bash
sudo amazon-linux-extras enable epel
sudo yum install zeppelin -y
```

### **2ï¸âƒ£ Acceder a Zeppelin en EMR**  
Una vez que el clÃºster estÃ¡ en ejecuciÃ³n:  

ğŸ”¹ **Desde la Consola AWS**:  
1. Ve a **Amazon EMR** > **Clusters**.  
2. Selecciona tu clÃºster y abre la pestaÃ±a **Aplicaciones**.  
3. Encuentra **Zeppelin** y copia el enlace de la interfaz web.  

ğŸ”¹ **Si no aparece el enlace**, debes configurar el acceso con un tÃºnel SSH:

### **3ï¸âƒ£ Crear un TÃºnel SSH para Acceder a Zeppelin**  
Para conectarte a Zeppelin desde tu navegador, necesitas un **tÃºnel SSH**.  

### **ğŸ”¹ Paso 1: Obtener la IP del Master Node**  
Ejecuta en AWS CLI:  
```bash
aws emr describe-cluster --cluster-id j-XXXXXX --query "Cluster.MasterPublicDnsName"
```
CopiarÃ¡ un resultado similar a:  
`ec2-XX-XXX-XX-XX.compute-1.amazonaws.com`

### **ğŸ”¹ Paso 2: Crear el TÃºnel SSH**  
Ejecuta en tu terminal:  
```bash
ssh -i "tu-clave.pem" -N -L 8890:localhost:8890 hadoop@ec2-XX-XXX-XX-XX.compute-1.amazonaws.com
```
âš ï¸ **Nota**:  
- Reemplaza `"tu-clave.pem"` por la clave SSH de tu instancia.  
- Reemplaza `ec2-XX-XXX-XX-XX.compute-1.amazonaws.com` con la IP de tu nodo master.

### **ğŸ”¹ Paso 3: Abrir Zeppelin en el Navegador**  
1. Abre un navegador y accede a:  
   ```
   http://localhost:8890
   ```
2. Â¡Listo! Ahora puedes ejecutar consultas en Zeppelin.

### **4ï¸âƒ£ Ejecutar CÃ³digo en Zeppelin**  
Una vez dentro de Zeppelin, puedes usar **PySpark** para analizar datos.  

Ejemplo de consulta en **PySpark**:  
```python
%pyspark
df = spark.read.csv("s3://mi-bucket/datos.csv", header=True, inferSchema=True)
df.show()
```

### **5ï¸âƒ£ Cerrar el TÃºnel SSH y el ClÃºster**  
Para evitar costos adicionales:  
1. **Cerrar el tÃºnel SSH** presionando `Ctrl + C` en la terminal.  
2. **Apagar el clÃºster** con:  
   ```bash
   aws emr terminate-clusters --cluster-ids j-XXXXXX
   ```

### **ğŸ¯ ConclusiÃ³n**  
Conectarse a Apache Zeppelin en AWS EMR te permite analizar datos de manera visual e interactiva. Si sigues estos pasos, podrÃ¡s acceder sin problemas y ejecutar consultas en Spark.  

### Resumen

### Â¿CÃ³mo conectar Apache Zeppelin a un clÃºster de EMR?

Conectar Apache Zeppelin a un clÃºster de EMR puede parecer un reto al principio, pero con los pasos adecuados, puedes hacerlo eficientemente y sacar el mÃ¡ximo provecho de tus cargas de trabajo en la nube. AquÃ­ te guiarÃ© en el proceso para habilitar esta conexiÃ³n y detallarÃ© las configuraciones necesarias para emplear Apache Zeppelin en Amazon EMR.

### Â¿QuÃ© se necesita para establecer la conexiÃ³n?

La conexiÃ³n a Apache Zeppelin desde un clÃºster de EMR requiere que ajustes los grupos de seguridad asociados al nodo maestro del clÃºster. Estos pasos son esenciales:

- **HabilitaciÃ³n de la conexiÃ³n web**: Por defecto, la conexiÃ³n web podrÃ­a no estar habilitada. Esto se debe a las restricciones impuestas por los grupos de seguridad que protegen el masternode del clÃºster.

- **ConfiguraciÃ³n del puerto adecuado**: Dependiendo de las herramientas instaladas en EMR, serÃ¡ necesario abrir diferentes puertos. Para Apache Zeppelin, es fundamental abrir el puerto 8890 en el masternode.

### Â¿CÃ³mo configurar el grupo de seguridad del nodo maestro?

Para habilitar la conexiÃ³n, es necesario modificar el grupo de seguridad del nodo principal. AquÃ­ se explica cÃ³mo hacerlo:

- **Acceso a la consola de EMR**: Ve a la consola de Amazon EMR y localiza tu clÃºster activo.
- **IdentificaciÃ³n de los grupos de seguridad**: Busca los grupos de seguridad asignados al nodo maestro y al nodo esclavo.
- **ModificaciÃ³n de reglas de entrada (inbound rules)**: En el grupo de seguridad del nodo maestro, aÃ±ade una nueva regla de entrada que permita el trÃ¡fico a travÃ©s del puerto 8890 desde cualquier direcciÃ³n de origen.
- **Guardado de configuraciones**: Guarda los cambios. Esto habilitarÃ¡ la conexiÃ³n web al clÃºster.

### Â¿CÃ³mo probar la conexiÃ³n a Zeppelin?

Una vez configurado el grupo de seguridad, sigue estos pasos para asegurarte de que tienes acceso a Apache Zeppelin:

- **Copiar y probar el DNS**: Copia el DNS pÃºblico del nodo maestro y prueba el acceso a Apache Zeppelin en un navegador utilizando el siguiente formato de URL: `http://<DNS_publico>:8890`.
- **VerificaciÃ³n de ejecuciÃ³n**: El navegador deberÃ¡ cargar la pÃ¡gina de inicio de Apache Zeppelin que estÃ¡ corriendo en tu clÃºster de EMR.

### Â¿CÃ³mo mejorar la seguridad en Apache Zeppelin?

La seguridad es crucial cuando operamos en entornos de nube. Por esto, es importante seguir las mejores prÃ¡cticas de seguridad y configuraciones adicionales para proteger tus datos.

- **Subred y balanceador de carga**: Se recomienda ejecutar el clÃºster dentro de una subred privada y poner un balanceador de carga en la subred pÃºblica. Esto ayuda a asegurar que solamente el trÃ¡fico permitido tenga acceso.
- **Certificados de seguridad**: Utiliza servicios como Route 53 para agregar un certificado de seguridad y un dominio, aumentando asÃ­ la seguridad de la conexiÃ³n.
- **Archivo shiro.ini**: Apache Zeppelin permite configuraciones avanzadas de seguridad mediante el archivo shiro.ini. Configura integraciones con Directorio Activo para requerir autenticaciÃ³n del usuario con nombre y contraseÃ±a.

### Â¿CÃ³mo visualizar mÃ¡s recursos en Spark?

Si deseas habilitar y visualizar recursos adicionales en Spark dentro de Apache Zeppelin, como el Spark History Server, sigue estas recomendaciones:

- **ConfiguraciÃ³n de vista de recursos detallados**: AsegÃºrate de que tienes habilitado Yarn y otras configuraciones en tu clÃºster para poder ver mÃ©tricas detalladas a nivel grÃ¡fico.
- **Acceso pÃºblico y seguridad**: Aunque puedas hacer visualizaciones pÃºblicas, asegÃºrate de seguir las recomendaciones de seguridad para no exponer tus datos a riesgos innecesarios.

El proceso de conexiÃ³n a Apache Zeppelin en EMR es un paso clave para maximizar tus trabajos de transformaciÃ³n en la nube. Explora las opciones de integraciones de seguridad y ajuste de recursos como Spark para generar informes detallados y proteger tus aplicaciones. Con la infraestructura de nube de AWS y las capacidades de anÃ¡lisis de Zeppelin, tus datos estarÃ¡n seguros y accesibles en todo momento. Â¡Sigue explorando y aprendiendo!

**Lecturas recomendadas**

[Apache Zeppelin - Amazon EMR](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-zeppelin.html)

[View Web Interfaces Hosted on Amazon EMR Clusters - Amazon EMR](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html)

## Demo- Despliegue automÃ¡tico de EMR con cloudformation

AWS CloudFormation permite desplegar clÃºsteres de **EMR (Elastic MapReduce)** de manera automatizada usando plantillas en **YAML o JSON**. A continuaciÃ³n, te explico cÃ³mo crear y desplegar un clÃºster EMR con **CloudFormation**.

### **1ï¸âƒ£ Crear la Plantilla de CloudFormation**
A continuaciÃ³n, una plantilla bÃ¡sica en **YAML** que despliega un clÃºster EMR con **Spark y Zeppelin**:

```yaml
AWSTemplateFormatVersion: "2010-09-09"
Description: "CloudFormation para desplegar un ClÃºster EMR"

Resources:
  EMRCluster:
    Type: "AWS::EMR::Cluster"
    Properties:
      Name: "MiClusterEMR"
      ReleaseLabel: "emr-6.7.0"  # VersiÃ³n de EMR
      Applications:
        - Name: "Spark"
        - Name: "Zeppelin"
      Instances:
        MasterInstanceGroup:
          InstanceType: "m5.xlarge"
          InstanceCount: 1
        CoreInstanceGroup:
          InstanceType: "m5.xlarge"
          InstanceCount: 2
      JobFlowRole: "EMR_EC2_DefaultRole"
      ServiceRole: "EMR_DefaultRole"
      VisibleToAllUsers: true
```

### **2ï¸âƒ£ Implementar la Plantilla en AWS CloudFormation**
### **ğŸ”¹ OpciÃ³n 1: Subir la Plantilla desde la Consola AWS**
1. Ir a la consola de **AWS CloudFormation**.
2. Seleccionar **Crear pila** â†’ **Con recursos nuevos**.
3. Seleccionar **Cargar un archivo** y subir el archivo `.yaml`.
4. Configurar los parÃ¡metros y hacer clic en **Crear pila**.
5. Esperar a que el estado cambie a **CREATE_COMPLETE**.

### **ğŸ”¹ OpciÃ³n 2: Desplegar con AWS CLI**
Si tienes instalado **AWS CLI**, ejecuta:
```bash
aws cloudformation create-stack --stack-name MiClusterEMR \
  --template-body file://mi_emr_template.yaml \
  --capabilities CAPABILITY_NAMED_IAM
```

Para verificar el estado del despliegue:
```bash
aws cloudformation describe-stacks --stack-name MiClusterEMR
```

### **3ï¸âƒ£ Acceder a Zeppelin en el ClÃºster EMR**
Una vez creado el clÃºster, puedes conectarte a Zeppelin con un **tÃºnel SSH**:

1. **Obtener la IP del nodo Master**:
   ```bash
   aws emr describe-cluster --cluster-id j-XXXXXX --query "Cluster.MasterPublicDnsName"
   ```

2. **Crear el tÃºnel SSH**:
   ```bash
   ssh -i "tu-clave.pem" -N -L 8890:localhost:8890 hadoop@ec2-XX-XXX-XX-XX.compute-1.amazonaws.com
   ```

3. **Abrir Zeppelin en el navegador**:
   ```
   http://localhost:8890
   ```

### **4ï¸âƒ£ Eliminar el ClÃºster para Ahorrar Costos**
Si ya no necesitas el clÃºster, elimÃ­nalo con:
```bash
aws cloudformation delete-stack --stack-name MiClusterEMR
```

### **ğŸ¯ ConclusiÃ³n**
Con esta configuraciÃ³n, puedes desplegar un clÃºster EMR de forma automÃ¡tica con **CloudFormation**, incluyendo **Spark y Zeppelin** para anÃ¡lisis de datos.

### Resumen

### Â¿CÃ³mo desplegar un clÃºster EMR de manera automatizada?

Desplegar un clÃºster EMR (Elastic MapReduce) de forma automatizada es una prÃ¡ctica esencial en entornos productivos. Utilizar infraestructuras como cÃ³digo facilita la repeticiÃ³n y configuraciÃ³n de los procesos, reduciendo el esfuerzo de administraciÃ³n. Vamos a explorar cÃ³mo lograrlo a travÃ©s de una plantilla de CloudFormation.

### Â¿QuÃ© es una plantilla de CloudFormation?

Una plantilla de CloudFormation es un recurso de AWS que permite definir la infraestructura y servicios que se desean implementar. Dentro de Ã©sta, se describe cada componente de una estructura tecnolÃ³gica, y se configura para implementar un clÃºster EMR de manera automatizada:

- **RegiÃ³n y nombre del ambiente**: Se define la regiÃ³n donde el clÃºster se desplegarÃ¡, junto con un nombre para identificar el entorno.

- **Subredes y VPCs**: Dependiendo del entorno (pÃºblico o privado), se determinan las subredes y VPCs donde el clÃºster operarÃ¡, permitiendo flexibilidad al modificar directamente las subredes o usar mapeos predefinidos.

### Â¿CÃ³mo gestionar los steps?

Los "steps" son acciones o comandos que se ejecutan secuencialmente dentro del clÃºster. En el cÃ³digo de infraestructura, estos steps estÃ¡n organizados con las siguientes reglas:

- **Dependencia**: Un step no comenzarÃ¡ hasta que el anterior finalice, asegurando un flujo de trabajo coherente.

- **Acciones en caso de fallo**: Se determina la respuesta del clÃºster si un step falla, como continuar con la ejecuciÃ³n o cancelar operaciones subsiguientes.

- **Argumentos adicionales**: Se pueden incluir directorios especÃ­ficos o ejecuciones puntuales personalizadas.

### Â¿CÃ³mo configurar las instancias del clÃºster?

En la plantilla, se especifica la cantidad y tipos de instancias, abarcando desde instancias master hasta instancias type core, todas con capacidad de demanda on demand. AdemÃ¡s, aspectos como el tamaÃ±o y nombre de las instancias se determinan segÃºn necesidades especÃ­ficas. Entre otros detalles, se incluyen:

- **Subred de las instancias**: Se puede definir usando directamente el ID de la subred o referenciar mapeos que obtengan el ID adecuado segÃºn el entorno.

- **Seguridad y conexiones**: La plantilla incluye configuraciones predeterminadas para grupos de seguridad, diferenciando entre desplegues en subredes pÃºblicas y privadas, y contemplando el uso de llaves SSH para conexiÃ³n a instancias master.

- **Bootstrap actions**: Acciones que se ejecutan antes de que el clÃºster estÃ© activo, como el uso de scripts localizados en buckets S3.

### Â¿QuÃ© aplicaciones y configuraciones adicionales se pueden incluir?

El clÃºster EMR es altamente configurable. AquÃ­ algunos aspectos que se pueden personalizar:

- **Aplicaciones instaladas**: Se define quÃ© aplicaciones instalar, como Zeppelin, Hadoop, Spark, especificando versiones que dependen de la versiÃ³n del EMR.

- **Configuraciones de logs y Java**: Es posible definir configuraciones especÃ­ficas de logs y ajustar la versiÃ³n de Java (p. ej., Java 1.8 en este caso).

- **Roles y etiquetas**: Se especifican los roles que el clÃºster utilizarÃ¡ por defecto y las etiquetas que ayudarÃ¡n a identificar los recursos.

### Â¿CÃ³mo se automatiza el despliegue en un entorno productivo?

El uso de repositorios de cÃ³digo y herramientas de integraciÃ³n continua como CodePipeline facilita el despliegue automatizado:

1. **Repositorio de cÃ³digo**: La plantilla de CloudFormation se almacena en repositorios como GitHub o Bitbucket.

3. **CodePipeline**: Toma las tareas del repositorio y despliega la plantilla, lanzando el clÃºster y ejecutando los steps.

5. **Eventos automÃ¡tico**s: Herramientas como CloudWatch pueden programar eventos diarios, como lanzar pipelines a medianoche para procesar, por ejemplo, los logs del dÃ­a anterior.

Esta automatizaciÃ³n no solo agiliza procesos, sino que tambiÃ©n optimiza costos, al permitir apagados automÃ¡ticos del clÃºster tras completar las tareas, evitando gastos innecesarios por instancias funcionando sin uso activo.

**Lecturas recomendadas**

[GitHub - czam01/emr-cloudformation](https://github.com/czam01/emr-cloudformation)

## AWS - Lambda

AWS Lambda es un servicio de computaciÃ³n **sin servidor (serverless)** que te permite ejecutar cÃ³digo sin necesidad de gestionar servidores. Se activa mediante eventos y se escala automÃ¡ticamente.  

âœ… **Casos de uso**:  
- Procesamiento de datos en tiempo real  
- AutomatizaciÃ³n de tareas  
- Respuesta a eventos en S3, DynamoDB, API Gateway, etc.  
- IntegraciÃ³n con otros servicios de AWS  

### **1ï¸âƒ£ Creando una funciÃ³n Lambda desde la Consola**  
### ğŸ”¹ **Pasos para crear una funciÃ³n Lambda en AWS**  
1. **Ir a la consola de AWS Lambda**  
2. **Hacer clic en "Crear funciÃ³n"**  
3. Seleccionar **"Crear desde cero"**  
4. **Asignar un nombre** y elegir el **runtime** (por ejemplo, Python 3.9)  
5. **Asignar permisos** (usar una IAM Role con acceso adecuado)  
6. Hacer clic en **"Crear funciÃ³n"**  

### ğŸ”¹ **Ejemplo de cÃ³digo en Python**  
```python
import json

def lambda_handler(event, context):
    return {
        'statusCode': 200,
        'body': json.dumps('Â¡Hola desde AWS Lambda!')
    }
```

---

### **2ï¸âƒ£ ImplementaciÃ³n con AWS CLI**  
Si prefieres desplegar Lambda desde la terminal, sigue estos pasos:

### **ğŸ”¹ 1. Crear el archivo de cÃ³digo**
Guarda este cÃ³digo en un archivo `lambda_function.py`:
```python
def lambda_handler(event, context):
    return {"message": "Hola desde Lambda CLI"}
```

### **ğŸ”¹ 2. Crear un archivo ZIP**
Empaqueta el cÃ³digo en un archivo ZIP:
```bash
zip function.zip lambda_function.py
```

### **ğŸ”¹ 3. Crear la funciÃ³n Lambda con AWS CLI**
```bash
aws lambda create-function \
  --function-name MiFuncionLambda \
  --runtime python3.9 \
  --role arn:aws:iam::XXXXXXXXXXXX:role/service-role/rol_lambda \
  --handler lambda_function.lambda_handler \
  --zip-file fileb://function.zip
```

### **ğŸ”¹ 4. Invocar la funciÃ³n Lambda desde CLI**
```bash
aws lambda invoke --function-name MiFuncionLambda response.json
cat response.json
```

### **3ï¸âƒ£ Activar Lambda con S3 (Ejemplo de Trigger)**  
Puedes configurar Lambda para que se ejecute automÃ¡ticamente cuando un archivo se suba a un bucket de **S3**.

### ğŸ”¹ **Pasos para configurar el Trigger en la Consola**  
1. Ir a **AWS Lambda** y seleccionar la funciÃ³n creada  
2. Hacer clic en **"Agregar Trigger"**  
3. Seleccionar **S3** y elegir el bucket  
4. En **evento**, seleccionar **"PUT"** (cuando se sube un archivo)  
5. Guardar los cambios  

### ğŸ”¹ **Ejemplo de CÃ³digo que procesa archivos en S3**
```python
import json
import boto3

s3 = boto3.client('s3')

def lambda_handler(event, context):
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        print(f"Nuevo archivo en S3: {bucket}/{key}")

    return {"statusCode": 200, "body": json.dumps("Archivo procesado")}
```

### **4ï¸âƒ£ Eliminar la funciÃ³n Lambda**  
Si ya no necesitas la funciÃ³n, elimÃ­nala con:
```bash
aws lambda delete-function --function-name MiFuncionLambda
```

### **ğŸ¯ ConclusiÃ³n**
AWS Lambda es una soluciÃ³n eficiente para **ejecutar cÃ³digo sin servidores** y reaccionar a eventos en tiempo real.

**Resumen**

Al momento de hacer proyectos de Big Data con Lambda debes tomar en cuenta:

- La **cantidad de llamadas concurrentes** a la funciÃ³n lambda, por defecto tienes un lÃ­mite de 1000 llamadas concurrentes, es posible llegar hasta 20000.
- Se puede integrar con Kinesis Firehose para realizar transformaciones de datos.
- Es recomendable utilizar **colas de trabajo** para que las tareas estÃ©n en espera mientras la lambda se va desocupando. Pierdes un poco de real-time, pero no habrÃ¡ delay en la lambda.
- Optimizar y automatizar el despliegue de cÃ³digo en las lambdas usando Codepipeline y Boto3.

## Ejemplos AWS- Lambda

A continuaciÃ³n, te muestro varios ejemplos de funciones **AWS Lambda** con distintos propÃ³sitos, utilizando **Python**. 

### **1ï¸âƒ£ Lambda BÃ¡sico - "Hola Mundo"**
Ejecuta una funciÃ³n bÃ¡sica que retorna un mensaje de respuesta.  
```python
import json

def lambda_handler(event, context):
    return {
        'statusCode': 200,
        'body': json.dumps('Â¡Hola desde AWS Lambda!')
    }
```
âœ… **Casos de uso:** Pruebas bÃ¡sicas, despliegue inicial.

### **2ï¸âƒ£ Procesando un Evento de S3**  
Esta funciÃ³n se activa cuando un archivo se sube a un bucket de **S3**.  
```python
import json
import boto3

s3 = boto3.client('s3')

def lambda_handler(event, context):
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']
        print(f"Archivo subido: {bucket}/{key}")

    return {"statusCode": 200, "body": json.dumps("Evento procesado")}
```
âœ… **Casos de uso:** Procesamiento de archivos, notificaciones automÃ¡ticas.

### **3ï¸âƒ£ Guardar Datos en DynamoDB**  
Esta funciÃ³n recibe un evento, extrae datos y los guarda en **DynamoDB**.  
```python
import boto3
import json

dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('MiTabla')

def lambda_handler(event, context):
    item = {
        'id': event['id'],
        'nombre': event['nombre'],
        'edad': event['edad']
    }
    table.put_item(Item=item)
    
    return {"statusCode": 200, "body": json.dumps("Datos guardados en DynamoDB")}
```
âœ… **Casos de uso:** Registro de usuarios, almacenamiento de datos estructurados.

### **4ï¸âƒ£ Enviar un Correo con SES (Simple Email Service)**  
EnvÃ­a un email usando **AWS SES**.  
```python
import boto3

ses = boto3.client('ses')

def lambda_handler(event, context):
    response = ses.send_email(
        Source="tucorreo@example.com",
        Destination={'ToAddresses': ["destino@example.com"]},
        Message={
            'Subject': {'Data': "NotificaciÃ³n desde AWS Lambda"},
            'Body': {'Text': {'Data': "Este es un correo de prueba"}}
        }
    )
    
    return {"statusCode": 200, "body": "Correo enviado correctamente"}
```
âœ… **Casos de uso:** Notificaciones automÃ¡ticas, alertas.

### **5ï¸âƒ£ Ejecutar un Query en Athena**  
Ejecuta una consulta en **AWS Athena** y retorna los resultados.  
```python
import boto3

athena = boto3.client('athena')

def lambda_handler(event, context):
    query = "SELECT * FROM mi_base.mi_tabla LIMIT 10;"
    response = athena.start_query_execution(
        QueryString=query,
        QueryExecutionContext={'Database': 'mi_base'},
        ResultConfiguration={'OutputLocation': 's3://mi-bucket-athena/'}
    )
    return {"statusCode": 200, "body": f"Query ejecutado: {response['QueryExecutionId']}"}
```
âœ… **Casos de uso:** AnÃ¡lisis de datos, consultas sin servidores.

### **6ï¸âƒ£ Conectando Lambda con API Gateway**  
Lambda puede servir como backend para **API Gateway**.  
```python
import json

def lambda_handler(event, context):
    response = {
        "statusCode": 200,
        "body": json.dumps({"mensaje": "Â¡Hola desde API Gateway y Lambda!"})
    }
    return response
```
âœ… **Casos de uso:** CreaciÃ³n de APIs sin servidores.

### **7ï¸âƒ£ Detener Instancias de EC2**  
Esta funciÃ³n **detiene todas las instancias EC2 en una regiÃ³n** especÃ­fica.  
```python
import boto3

ec2 = boto3.client('ec2')

def lambda_handler(event, context):
    instances = ec2.describe_instances(Filters=[{"Name": "instance-state-name", "Values": ["running"]}])
    instance_ids = [inst['InstanceId'] for res in instances['Reservations'] for inst in res['Instances']]
    
    if instance_ids:
        ec2.stop_instances(InstanceIds=instance_ids)
        return {"statusCode": 200, "body": "Instancias detenidas"}
    else:
        return {"statusCode": 200, "body": "No hay instancias activas"}
```
âœ… **Casos de uso:** OptimizaciÃ³n de costos, apagado programado.

### ğŸ“Œ **ConclusiÃ³n**
AWS Lambda se puede usar para mÃºltiples propÃ³sitos como:  
âœ” AutomatizaciÃ³n  
âœ” Procesamiento de eventos  
âœ” CreaciÃ³n de APIs  
âœ” IntegraciÃ³n con otros servicios de AWS  

### Resumen

### Â¿CÃ³mo se utilizan las funciones Lambda en proyectos de Big Data?

Las funciones Lambda han demostrado su eficiencia y versatilidad en la gestiÃ³n de datos a gran escala. En el siguiente contenido exploraremos cÃ³mo estas funciones se implementan en proyectos de Big Data para ofrecer capacidades de procesamiento en tiempo real y batch. Al comprender su funcionamiento, podrÃ¡s transformar la manera en que gestionas y distribuyes datos en tus proyectos.

### Â¿CuÃ¡l es el flujo de datos en tiempo real con Lambda?

En proyectos de Big Data, se puede establecer un sofisticado flujo de datos usando funciones Lambda. A travÃ©s de CloudWatch, se genera un flujo de logs que activa una funciÃ³n Lambda de distribuciÃ³n. Esta funciÃ³n, tambiÃ©n conocida como Lambda de Fan Out, recibe los eventos y los distribuye eficientemente a mÃºltiples Lambdas que, a su vez, alimentan diferentes endpoints.

- La distribuciÃ³n se realiza mediante SNS (Simple Notification Service), aunque tambiÃ©n es posible utilizar SQS (Simple Queue Service).
- Con SQS, hay que tener presente que un evento puede llegar mÃ¡s de una vez, por lo que es necesario manejar posibles duplicados.

Este flujo garantiza que, desde CloudWatch, los eventos son procesados y distribuidos hacia aplicaciones como Elasticsearch y Kibana, optimizando la gestiÃ³n de Ã­ndices y consultas.

### Â¿CÃ³mo se integran las Lambdas con bases de datos en memoria?

En ciertos escenarios, es esencial utilizar una base de datos en memoria, como Redis, para evitar la duplicaciÃ³n de eventos crÃ­ticos al alimentar un endpoint. Para lograr esto:

- Se coloca una Lambda en una VPC (Virtual Private Cloud) para acceder a Redis mediante un NAT Gateway.
- La funciÃ³n Lambda consulta Redis para verificar si un evento ha sido procesado antes. Si no ha sido procesado, la Lambda procede con el procesamiento, asegurando la unicidad de eventos en el endpoint.

Esta implementaciÃ³n es crucial cuando se requiere que eventos Ãºnicos lleguen a un endpoint sin duplicaciones.

### Â¿CÃ³mo pueden reemplazar las Lambdas a un clÃºster de EMR?

Un clÃºster de Elastic MapReduce (EMR) puede ser sustituido por una orquestaciÃ³n de Lambdas y S3 para procesamiento batch, emulando funcionalidades de Map Reduce.

- Las funciones Lambda pueden enviar eventos a S3, activando otras Lambdas para consolidar informaciÃ³n.
- Un Coordinador se encarga de organizar las tareas de reducciÃ³n y consolidaciÃ³n.

Este enfoque ofrece una soluciÃ³n eficiente y escalable para procesos batch, eliminando la necesidad de mantener un clÃºster EMR dedicado.

### Aplicaciones prÃ¡cticas de las funciones Lambda

Las funciones Lambda son un pilar esencial en la arquitectura de Big Data, ya sea en tiempo real o para procesamiento batch. Su versatilidad permite su uso en:

- Proyectos de anÃ¡lisis de datos en tiempo real, como la alimentaciÃ³n de Elasticsearch.
- Procesamiento de datos batch, replicando funcionalidades de Map Reduce.
- IntegraciÃ³n con otros servicios en la nube, como Kinesis Firehose.

Incorporando Lambda en tu diseÃ±o arquitectÃ³nico, puedes optimizar tus procesos de Big Data, incrementando la eficiencia, reduciendo costos y mejorando la escalabilidad de tus operaciones. Â¡ContinÃºa aprendiendo y descubre todo lo que las funciones Lambda pueden ofrecerte en tus innovadores proyectos de datos! 

## Demo - Creando una lambda para BigData

AquÃ­ tienes un **Dockerfile** para crear un entorno con AWS Lambda orientado a Big Data. La funciÃ³n Lambda podrÃ­a procesar datos desde S3, utilizar PySpark, Pandas o Boto3 para interactuar con servicios de AWS.

### **Dockerfile para AWS Lambda con Big Data**
```dockerfile
# Imagen base oficial de AWS Lambda con Python
FROM public.ecr.aws/lambda/python:3.8

# Actualizar paquetes e instalar dependencias necesarias
RUN yum update -y && \
    yum install -y unzip tar gzip curl && \
    yum clean all

# Instalar paquetes de Python para Big Data
RUN pip install --no-cache-dir \
    boto3 \
    pandas \
    numpy \
    pyarrow \
    fastparquet \
    s3fs \
    requests

# Copiar el cÃ³digo fuente de la Lambda
COPY lambda_function.py ${LAMBDA_TASK_ROOT}

# Configurar la Lambda para ejecutarse con el archivo
CMD ["lambda_function.lambda_handler"]
```

### **CÃ³digo de Ejemplo (`lambda_function.py`)**
Este cÃ³digo lee un archivo CSV de un bucket de S3, lo convierte en Parquet y lo guarda nuevamente en S3.

```python
import json
import boto3
import pandas as pd
import io

s3 = boto3.client('s3')

def lambda_handler(event, context):
    bucket_name = "mi-bucket-datos"
    input_file = "input/datos.csv"
    output_file = "output/datos.parquet"

    # Descargar el archivo CSV de S3
    obj = s3.get_object(Bucket=bucket_name, Key=input_file)
    df = pd.read_csv(io.BytesIO(obj['Body'].read()))

    # Convertir a formato Parquet
    buffer = io.BytesIO()
    df.to_parquet(buffer, engine='pyarrow')

    # Subir archivo Parquet a S3
    s3.put_object(Bucket=bucket_name, Key=output_file, Body=buffer.getvalue())

    return {
        'statusCode': 200,
        'body': json.dumps(f'Archivo {output_file} guardado en S3')
    }
```

### **CÃ³mo Construir y Ejecutar**
1. **Construir la imagen Docker**
   ```sh
   docker build -t my-lambda-bigdata .
   ```

2. **Ejecutar Lambda en Local**
   ```sh
   docker run -p 9000:8080 my-lambda-bigdata
   ```

3. **Probar con una PeticiÃ³n Local**
   ```sh
   curl -XPOST "http://localhost:9000/2015-03-31/functions/function/invocations" -d '{}'
   ```

Este setup te permite desplegar una funciÃ³n Lambda optimizada para Big Data en AWS. ğŸš€  
Si necesitas mÃ¡s optimizaciÃ³n (por ejemplo, integrar PySpark en AWS EMR), dime y lo ajustamos.

### Resumen

### Â¿CÃ³mo podemos crear una funciÃ³n Lambda en AWS para Big Data?

Las funciones Lambda en AWS son una herramienta clave en la gestiÃ³n y procesamiento de Big Data gracias a su capacidad para ejecutar cÃ³digo en respuesta a eventos y su integraciÃ³n con otros servicios de AWS. A continuaciÃ³n, te explicamos cÃ³mo crear una funciÃ³n Lambda desde cero, paso a paso, y los aspectos crÃ­ticos a considerar.

### Â¿CÃ³mo crear una funciÃ³n Lambda desde cero?

- **Accede a la consola de AWS**: Inicia sesiÃ³n en tu consola de AWS y busca el servicio Lambda.
- **Crea la funciÃ³n**: Selecciona "Crear funciÃ³n" y elige la opciÃ³n de crear una desde cero. Asigna un nombre, por ejemplo, "Platzi", y selecciona el tiempo de ejecuciÃ³n adecuado (en este caso, Python 3.6 es usado).
- **Selecciona un rol**: Es crucial definir el rol que determinarÃ¡ los permisos de la funciÃ³n Lambda para interactuar con otros servicios.

### Â¿QuÃ© son los desencadenadores o triggers?

Los triggers son eventos que inician la ejecuciÃ³n de una funciÃ³n Lambda. Para proyectos de Big Data, es comÃºn utilizar SNS o SQS. Si optas por SQS, se conectarÃ¡ a colas de tipo estÃ¡ndar. Estos servicios permiten orquestar flujos de trabajo complejos al notificar a tu funciÃ³n Lambda cuando ciertos eventos ocurren.

### Â¿QuÃ© es la funcionalidad "layers"?

"Layers" es una funcionalidad que simplifica la gestiÃ³n de librerÃ­as compartidas entre mÃºltiples funciones Lambda. Al utilizar "layers", puedes centralizar y replicar librerÃ­as de manera eficiente, reduciendo el tiempo de administraciÃ³n.

### Â¿CuÃ¡l es la importancia de las variables de entorno?

En Big Data, las variables de entorno juegan un papel crucial para el manejo seguro de conexiones, como aquellas a bases de datos. Siempre se recomienda encriptar esta informaciÃ³n, utilizando servicios como KMS para garantizar la confidencialidad.

### Â¿CÃ³mo gestionar roles y permisos?

El rol de la funciÃ³n Lambda define con quÃ© servicios puede interactuar. Por ejemplo, un rol puede otorgar acceso a CloudWatch Logs y CloudFormation. Es fundamental aplicar el principio de menor privilegio, otorgando solo los permisos necesarios.

### Â¿CÃ³mo optimizar y configurar nuestras funciones Lambda?

La optimizaciÃ³n y configuraciÃ³n adecuadas de funciones Lambda son esenciales para maximizar la eficacia en los proyectos de Big Data. Exploremos varias configuraciones importantes.

### Â¿CÃ³mo gestionar la memoria y el tiempo de ejecuciÃ³n?

- **Memoria**: Ajusta la memoria en funciÃ³n del cÃ³digo y la ejecuciÃ³n necesaria para tu funciÃ³n, incrementando la asignaciÃ³n a medida que lo necesites.
- **Tiempo de EjecuciÃ³n**: TambiÃ©n conocido como "timeout", puedes configurarlo hasta un mÃ¡ximo de 15 minutos, adaptÃ¡ndolo a los requerimientos de tu proceso.

### Â¿QuÃ© consideraciones debemos tomar respecto a la red?

Es posible desplegar funciones Lambda dentro de una VPC, definiendo la subred y el grupo de seguridad. Esto permite controlar de manera precisa el entorno de red en el que se ejecuta tu funciÃ³n.

### Â¿QuÃ© significa "dead letter queue"?

Las "dead letter queues" son herramientas cruciale que aseguran la no pÃ©rdida de eventos crÃ­ticos en la gestiÃ³n de Big Data. En el caso de fallos o errores regulares en la ejecuciÃ³n de funciones, los mensajes problemÃ¡ticos pueden ser redirigidos a una cola secundaria para posterior revisiÃ³n y procesado.

### Â¿Por quÃ© es importante habilitar el servicio de X-Ray?

Activar X-Ray permite el seguimiento detallado de la ejecuciÃ³n de funciones Lambda. Esto es vital para identificar cuellos de botella y tiempos de retardo, proporcionando un anÃ¡lisis en profundidad del rendimiento de tus aplicaciones en la nube.

### Â¿CuÃ¡les son otros aspectos esenciales en la configuraciÃ³n de Lambdas para Big Data?

Finalmente, algunos aspectos mÃ¡s avanzados permiten una mayor eficiencia y seguimiento en ejecuciÃ³n de funciones Lambda.

### Â¿CÃ³mo configuramos la concurrencia?

Lambda permite una concurrencia predeterminada de 1000 instancias simultÃ¡neas. Es posible aumentar esta cantidad hasta 20,000 mediante una solicitud a AWS. Reserva concurrencia para asegurarte de que tus funciones crÃ­ticas siempre tengan recursos disponibles cuando los necesiten.

### Â¿CÃ³mo gestionamos la monitorizaciÃ³n y eventos?

Es esencial registrar todas las ejecuciones mediante CloudWatch Logs para asegurarte de capturar mÃ©tricas y eventos que pueden ser crÃ­ticos para solucionar problemas y entender el comportamiento de tu aplicaciÃ³n.

### Â¿QuÃ© roles tienen Glue, EMR y Lambdas en la transformaciÃ³n de Big Data?

En proyectos de Big Data, diferentes servicios juegan roles complementarios:

- **Glue**: Servicio ETL completamente administrado y serverless.
- **EMR (Elastic MapReduce)**: Permite la transformaciÃ³n y anÃ¡lisis de datos utilizando clÃºsteres administrados.
- **Lambdas**: Ofrecen flexibilidad para proyectos en tiempo real y batch, permitiendo la transformaciÃ³n de informaciÃ³n sin necesidad de gestionar servidores.

Te animo a continuar explorando estas capacidades en tus proyectos, aprovechando al mÃ¡ximo las herramientas y configuraciones disponibles en AWS para transformar el Big Data de manera eficiente y segura.



docker run -p 8080:8080 --rm --name zeppelin apache/zeppelin:0.12.0