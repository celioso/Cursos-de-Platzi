# Curso de Big Data en AWS

## Iniciando con Big Data

Big Data es un campo orientado al anÃ¡lisis, procesamiento y almacenamiento de grandes cantidades de informaciÃ³n que permite mejorar el valor de tu negocio. Utilizando la informaciÃ³n mediante este esquema podemos detectar puntos de optimizaciÃ³n en diferentes Ã¡reas.

Actualmente encontramos diferentes proveedores de Cloud computing con soporte de Big Data, compitiendo entre sÃ­ por atraer la mayor cantidad de clientes a sus nubes, destacando Amazon Web Services por sus mÃºltiples servicios para manejo de grandes cantidades de informaciÃ³n.

En este curso aprenderÃ¡s:

- Â¿CÃ³mo tomar data desde el origen para llevarla a Cloud?
- Â¿CÃ³mo transformar la data?
- Â¿CÃ³mo visualizar la data?
- Â¿CÃ³mo proteger los datos?

Tomando en cuenta tres aspectos importantes:

1. Automatizar todos los procesos.
2. Orquestar las distintas tareas.
3. Involucrar un buen nivel de seguridad sobre nuestros datos.

Antes de comenzar este curso te sugerimos tomar previamente los cursos de :

[Amazon Web Services](https://platzi.com/aws/)
[Cloud Computing](https://platzi.com/clases/aws-computo/)
[Storage en AWS](https://platzi.com/clases/storage-aws/)
[Base de datos en AWS](https://platzi.com/clases/db-aws/)

Â¡Nunca pares de aprender!

## Cloud Computing en proyectos de BigData

El **Cloud Computing** ha revolucionado la manera en que se gestionan y procesan grandes volÃºmenes de datos en proyectos de **Big Data**. La escalabilidad, flexibilidad y el pago por uso de la nube permiten a las empresas analizar grandes cantidades de informaciÃ³n sin necesidad de invertir en infraestructura costosa.

### **ğŸš€ Beneficios del Cloud Computing en Big Data**  

1ï¸âƒ£ **Escalabilidad**  
   - Permite ajustar recursos (CPU, memoria, almacenamiento) de forma dinÃ¡mica segÃºn la demanda.  
   - Ejemplo: **AWS EMR, Google Dataproc, Azure HDInsight**.  

2ï¸âƒ£ **Procesamiento Distribuido**  
   - Uso de tecnologÃ­as como **Apache Hadoop, Apache Spark** para procesamiento masivo de datos en paralelo.  
   - Ejemplo: AWS Glue, Databricks en Azure.  

3ï¸âƒ£ **Almacenamiento Flexible y EconÃ³mico**  
   - Bases de datos optimizadas para Big Data como **Amazon S3, Google Cloud Storage, Azure Blob Storage**.  
   - Bases de datos NoSQL: **Amazon DynamoDB, Google Bigtable, MongoDB Atlas**.  

4ï¸âƒ£ **Bajo Costo**  
   - Pago por uso evita inversiones en hardware.  
   - Uso de instancias spot/preemptibles para reducir costos.  

5ï¸âƒ£ **Seguridad y Cumplimiento**  
   - Cifrado, control de acceso y auditorÃ­as en tiempo real.  
   - Normativas como **GDPR, HIPAA, SOC2**.  

6ï¸âƒ£ **IntegraciÃ³n con Machine Learning e IA**  
   - Servicios como **AWS SageMaker, Google Vertex AI, Azure ML** para entrenar modelos con grandes volÃºmenes de datos.

### **ğŸ”§ Herramientas Cloud para Big Data**  

âœ… **Almacenamiento:**  
   - **Amazon S3, Google Cloud Storage, Azure Data Lake**  

âœ… **Procesamiento de datos:**  
   - **Amazon EMR (Hadoop/Spark), Google Dataflow (Apache Beam), Azure HDInsight**  

âœ… **Bases de datos:**  
   - **Amazon Redshift, BigQuery, Snowflake, Azure Synapse Analytics**  

âœ… **Streaming de datos:**  
   - **Kafka en AWS MSK, Google Pub/Sub, Azure Event Hubs**  

âœ… **VisualizaciÃ³n de datos:**  
   - **Amazon QuickSight, Google Looker, Power BI, Tableau** 

### **ğŸ“Œ Casos de uso en Big Data con Cloud Computing**  

ğŸ”¹ **AnÃ¡lisis de redes sociales** ğŸ“Š â†’ Procesamiento en tiempo real de tweets con **Kafka + Spark en AWS**.  
ğŸ”¹ **Recomendaciones de productos** ğŸ›’ â†’ Uso de **BigQuery + TensorFlow en GCP** para anÃ¡lisis de comportamiento.  
ğŸ”¹ **DetecciÃ³n de fraudes financieros** ğŸ’³ â†’ Procesamiento en **Azure Synapse + ML** para detectar transacciones sospechosas.  
ğŸ”¹ **Salud y genÃ³mica** ğŸ§¬ â†’ AnÃ¡lisis de grandes volÃºmenes de datos genÃ©ticos con **AWS Lambda + S3 + SageMaker**.

### **ğŸš€ ConclusiÃ³n**  
El **Cloud Computing** permite manejar proyectos de **Big Data** de manera eficiente, escalable y econÃ³mica. Su integraciÃ³n con herramientas de **IA y Machine Learning** facilita la creaciÃ³n de soluciones avanzadas para distintos sectores.

**Resumen**

En este curso impartido por Carlos Zambrano, profesor con gran experiencia en el sector financiero y de transformaciÃ³n de data, vas a aprender a:

- CÃ³mo tomar data desde el origen para llevarla a Cloud.
- CÃ³mo transformar la data.
- CÃ³mo visualizar la data.

Tomando en cuenta tres aspectos importantes:

1. Automatizar todos los procesos.
2. Orquestar las distintas tareas.
3. Involucrar un buen nivel de seguridad sobre nuestros datos.

**Lecturas recomendadas**

[Map AWS services to Google Cloud Platform products  |  Google Cloud Platform Free Tier  |  Google Cloud](https://cloud.google.com/free/docs/map-aws-google-cloud-platform)

[Â¿QuÃ© son los big data? â€“ Amazon Web Services (AWS)](https://aws.amazon.com/es/big-data/what-is-big-data/)

[Big Data Analytics Solutions  |  Google Cloud](https://cloud.google.com/solutions/big-data/)

[Big Data and Advanced Analytics Solutions | Microsoft Azure](https://azure.microsoft.com/en-us/solutions/big-data/)

## IntroducciÃ³n al manejo de datos en Cloud

El **manejo de datos en la nube (Cloud Data Management)** es una estrategia clave para empresas y organizaciones que desean almacenar, procesar y analizar datos de manera eficiente y escalable. La computaciÃ³n en la nube ha transformado la forma en que los datos son gestionados, eliminando las limitaciones de la infraestructura local y ofreciendo soluciones flexibles y de pago por uso.

### **ğŸ”¹ Â¿QuÃ© es el Manejo de Datos en la Nube?**  

Es el conjunto de tÃ©cnicas y herramientas utilizadas para **almacenar, procesar, proteger y analizar datos** en plataformas de nube como **AWS, Google Cloud y Microsoft Azure**.  

Los datos pueden almacenarse en diferentes formatos y tipos de almacenamiento, dependiendo de su estructura y finalidad:  

âœ… **Estructurados** â†’ Bases de datos relacionales (SQL).  
âœ… **No estructurados** â†’ Archivos multimedia, documentos, correos electrÃ³nicos.  
âœ… **Semiestructurados** â†’ JSON, XML, logs de servidores.

### **ğŸš€ Beneficios del Manejo de Datos en la Nube**  

1ï¸âƒ£ **Escalabilidad**  
   - Capacidad de aumentar o reducir los recursos segÃºn la demanda.  
   - Ejemplo: **Google BigQuery escala automÃ¡ticamente** segÃºn las consultas.  

2ï¸âƒ£ **Costo-Eficiencia**  
   - Pago por uso, evitando inversiones en hardware.  
   - Opciones de almacenamiento econÃ³mico como **Amazon S3 Glacier** para datos archivados.  

3ï¸âƒ£ **Alta Disponibilidad y Resiliencia**  
   - ReplicaciÃ³n de datos en mÃºltiples regiones.  
   - Ejemplo: **Azure Storage** replica datos en diferentes ubicaciones.  

4ï¸âƒ£ **Seguridad y Cumplimiento**  
   - Cifrado de datos en trÃ¡nsito y en reposo.  
   - Cumplimiento con normativas como **GDPR, HIPAA, SOC2**.  

5ï¸âƒ£ **Accesibilidad Global**  
   - Datos accesibles desde cualquier parte del mundo con baja latencia.  
   - IntegraciÃ³n con **APIs y herramientas de analÃ­tica avanzada**.

### **ğŸ”§ Principales Servicios Cloud para el Manejo de Datos**  

ğŸ“Œ **Almacenamiento**  
   - **Amazon S3, Google Cloud Storage, Azure Blob Storage** â†’ Archivos y datos no estructurados.  
   - **Amazon EBS, Google Persistent Disk, Azure Managed Disks** â†’ Almacenamiento para mÃ¡quinas virtuales.  

ğŸ“Œ **Bases de Datos**  
   - **Relacionales:** Amazon RDS, Cloud SQL, Azure SQL Database.  
   - **NoSQL:** Amazon DynamoDB, Google Firestore, Azure Cosmos DB.  

ğŸ“Œ **Procesamiento de Datos**  
   - **Batch:** AWS Glue, Google Dataflow, Azure Data Factory.  
   - **Streaming:** AWS Kinesis, Google Pub/Sub, Azure Event Hubs.  

ğŸ“Œ **AnÃ¡lisis y VisualizaciÃ³n**  
   - **BigQuery (GCP), Amazon Redshift, Azure Synapse** â†’ AnalÃ­tica de datos a gran escala.  
   - **Amazon QuickSight, Google Looker, Power BI** â†’ Dashboards e informes.

### **ğŸ“Š Casos de Uso en la Nube**  

ğŸ”¹ **Empresas de Retail** ğŸ›’ â†’ Uso de **BigQuery** para anÃ¡lisis de tendencias de compra.  
ğŸ”¹ **Finanzas** ğŸ’³ â†’ **DynamoDB + SageMaker** para detecciÃ³n de fraudes en AWS.  
ğŸ”¹ **Salud** ğŸ¥ â†’ **FHIR en Google Cloud** para gestionar historiales clÃ­nicos.  
ğŸ”¹ **Streaming y Entretenimiento** ğŸ¬ â†’ Uso de **Azure Media Services** para distribuciÃ³n de contenido.

### **ğŸŒŸ ConclusiÃ³n**  
El manejo de datos en la nube permite a las organizaciones ser mÃ¡s Ã¡giles, reducir costos y aprovechar el poder del **Big Data y la Inteligencia Artificial**. La combinaciÃ³n de almacenamiento escalable, seguridad robusta y herramientas avanzadas de anÃ¡lisis hacen de la nube la mejor opciÃ³n para gestionar datos en la era digital.

### Resumen

Algunos puntos para tomar en cuenta al momento de iniciar en el manejo de datos en la nube, sin importar quÃ© servicio utilices, son:

- Cuando trabajas en la nube puedes tener un **crecimiento completamente escalable**, iniciando desde MB hasta EB.
- A medida que tu aplicaciÃ³n crezca puedes ir escalando el procesamiento de datos en la nube.
- En Cloud tienes acceso a un **gran nivel de eficiencia** a un bajo costo, solamente te van a cobrar mientras utilices las herramientas.
- Existen muchos servicios de procesamiento en la nube, escoge el que mÃ¡s se acomode a tus necesidades.

## Datos en Cloud

El almacenamiento y gestiÃ³n de datos en la nube ha revolucionado la forma en que las empresas y organizaciones manejan la informaciÃ³n. Con la llegada de tecnologÃ­as de **Cloud Computing**, los datos ahora pueden ser almacenados, procesados y analizados con mayor flexibilidad, escalabilidad y seguridad.

### **ğŸ“Œ Â¿QuÃ© son los Datos en la Nube?**  

Los **Datos en la Nube** son toda la informaciÃ³n que se almacena y procesa en servidores remotos gestionados por proveedores de servicios en la nube como **Amazon Web Services (AWS), Google Cloud Platform (GCP) y Microsoft Azure**. Estos datos pueden incluir:  

âœ… **Datos estructurados** â†’ Bases de datos relacionales (SQL).  
âœ… **Datos semiestructurados** â†’ JSON, XML, logs de servidores.  
âœ… **Datos no estructurados** â†’ Archivos multimedia, correos electrÃ³nicos, documentos.  

### **ğŸš€ Beneficios de Almacenar Datos en la Nube**  

ğŸ”¹ **Escalabilidad**: Se ajusta automÃ¡ticamente segÃºn la demanda.  
ğŸ”¹ **ReducciÃ³n de costos**: Pago por uso, eliminando infraestructura fÃ­sica.  
ğŸ”¹ **Accesibilidad global**: Disponibilidad en cualquier parte del mundo.  
ğŸ”¹ **Seguridad avanzada**: Cifrado, autenticaciÃ³n y cumplimiento normativo.  
ğŸ”¹ **Alta disponibilidad y recuperaciÃ³n ante desastres**: ReplicaciÃ³n en mÃºltiples regiones.

### **ğŸ”§ Tipos de Almacenamiento de Datos en la Nube**  

### **1ï¸âƒ£ Almacenamiento de Archivos**  
ğŸ“Œ **Ejemplos:**  
- **Amazon S3** (AWS)  
- **Google Cloud Storage** (GCP)  
- **Azure Blob Storage** (Microsoft)  
ğŸ“Œ **Usos:**  
- Almacenamiento de imÃ¡genes, videos y documentos.  
- Backup y recuperaciÃ³n de datos.  

### **2ï¸âƒ£ Bases de Datos Relacionales (SQL)**  
ğŸ“Œ **Ejemplos:**  
- **Amazon RDS** (AWS)  
- **Cloud SQL** (GCP)  
- **Azure SQL Database** (Microsoft)  
ğŸ“Œ **Usos:**  
- Aplicaciones empresariales y de e-commerce.  
- Sistemas de gestiÃ³n de clientes (CRM).  

### **3ï¸âƒ£ Bases de Datos NoSQL**  
ğŸ“Œ **Ejemplos:**  
- **Amazon DynamoDB** (AWS)  
- **Google Firestore** (GCP)  
- **Azure Cosmos DB** (Microsoft)  
ğŸ“Œ **Usos:**  
- Aplicaciones con datos de rÃ¡pida lectura/escritura.  
- Almacenamiento de sesiones web y perfiles de usuario.  

### **4ï¸âƒ£ Almacenamiento de Datos para Big Data**  
ğŸ“Œ **Ejemplos:**  
- **Amazon Redshift** (AWS)  
- **Google BigQuery** (GCP)  
- **Azure Synapse Analytics** (Microsoft)  
ğŸ“Œ **Usos:**  
- Procesamiento y anÃ¡lisis de grandes volÃºmenes de datos.  
- Machine Learning y anÃ¡lisis predictivo. 

### **ğŸ“Š Casos de Uso de Datos en la Nube**  

ğŸ¥ **Salud:** Historias clÃ­nicas en la nube con **FHIR en Google Cloud**.  
ğŸ¦ **Finanzas:** AnÃ¡lisis de fraude en **AWS usando Machine Learning**.  
ğŸ›ï¸ **E-commerce:** PersonalizaciÃ³n de productos con **BigQuery en Google Cloud**.  
ğŸ¬ **Entretenimiento:** Streaming de contenido con **Azure Media Services**. 

### **ğŸŒŸ ConclusiÃ³n**  
El almacenamiento y manejo de **datos en la nube** permite a las organizaciones optimizar costos, mejorar la seguridad y potenciar el anÃ¡lisis de datos. Con opciones de almacenamiento escalables y herramientas avanzadas, la nube es la mejor opciÃ³n para gestionar datos en la era digital.

### Resumen

Hay algunos puntos importantes que debemos tener en cuenta al momento de manejar nuestra data en un servicio en la nube:

- Debemos seleccionar el servicio que mejor se ajuste a nuestras necesidades de almacenamiento.
- Lo primero que debemos hacer es extraer de otras fuentes la informaciÃ³n que vamos a necesitar.
- Debemos validar nuestra informaciÃ³n, verificar que sea consistente.
- Verificar los tipos de datos que vamos a extraer.
- Al momento de realizar pruebas a nuestra informaciÃ³n debemos utilizar un subset de la data.

## Â¿QuÃ© nube deberÃ­a utilizar en mi proyecto de Big Data?

Actualmente el mercado de Cloud Computing tiene varios actores compitiendo entre sÃ­ por atraer la mayor cantidad de clientes a sus nubes, encontramos MÃºltiples opciones como: Amazon Web Services, Azure, Alibaba Cloud, Google Cloud Platform, Oracle Cloud, Rackspace, Digital Ocean y Softlayer entre muchas otras.

Dentro de esta variedad de proveedores muchas veces es complejo tomar decisiones de cuÃ¡l utilizar, el criterio para esta decisiÃ³n puede estar dado por diferentes factores como:

1. **Costo**: Valor de los servicios que serÃ¡n utilizados en el proyecto.
2. **Tipo de pricing**: Por demanda (por hora, minuto o segundo), subasta, reservado.
3. **Servicios**: Variedad de servicios provistos por el cloud provider. Â¿CuÃ¡l servicio se ajusta mejor a mis necesidades?
4. **UbicaciÃ³n**: DistribuciÃ³n de las regiones/zonas donde el cloud provider preste servicios por temas de latencia y experiencia usuario esto puede ser decisivo.
5. **Niveles de Servicio**: Consultar la documentaciÃ³n por servicio y los niveles ofrecidos de disponibilidad.
6. **Soporte**: Tipos de soporte, costo, tiempos de respuesta y nivel de soporte (basic, business, enterprise).
7. **Estudios de mercado**: Revisar los diferentes estudios de mercado, por ejemplo: el cuadrante mÃ¡gico de Gartner, en los cuales se evalÃºan en diferentes aspectos los servicios provistos.
8. **DocumentaciÃ³n**: Consultar la documentaciÃ³n de los cloud provider, muchas veces no es muy clara o estÃ¡ incompleta referente a sus servicios.

![Infografia big data AWS v3 curso](images/Infografia-big-data-AWS-v3-curso.png)

DespuÃ©s de revisar las diferentes opciones que proveen los cloud providers encontramos variedad en servicios de acuerdo a su funcionalidad, otras nubes como Azure, Softlayer, Alibaba tambiÃ©n cuentan con servicios orientados al procesamiento de datos, sin embargo dentro de su ecosistema no es tan completo el set de servicios, por tal motivo siempre que pensemos en proyectos de BigData los mejores cloud provider serÃ¡n AWS y GCP que estudiaras en este curso.

## Arquitecturas Lambda

Las **arquitecturas Lambda** son un enfoque de diseÃ±o en **computaciÃ³n en la nube** que permite el procesamiento de datos en tiempo real con alta escalabilidad y eficiencia. Son ampliamente utilizadas en aplicaciones de **Big Data, anÃ¡lisis en tiempo real e Internet de las Cosas (IoT)**.

### **ğŸ“Œ Â¿QuÃ© es una Arquitectura Lambda?**  

Una **Arquitectura Lambda** es un modelo diseÃ±ado para **procesar y analizar grandes volÃºmenes de datos en tiempo real y en lotes** de manera eficiente. Se basa en dos capas principales:  

1ï¸âƒ£ **Capa de Velocidad (Speed Layer)** â†’ Procesa datos en tiempo real.  
2ï¸âƒ£ **Capa de Batch (Batch Layer)** â†’ Procesa datos histÃ³ricos o en lotes.  
3ï¸âƒ£ **Capa de Servicio (Serving Layer)** â†’ Combina los resultados de ambas capas y los expone a aplicaciones.  

ğŸ’¡ **Este enfoque combina lo mejor de los sistemas de procesamiento en tiempo real y en batch, garantizando baja latencia y datos precisos.**  


### **ğŸš€ Componentes de una Arquitectura Lambda**  

### **1ï¸âƒ£ Capa de Ingesta de Datos**  
ğŸ“Œ **Recibe y almacena datos desde mÃºltiples fuentes.**  
ğŸ”¹ **Ejemplos:**  
- Amazon Kinesis (AWS)  
- Google Pub/Sub (GCP)  
- Azure Event Hub (Microsoft)  

### **2ï¸âƒ£ Capa de Procesamiento en Batch**  
ğŸ“Œ **Procesa grandes volÃºmenes de datos con alta precisiÃ³n.**  
ğŸ”¹ **Ejemplos:**  
- Apache Hadoop  
- AWS Glue  
- Google Dataflow  

### **3ï¸âƒ£ Capa de Procesamiento en Tiempo Real (Stream Processing)**  
ğŸ“Œ **Analiza datos con baja latencia en tiempo real.**  
ğŸ”¹ **Ejemplos:**  
- Apache Kafka  
- AWS Lambda  
- Azure Stream Analytics  

### **4ï¸âƒ£ Capa de Almacenamiento**  
ğŸ“Œ **Guarda datos para anÃ¡lisis y consultas futuras.**  
ğŸ”¹ **Ejemplos:**  
- Amazon S3 (AWS)  
- Google BigQuery (GCP)  
- Azure Data Lake (Microsoft)  

### **5ï¸âƒ£ Capa de Servicio (Serving Layer)**  
ğŸ“Œ **Proporciona acceso a los datos para dashboards y aplicaciones.**  
ğŸ”¹ **Ejemplos:**  
- Amazon Redshift  
- Google Looker Studio  
- Azure Synapse Analytics

### **ğŸ› ï¸ Caso de Uso: Procesamiento de Datos en Streaming con AWS Lambda**  

ğŸ”¹ **Escenario:** Una empresa de e-commerce quiere analizar el comportamiento de los usuarios en tiempo real.  

ğŸ”¹ **SoluciÃ³n con Arquitectura Lambda en AWS:**  
1ï¸âƒ£ **AWS Kinesis** captura eventos de navegaciÃ³n web.  
2ï¸âƒ£ **AWS Lambda** procesa los eventos en tiempo real.  
3ï¸âƒ£ **Amazon DynamoDB** almacena la informaciÃ³n procesada.  
4ï¸âƒ£ **Amazon QuickSight** genera reportes y visualizaciones.

### **ğŸ”„ Diferencia entre Arquitectura Lambda y Kappa**  

| CaracterÃ­stica  | **Lambda**  | **Kappa**  |
|---------------|------------|------------|
| Procesamiento | Batch + Streaming | Solo Streaming |
| Complejidad  | Alta (doble pipeline) | Baja (un solo pipeline) |
| Uso comÃºn    | Big Data, IoT | Machine Learning, IoT |

ğŸ’¡ **Si tu aplicaciÃ³n requiere procesamiento en tiempo real y en batch, usa Lambda. Si solo necesitas datos en streaming, Kappa es mejor.**

### **ğŸŒŸ Beneficios de las Arquitecturas Lambda**  

âœ… **Escalabilidad automÃ¡tica** â†’ Se adapta a grandes volÃºmenes de datos.  
âœ… **Baja latencia** â†’ Procesa informaciÃ³n en tiempo real.  
âœ… **Flexibilidad** â†’ Compatible con mÃºltiples tecnologÃ­as.  
âœ… **Alta disponibilidad** â†’ ReplicaciÃ³n y tolerancia a fallos.  

### **ğŸ” ConclusiÃ³n**  
Las **Arquitecturas Lambda** son ideales para manejar **Big Data, anÃ¡lisis en tiempo real e IoT**. Su combinaciÃ³n de procesamiento batch y en streaming las hace versÃ¡tiles y poderosas en la era digital.

**Resumen**

La arquitectura Lambda es atribuida a Nathan Marz, diseÃ±ada para ser escalable, tolerante a fallos y de alto procesamiento de datos.

Tiene una gran robustez, puede procesar una alta cantidad de datos. EstÃ¡ compuesta por tres capas:

1. **Batch**: En esta capa vamos a procesar toda la informaciÃ³n almacenada con anterioridad, desde el dÃ­a anterior hasta meses.
2. **Serve**: Dentro de esta capa es posible visualizar la data procesada de la capa batch.
3. **Speed**: Conforme llega la data se va a ir procesando.

## Arquitectura Kappa

La **Arquitectura Kappa** es un enfoque de diseÃ±o para el procesamiento de datos **en tiempo real**, eliminando la necesidad de procesar datos en batch. Es ideal para aplicaciones de **Big Data, Machine Learning, IoT y anÃ¡lisis en streaming**.

### **ğŸ“Œ Â¿QuÃ© es una Arquitectura Kappa?**  

Es un modelo que **procesa datos exclusivamente en tiempo real** mediante **event streaming**. A diferencia de la **Arquitectura Lambda**, **no tiene una capa de batch**, lo que simplifica la infraestructura y reduce la latencia.  

ğŸ’¡ **Se basa en un Ãºnico pipeline de datos en streaming.**

### **ğŸš€ Componentes de una Arquitectura Kappa**  

### **1ï¸âƒ£ Capa de Ingesta de Datos**  
ğŸ“Œ **Recibe datos en tiempo real desde mÃºltiples fuentes.**  
ğŸ”¹ **Ejemplos:**  
- Apache Kafka  
- Amazon Kinesis  
- Google Pub/Sub  
- Azure Event Hub  

### **2ï¸âƒ£ Capa de Procesamiento en Streaming**  
ğŸ“Œ **Transforma y analiza datos en tiempo real.**  
ğŸ”¹ **Ejemplos:**  
- Apache Flink  
- Apache Spark Streaming  
- AWS Lambda  
- Google Dataflow  

### **3ï¸âƒ£ Capa de Almacenamiento**  
ğŸ“Œ **Guarda datos estructurados y no estructurados para consultas futuras.**  
ğŸ”¹ **Ejemplos:**  
- Amazon S3 (AWS)  
- Google BigQuery (GCP)  
- Azure Data Lake (Microsoft)  

### **4ï¸âƒ£ Capa de Consumo y VisualizaciÃ³n**  
ğŸ“Œ **Permite a los usuarios acceder a los datos procesados.**  
ğŸ”¹ **Ejemplos:**  
- Amazon QuickSight  
- Google Looker Studio  
- Power BI

### **ğŸ”„ Diferencia entre Arquitectura Kappa y Lambda**  

| CaracterÃ­stica  | **Lambda**  | **Kappa**  |
|---------------|------------|------------|
| Procesamiento | Batch + Streaming | Solo Streaming |
| Complejidad  | Alta (doble pipeline) | Baja (un solo pipeline) |
| Latencia     | Baja, pero con batch | MÃ­nima |
| Uso comÃºn    | Big Data, IoT | IoT, ML, Finanzas |

ğŸ’¡ **Si necesitas procesamiento en batch y en tiempo real, usa Lambda. Si solo necesitas tiempo real, Kappa es mejor.**

### **ğŸ› ï¸ Caso de Uso: AnÃ¡lisis de Sensores IoT con Kappa**  

ğŸ”¹ **Escenario:** Una empresa de manufactura quiere monitorear la temperatura de sus mÃ¡quinas en tiempo real.  

ğŸ”¹ **SoluciÃ³n con Arquitectura Kappa:**  
1ï¸âƒ£ **Kafka o Kinesis** recibe datos de sensores en streaming.  
2ï¸âƒ£ **Apache Flink** procesa y detecta anomalÃ­as en tiempo real.  
3ï¸âƒ£ **Amazon DynamoDB** almacena la informaciÃ³n relevante.  
4ï¸âƒ£ **Amazon QuickSight** visualiza datos en tiempo real.

### **ğŸŒŸ Beneficios de la Arquitectura Kappa**  

âœ… **Menos complejidad** â†’ Un solo pipeline de datos.  
âœ… **Menor latencia** â†’ Respuesta en tiempo real.  
âœ… **Mayor escalabilidad** â†’ Perfecto para datos de alto volumen.  
âœ… **Ideal para Machine Learning** â†’ Modelos entrenados en tiempo real.

### **ğŸ” ConclusiÃ³n**  
La **Arquitectura Kappa** es ideal para sistemas que requieren **procesamiento en tiempo real**, como **IoT, finanzas, seguridad y ML**. Si buscas baja latencia y simplicidad, Kappa es una excelente opciÃ³n. ğŸš€

### Resumen

Fue presentada por Jay Krepsen en el 2014 como una evoluciÃ³n de la arquitectura lambda. Elimina la capa batch haciendo que todo se procese en tiempo real.

La arquitectura Kappa sigue los siguientes pilares:

1. Todo es un stream.
2. InformaciÃ³n de origen no modificada.
3. Solo un flujo de procesamiento.
4. Capaz de reprocesar.

## Arquitectura Batch

La **Arquitectura Batch** es un enfoque tradicional para el procesamiento de datos en grandes volÃºmenes. Se basa en la ejecuciÃ³n de tareas en **lotes** o **bloques** de datos en un periodo determinado, en lugar de procesar cada evento individualmente en tiempo real.

### **ğŸ“Œ Â¿QuÃ© es una Arquitectura Batch?**  
Es un modelo de procesamiento de datos donde los datos se recopilan, almacenan y luego se procesan en **bloques o lotes** en un intervalo especÃ­fico.  

ğŸ’¡ **Ejemplo:** Un banco genera un informe de todas las transacciones realizadas en el dÃ­a y lo procesa cada noche.

### **ğŸš€ Componentes de una Arquitectura Batch**  

### **1ï¸âƒ£ Capa de Ingesta de Datos**  
ğŸ“Œ **Recopila datos de diversas fuentes y los almacena para su posterior procesamiento.**  
ğŸ”¹ **Ejemplos:**  
- Amazon S3  
- Google Cloud Storage  
- Azure Data Lake  

### **2ï¸âƒ£ Capa de Procesamiento Batch**  
ğŸ“Œ **Ejecuta trabajos en lotes periÃ³dicamente.**  
ğŸ”¹ **Ejemplos:**  
- Apache Hadoop  
- AWS Glue  
- Google Dataflow  
- Azure Data Factory  

### **3ï¸âƒ£ Capa de Almacenamiento**  
ğŸ“Œ **Guarda los datos procesados para su consulta y anÃ¡lisis.**  
ğŸ”¹ **Ejemplos:**  
- Amazon Redshift  
- Google BigQuery  
- Azure Synapse Analytics  

### **4ï¸âƒ£ Capa de Consumo y VisualizaciÃ³n**  
ğŸ“Œ **Proporciona acceso a los datos procesados.**  
ğŸ”¹ **Ejemplos:**  
- Tableau  
- Power BI  
- Amazon QuickSight

### **ğŸ”„ Â¿CÃ³mo funciona una Arquitectura Batch?**  

1ï¸âƒ£ **Se recopilan los datos** en intervalos especÃ­ficos.  
2ï¸âƒ£ **Se almacenan en un sistema de archivos o base de datos.**  
3ï¸âƒ£ **Se procesan en lotes** mediante herramientas como Apache Spark o AWS Glue.  
4ï¸âƒ£ **Se almacenan los resultados** en un Data Warehouse.  
5ï¸âƒ£ **Se consumen los datos** mediante dashboards o informes.

### **ğŸ”¹ Casos de Uso de la Arquitectura Batch**  

âœ… **Procesamiento de grandes volÃºmenes de datos histÃ³ricos.**  
âœ… **GeneraciÃ³n de reportes y anÃ¡lisis de tendencias.**  
âœ… **Carga de datos en Data Warehouses.**  
âœ… **Procesamiento de facturaciÃ³n y pagos en empresas.** 

### **âš–ï¸ Diferencia entre Batch y Streaming**  

| CaracterÃ­stica  | **Batch**  | **Streaming**  |
|---------------|------------|---------------|
| Procesamiento | Lotes de datos | Evento por evento |
| Latencia     | Alta (horas/dÃ­as) | Baja (segundos) |
| Complejidad  | Baja | Media/Alta |
| Uso comÃºn    | Reportes, anÃ¡lisis histÃ³ricos | IoT, Finanzas en tiempo real |

ğŸ’¡ **Si necesitas procesamiento periÃ³dico de grandes volÃºmenes de datos, Batch es ideal. Si necesitas datos en tiempo real, usa Streaming.**

### **ğŸŒŸ Beneficios de la Arquitectura Batch**  

âœ… **Alta eficiencia en grandes volÃºmenes de datos.**  
âœ… **OptimizaciÃ³n de recursos, ya que no requiere procesamiento continuo.**  
âœ… **Facilidad de mantenimiento y escalabilidad.**  
âœ… **Menor costo en comparaciÃ³n con arquitecturas en tiempo real.** 

### **ğŸ” ConclusiÃ³n**  
La **Arquitectura Batch** es ideal cuando se necesita procesar grandes volÃºmenes de datos de manera eficiente sin la necesidad de respuestas en tiempo real. Se usa en **anÃ¡lisis de datos histÃ³ricos, reportes y procesos de negocio periÃ³dicos**. ğŸš€

**Resumen**

Esta arquitectura es una parte especÃ­fica de la arquitectura Lambda.

Es importante comprender que en esta arquitectura vamos a tener un Input, un proceso y un Output, estos tres orquestados por un sistema donde el Input y Output deben de conectarse a una base de datos.

## Llevar tu informaciÃ³n al cloud

Migrar tu informaciÃ³n a la nube (Cloud) implica trasladar datos, aplicaciones y cargas de trabajo desde infraestructuras locales a servicios en la nube proporcionados por empresas como AWS, Google Cloud o Azure.  

### **Ventajas de mover tu informaciÃ³n a la nube**  
âœ… **Escalabilidad**: Puedes aumentar o reducir recursos segÃºn la demanda.  
âœ… **Alta disponibilidad**: Accede a tus datos desde cualquier lugar con conexiÃ³n a internet.  
âœ… **Seguridad**: La mayorÃ­a de los proveedores ofrecen cifrado y medidas de seguridad avanzadas.  
âœ… **ReducciÃ³n de costos**: Evita gastos en hardware y mantenimiento.  

### **Tipos de migraciÃ³n a la nube**  
â˜ **Lift and Shift**: Mover los datos y aplicaciones sin modificaciones.  
â˜ **Replataforming**: Adaptar aplicaciones para aprovechar mejor los servicios en la nube.  
â˜ **Refactoring**: RediseÃ±ar aplicaciones para optimizar su desempeÃ±o en la nube.  

### **Servicios clave en la nube para almacenar informaciÃ³n**  
ğŸ“Œ **Amazon S3**: Almacenamiento escalable y seguro.  
ğŸ“Œ **Google Cloud Storage**: AlmacÃ©n de objetos para cargas de trabajo empresariales.  
ğŸ“Œ **Azure Blob Storage**: SoluciÃ³n de almacenamiento de Microsoft.  
ğŸ“Œ **Bases de datos en la nube**: DynamoDB, Aurora, BigQuery, Firestore, entre otras.  

Si quieres mÃ¡s detalles sobre cÃ³mo migrar a la nube, dime quÃ© tipo de datos o cargas de trabajo deseas mover. ğŸš€

### Resumen

Existen distintas formas de conectar tu sistema en la nube con la fuente de origen de datos que necesitas, algunas opciones son:

- Utilizar el SDK de la nube utilizando el lenguaje de programaciÃ³n de tu preferencia.
- Usar el CLI para conectarse a la nube.
- Utilizar servicios especializados para la tarea.

Es posible combinar esas opciones para llevar la informaciÃ³n que necesites a tu cloud provider.

## Demo - Creando nuestro IDE en la nube con Python - Boto3

Para crear un **IDE en la nube con Python** utilizando **Boto3** (el SDK de AWS para Python), podemos aprovechar servicios de AWS como **EC2, S3, Cloud9 y Lambda**. AquÃ­ te dejo una guÃ­a paso a paso:

### **1ï¸âƒ£ Configurar Boto3 en tu entorno local**  
Antes de comenzar, instala **Boto3** y configura las credenciales de AWS en tu mÃ¡quina local:  

```bash
pip install boto3
aws configure
```
Ingresa tu **AWS Access Key**, **Secret Key**, regiÃ³n y formato de salida.

### **2ï¸âƒ£ Crear una instancia EC2 para alojar el IDE**  
Podemos lanzar una instancia EC2 que sirva como nuestro entorno de desarrollo.  

```python
import boto3

ec2 = boto3.resource('ec2')

# Lanzar una instancia EC2 con Ubuntu
instance = ec2.create_instances(
    ImageId='ami-0c55b159cbfafe1f0',  # ID de la AMI (Ubuntu)
    MinCount=1,
    MaxCount=1,
    InstanceType='t2.micro',
    KeyName='mi-clave-aws',  # Debes haber creado una clave SSH en AWS
    SecurityGroups=['default'],
    UserData="""#!/bin/bash
    sudo apt update -y
    sudo apt install -y python3 python3-pip
    pip3 install jupyter boto3
    """,
    TagSpecifications=[
        {
            'ResourceType': 'instance',
            'Tags': [{'Key': 'Name', 'Value': 'IDE-Python-Cloud'}]
        }
    ]
)

print(f"Instancia EC2 creada: {instance[0].id}")
```
Esto inicia un **servidor Ubuntu con Python y Jupyter**.

### **3ï¸âƒ£ Configurar un Bucket S3 para almacenamiento de proyectos**
Podemos crear un bucket S3 para guardar archivos y proyectos.

```python
s3 = boto3.client('s3')

bucket_name = 'mi-ide-python-cloud'

s3.create_bucket(
    Bucket=bucket_name,
    CreateBucketConfiguration={'LocationConstraint': 'us-east-1'}
)

print(f"Bucket {bucket_name} creado exitosamente")
```
Subir archivos al IDE en la nube:

```python
s3.upload_file('mi_script.py', bucket_name, 'mi_script.py')
print("Archivo subido a S3")
```

### **4ï¸âƒ£ OpciÃ³n avanzada: Usar AWS Cloud9**
AWS **Cloud9** es un IDE en la nube administrado por AWS que se puede lanzar con **Boto3**.

```python
cloud9 = boto3.client('cloud9')

response = cloud9.create_environment_ec2(
    name='MiIDECloud9',
    instanceType='t2.micro',
    automaticStopTimeMinutes=30
)

print(f"IDE Cloud9 creado: {response['environmentId']}")
```
Cloud9 permite codificar directamente en un navegador sin necesidad de configurar servidores.

### **5ï¸âƒ£ Acceder al IDE**
- Si usaste EC2, puedes conectarte con SSH:
  ```bash
  ssh -i mi-clave-aws.pem ubuntu@IP_PUBLICA
  ```
- Para Jupyter Notebook, abre en tu navegador:
  ```
  http://IP_PUBLICA:8888
  ```
- Si usaste Cloud9, abre la consola de AWS y accede desde la secciÃ³n **Cloud9**.

### ğŸš€ **ConclusiÃ³n**
Hemos creado un **IDE en la nube con Python** utilizando EC2, S3 y Cloud9 con Boto3. Puedes escalarlo agregando **Docker, VSCode Server, Lambda o API Gateway** para mÃ¡s funcionalidades.

**Resumen**

Vamos a utilizar el servicio de Cloud de Amazon para este curso, especÃ­ficamente para esta demo usaremos el SDK de AWS para Python.

Python es una gran opciÃ³n para procesamiento de datos ya que cuenta con librerÃ­as como Pandas, Anaconda PyBrain, NumPy.

**Lecturas recomendadas**

[AWS Cloud9 â€“ Amazon Web Services](https://aws.amazon.com/es/cloud9/)

[Boto 3 Documentation â€” Boto 3 Docs 1.9.106 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)

[GitHub - boto/boto3: AWS SDK for Python](https://github.com/boto/boto3)

## Â¿CÃ³mo usar Boto3?

### **ğŸ“Œ Â¿QuÃ© es Boto3?**  
Boto3 es el SDK de Python para interactuar con **AWS**. Permite gestionar servicios como **S3, EC2, DynamoDB, Lambda, RDS, entre otros**.

### **1ï¸âƒ£ InstalaciÃ³n de Boto3**
Para instalarlo en tu entorno de Python, usa:
```bash
pip install boto3
```

### **2ï¸âƒ£ ConfiguraciÃ³n de credenciales**  
Antes de usar Boto3, debes configurar tus credenciales de AWS. Puedes hacerlo de dos maneras:

### **ğŸ”¹ OpciÃ³n 1: Configurar usando AWS CLI**  
Si tienes instalado AWS CLI, ejecuta:
```bash
aws configure
```
Te pedirÃ¡:
- **AWS Access Key ID**  
- **AWS Secret Access Key**  
- **RegiÃ³n por defecto** (ejemplo: `us-east-1`)  
- **Formato de salida** (`json`, `text`, etc.)

Las credenciales se guardarÃ¡n en `~/.aws/credentials` (Linux/macOS) o `C:\Users\TU_USUARIO\.aws\credentials` (Windows).

### **ğŸ”¹ OpciÃ³n 2: Configurar manualmente**
Crea el archivo `~/.aws/credentials` y aÃ±ade:
```ini
[default]
aws_access_key_id = TU_ACCESS_KEY
aws_secret_access_key = TU_SECRET_KEY
region = us-east-1
```

### **3ï¸âƒ£ Uso de Boto3**
### **ğŸ”¹ Crear una sesiÃ³n en Boto3**
```python
import boto3

# Crear una sesiÃ³n con AWS
session = boto3.Session(
    aws_access_key_id="TU_ACCESS_KEY",
    aws_secret_access_key="TU_SECRET_KEY",
    region_name="us-east-1"
)
```
Si ya configuraste las credenciales con `aws configure`, puedes omitir los parÃ¡metros.

### **4ï¸âƒ£ Ejemplos de uso de Boto3**
### **ğŸ”¹ Listar buckets en S3**
```python
s3 = boto3.client("s3")

# Obtener lista de buckets
response = s3.list_buckets()
for bucket in response["Buckets"]:
    print(bucket["Name"])
```

### **ğŸ”¹ Subir un archivo a S3**
```python
s3.upload_file("mi_archivo.txt", "nombre-del-bucket", "carpeta/archivo.txt")
```

### **ğŸ”¹ Crear una instancia EC2**
```python
ec2 = boto3.resource("ec2")

# Crear una nueva instancia EC2
instances = ec2.create_instances(
    ImageId="ami-12345678", 
    MinCount=1,
    MaxCount=1,
    InstanceType="t2.micro"
)
print("Instancia creada:", instances[0].id)
```

### **ğŸ”¹ Leer datos de DynamoDB**
```python
dynamodb = boto3.resource("dynamodb")
table = dynamodb.Table("MiTabla")

# Obtener un elemento por su clave primaria
response = table.get_item(Key={"id": "123"})
print(response.get("Item"))
```

### **5ï¸âƒ£ Manejo de errores en Boto3**
Siempre es recomendable manejar excepciones para evitar fallos en el cÃ³digo:
```python
import botocore

try:
    s3.list_buckets()
except botocore.exceptions.NoCredentialsError:
    print("Credenciales no encontradas")
except botocore.exceptions.PartialCredentialsError:
    print("Credenciales incompletas")
```

### **ğŸš€ ConclusiÃ³n**
Boto3 es una herramienta poderosa para interactuar con AWS desde Python. Si necesitas ayuda con algÃºn servicio especÃ­fico, dime y te ayudo con cÃ³digo mÃ¡s detallado. ğŸ˜Š

## Â¿CÃ³mo usar Boto3?

### **ğŸ“Œ Â¿QuÃ© es Boto3?**  
Boto3 es el SDK de Python para interactuar con **AWS**. Permite gestionar servicios como **S3, EC2, DynamoDB, Lambda, RDS, entre otros**.

---

## **1ï¸âƒ£ InstalaciÃ³n de Boto3**
Para instalarlo en tu entorno de Python, usa:
```bash
pip install boto3
```

### **2ï¸âƒ£ ConfiguraciÃ³n de credenciales**  
Antes de usar Boto3, debes configurar tus credenciales de AWS. Puedes hacerlo de dos maneras:

### **ğŸ”¹ OpciÃ³n 1: Configurar usando AWS CLI**  
Si tienes instalado AWS CLI, ejecuta:
```bash
aws configure
```
Te pedirÃ¡:
- **AWS Access Key ID**  
- **AWS Secret Access Key**  
- **RegiÃ³n por defecto** (ejemplo: `us-east-1`)  
- **Formato de salida** (`json`, `text`, etc.)

Las credenciales se guardarÃ¡n en `~/.aws/credentials` (Linux/macOS) o `C:\Users\TU_USUARIO\.aws\credentials` (Windows).

### **ğŸ”¹ OpciÃ³n 2: Configurar manualmente**
Crea el archivo `~/.aws/credentials` y aÃ±ade:
```ini
[default]
aws_access_key_id = TU_ACCESS_KEY
aws_secret_access_key = TU_SECRET_KEY
region = us-east-1
```

### **3ï¸âƒ£ Uso de Boto3**
### **ğŸ”¹ Crear una sesiÃ³n en Boto3**
```python
import boto3

# Crear una sesiÃ³n con AWS
session = boto3.Session(
    aws_access_key_id="TU_ACCESS_KEY",
    aws_secret_access_key="TU_SECRET_KEY",
    region_name="us-east-1"
)
```
Si ya configuraste las credenciales con `aws configure`, puedes omitir los parÃ¡metros.

### **4ï¸âƒ£ Ejemplos de uso de Boto3**
### **ğŸ”¹ Listar buckets en S3**
```python
s3 = boto3.client("s3")

# Obtener lista de buckets
response = s3.list_buckets()
for bucket in response["Buckets"]:
    print(bucket["Name"])
```

### **ğŸ”¹ Subir un archivo a S3**
```python
s3.upload_file("mi_archivo.txt", "nombre-del-bucket", "carpeta/archivo.txt")
```

### **ğŸ”¹ Crear una instancia EC2**
```python
ec2 = boto3.resource("ec2")

# Crear una nueva instancia EC2
instances = ec2.create_instances(
    ImageId="ami-12345678", 
    MinCount=1,
    MaxCount=1,
    InstanceType="t2.micro"
)
print("Instancia creada:", instances[0].id)
```

### **ğŸ”¹ Leer datos de DynamoDB**
```python
dynamodb = boto3.resource("dynamodb")
table = dynamodb.Table("MiTabla")

# Obtener un elemento por su clave primaria
response = table.get_item(Key={"id": "123"})
print(response.get("Item"))
```

### **5ï¸âƒ£ Manejo de errores en Boto3**
Siempre es recomendable manejar excepciones para evitar fallos en el cÃ³digo:
```python
import botocore

try:
    s3.list_buckets()
except botocore.exceptions.NoCredentialsError:
    print("Credenciales no encontradas")
except botocore.exceptions.PartialCredentialsError:
    print("Credenciales incompletas")
```

### **ğŸš€ ConclusiÃ³n**
Boto3 es una herramienta poderosa para interactuar con AWS desde Python. Si necesitas ayuda con algÃºn servicio especÃ­fico, dime y te ayudo con cÃ³digo mÃ¡s detallado. ğŸ˜Š

### Resumen

### Â¿CÃ³mo utilizar Boto3 para interactuar con servicios de AWS?

Boto3 es la biblioteca de Python por excelencia para interactuar con los servicios de Amazon Web Services (AWS). Su comprensiÃ³n y uso efectivo son esenciales para cualquier profesional que trabaje con AWS, ya que permite gestionar diversos servicios desde un solo lugar.

### Â¿QuÃ© es Boto3?

Boto3 es el SDK (Software Development Kit) de Python para AWS. Con Ã©l puedes crear, configurar y gestionar servicios de AWS como S3 y Athena, entre otros. Ofrece una interfaz fÃ¡cil de usar para programadores, que facilita la automatizaciÃ³n de tareas repetitivas dentro del entorno de AWS.

### Â¿CÃ³mo acceder a la documentaciÃ³n de Boto3?

Para aprovechar al mÃ¡ximo Boto3, es crucial familiarizarse con la documentaciÃ³n proporcionada por AWS. Puedes encontrar informaciÃ³n detallada sobre cada servicio, ejemplos de cÃ³digo y explicaciones claras de los parÃ¡metros necesarios. AsÃ­ es como se accede a la documentaciÃ³n:

1. Abre una pestaÃ±a de tu navegador.
2. Busca "AWS Boto3 S3" o el servicio especÃ­fico que te interese.
3. Accede al primer enlace que normalmente te llevarÃ¡ a la documentaciÃ³n oficial.

### Â¿QuÃ© debemos tener en cuenta al utilizar Boto3 con AWS?

Al trabajar con Boto3, es vital inicializar el servicio especÃ­fico antes de realizar cualquier operaciÃ³n. Este proceso implica crear un cliente desde Boto3 y especificar el servicio dentro de los parÃ©ntesis y entre comillas sencillas. Veamos un ejemplo sencillo para inicializar el cliente de Athena:

```python
import boto3

client = boto3.client('athena')

# AquÃ­ harÃ­amos una consulta, iniciarÃ­amos su ejecuciÃ³n y obtendrÃ­amos la respuesta
```

### Â¿CÃ³mo interactuar con servicios como S3 y Athena?

Aparte de S3, otro servicio potente que puedes gestionar con Boto3 es Athena, el cual te permite lanzar consultas SQL sobre datos almacenados en S3. AquÃ­ hay un ejemplo prÃ¡ctico que muestra cÃ³mo inicializar y realizar operaciones con estos servicios:

1. S3: Puedes definir la regiÃ³n de operaciÃ³n si el script lo requiere.
2. Athena: Requiere la inicializaciÃ³n del servicio y luego puedes enviar consultas SQL para analizar los datos almacenados.

En general, un script tÃ­pico podrÃ­a verse asÃ­:

```python
import boto3

# Inicializar cliente de S3
s3 = boto3.client('s3', region_name='us-west-2')

# Listar buckets en S3
response = s3.list_buckets()
print(response['Buckets'])

# Inicializar cliente de Athena
athena = boto3.client('athena')

# Enviar una consulta a Athena
query_start = athena.start_query_execution(
    QueryString='SELECT * FROM database.table',
    QueryExecutionContext={'Database': 'my_database'},
    ResultConfiguration={
        'OutputLocation': 's3://my-athena-results-bucket/path/to/',
    }
)

print(query_start)
```

### Â¿Por quÃ© es esencial Boto3 para tus proyectos?

Boto3 es fundamental para cualquier proyecto de Big Data en AWS gracias a su capacidad para interconectar servicios de AWS mediante Python, el cual es un lenguaje robusto y ampliamente utilizado en anÃ¡lisis de datos. Con esta librerÃ­a, se combina la potencia de los servicios en la nube con la facilidad y versatilidad de Python, permitiendo desarrollar soluciones efectivas y eficientes.

Â¡AnÃ­mate a seguir explorando y utilizando Boto3 para automatizar tus procesos en AWS! La prÃ¡ctica y el conocimiento profundo de esta herramienta abrirÃ¡n muchas puertas en tu carrera como desarrollador o analista de datos.

**Lecturas recomendadas**

[Ejemplo de Python para AWS Cloud9 - AWS Cloud9](https://docs.aws.amazon.com/es_es/cloud9/latest/user-guide/sample-python.html)

[S3 â€” Boto 3 Docs 1.9.106 documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html)

[https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html)

## API Gateway

Amazon **API Gateway** es un servicio totalmente administrado de AWS que permite a los desarrolladores **crear, publicar, mantener, monitorear y asegurar APIs** en cualquier escala. Se utiliza para conectar clientes con **servicios backend** como **Lambda, EC2, DynamoDB o cualquier otro servicio HTTP/HTTPS**.

### **ğŸ”¹ CaracterÃ­sticas Principales**
1. **Soporte para diferentes tipos de APIs**  
   - RESTful APIs  
   - WebSocket APIs  
   - HTTP APIs (mÃ¡s ligeras y econÃ³micas que REST)

2. **Escalabilidad AutomÃ¡tica**  
   - Maneja millones de solicitudes sin intervenciÃ³n manual.  

3. **AutenticaciÃ³n y AutorizaciÃ³n**  
   - Compatible con **IAM, Cognito y Lambda Authorizers**.  

4. **GestiÃ³n de trÃ¡fico y seguridad**  
   - Soporta **rate limiting** y protecciÃ³n contra ataques DDoS.  

5. **Monitoreo y Logging**  
   - IntegraciÃ³n con **CloudWatch** para logs, mÃ©tricas y alertas.  

6. **TransformaciÃ³n de datos**  
   - Permite **mapear, modificar y validar** peticiones/respuestas.  

7. **IntegraciÃ³n con mÃºltiples backends**  
   - AWS Lambda, EC2, S3, DynamoDB, servicios HTTP, etc.

### **ğŸ”¹ Casos de Uso ğŸ“Œ**
âœ… CreaciÃ³n de microservicios  
âœ… API Gateway para aplicaciones mÃ³viles/web  
âœ… IntegraciÃ³n con AWS Lambda para una arquitectura **serverless**  
âœ… ExposiciÃ³n segura de endpoints para terceros  
âœ… Proxy para servicios internos de AWS  

### **ğŸ”¹ Ejemplo: Creando un API Gateway con Lambda**
1ï¸âƒ£ **Crear una funciÃ³n Lambda en AWS Lambda**  
2ï¸âƒ£ **Configurar API Gateway para que llame a la funciÃ³n Lambda**  
3ï¸âƒ£ **Implementar una polÃ­tica de seguridad (IAM o Cognito)**  
4ï¸âƒ£ **Probar la API con Postman o cURL**  

### **ğŸ”¹ Ejemplo en Python (Lambda)**
```python
import json

def lambda_handler(event, context):
    return {
        "statusCode": 200,
        "body": json.dumps({"message": "Â¡Hola desde AWS Lambda con API Gateway!"})
    }
```

Con API Gateway puedes **convertir cualquier servicio backend en una API escalable y segura**. ğŸš€

**Resumen**

Este servicio nos va a servir como puerta de enlace entre la data que tenemos y la plataforma en la nube.

- Soporta cientos de miles de llamadas concurrentes.
- Previene ataques DDOS.

## Storage Gateway

**AWS Storage Gateway** es un servicio hÃ­brido que permite conectar entornos **on-premise** con el almacenamiento en la **nube de AWS**. Se utiliza para extender la capacidad de almacenamiento local utilizando **Amazon S3, Glacier, EBS y otros servicios de AWS**.

### **ğŸ”¹ Tipos de Storage Gateway**
AWS ofrece **tres tipos principales** de Storage Gateway, segÃºn el caso de uso:  

1ï¸âƒ£ **File Gateway (NFS/SMB)**  
   - Permite almacenar archivos en **Amazon S3** y acceder a ellos como si fueran locales.  
   - Compatible con protocolos **NFS y SMB**.  
   - Ideal para **archivos de backup, machine learning y anÃ¡lisis de datos**.  

2ï¸âƒ£ **Volume Gateway (iSCSI)**  
   - Crea volÃºmenes que pueden ser montados en servidores **on-premise** como unidades de disco.  
   - Soporta **modo almacenado** (copia local + respaldo en AWS) o **modo en cachÃ©** (copia en AWS + cachÃ© local).  
   - Se integra con **EBS y S3** para backup y restauraciÃ³n.  

3ï¸âƒ£ **Tape Gateway (Virtual Tape Library - VTL)**  
   - Emula una **librerÃ­a de cintas** para backup en la nube.  
   - Usa **Amazon S3 y Glacier** para almacenamiento a largo plazo.  
   - Compatible con **software de backup tradicional** como Veeam, Commvault y NetBackup.

### **ğŸ”¹ Beneficios de Storage Gateway**
âœ… **ExtensiÃ³n del almacenamiento local sin comprar hardware adicional**  
âœ… **Backups automÃ¡ticos y recuperaciÃ³n ante desastres**  
âœ… **Acceso rÃ¡pido y seguro a datos en la nube**  
âœ… **ReducciÃ³n de costos operativos** al eliminar infraestructura fÃ­sica  
âœ… **Soporte para aplicaciones empresariales** como SAP, bases de datos y archivos compartidos

### **ğŸ”¹ Casos de Uso ğŸ“Œ**
ğŸ”¹ Empresas que desean **migrar gradualmente** a la nube  
ğŸ”¹ **Respaldo y recuperaciÃ³n de datos** desde servidores locales a AWS  
ğŸ”¹ **Archivos compartidos** accesibles desde mÃºltiples ubicaciones  
ğŸ”¹ **RetenciÃ³n de datos a largo plazo** con Glacier  

### **ğŸ”¹ Ejemplo de ImplementaciÃ³n de Storage Gateway**
1ï¸âƒ£ **Configurar una mÃ¡quina virtual** con el software de Storage Gateway en un entorno on-premise  
2ï¸âƒ£ **Conectar la mÃ¡quina a AWS** usando la consola de AWS  
3ï¸âƒ£ **Seleccionar el tipo de gateway** (File, Volume o Tape)  
4ï¸âƒ£ **Sincronizar con AWS S3, EBS o Glacier** segÃºn la configuraciÃ³n  
5ï¸âƒ£ **Acceder a los datos como si estuvieran en almacenamiento local**  

AWS Storage Gateway es una soluciÃ³n **hÃ­brida, flexible y econÃ³mica** para integrar almacenamiento local con la nube de AWS. ğŸš€

**Resumen**

Tiene tres caracterÃ­sticas importantes:

1. Nos permite enviar informaciÃ³n desde nuestro datacenter on-premise a la nube.
2. Puedes enviar los logs de una aplicaciÃ³n on-premise para que sean procesados.
3. Funciona en una mÃ¡quina virtual que instalamos en nuestro datacenter.

## Kinesis Data Streams 

**AWS Kinesis Data Streams** es un servicio de AWS que permite la **ingestiÃ³n, procesamiento y anÃ¡lisis de datos en tiempo real**. Es ideal para manejar flujos de datos generados continuamente, como logs, eventos de IoT, mÃ©tricas de aplicaciones y transacciones financieras.  

### **ğŸ”¹ CaracterÃ­sticas Principales**
âœ… **Procesamiento en tiempo real** ğŸ“Š  
âœ… **Alta escalabilidad** âš¡ (permite manejar desde KB/s hasta TB/hora)  
âœ… **Latencia baja (milisegundos)** â³  
âœ… **Persistencia de datos hasta 7 dÃ­as** ğŸ•’  
âœ… **IntegraciÃ³n con AWS Lambda, S3, DynamoDB, Redshift y mÃ¡s** ğŸ”„

### **ğŸ”¹ Componentes Claves de Kinesis Data Streams**
1ï¸âƒ£ **Shards (Fragmentos)**  
   - Cada stream se divide en shards, que determinan la capacidad del flujo.  
   - Cada shard puede manejar hasta **1 MB/s de escritura y 2 MB/s de lectura**.  
   - Puedes **escalar horizontalmente** agregando mÃ¡s shards.  

2ï¸âƒ£ **Producers (Productores)**  
   - Fuentes que envÃ­an datos a Kinesis.  
   - Ejemplos: logs de servidores, eventos de IoT, transacciones de e-commerce.  

3ï¸âƒ£ **Consumers (Consumidores)**  
   - Aplicaciones que leen y procesan los datos en tiempo real.  
   - Se pueden conectar mÃºltiples consumidores como **AWS Lambda, Kinesis Data Firehose, Apache Flink, o EC2**.

### **ğŸ”¹ Casos de Uso ğŸ“Œ**
ğŸ”¹ **Monitoreo en tiempo real** de logs de servidores y aplicaciones  
ğŸ”¹ **AnÃ¡lisis de tendencias de redes sociales** en tiempo real  
ğŸ”¹ **DetecciÃ³n de fraudes en transacciones bancarias** ğŸ’³  
ğŸ”¹ **Procesamiento de datos de IoT** en fÃ¡bricas, dispositivos inteligentes, etc.  
ğŸ”¹ **Streaming de eventos de videojuegos** ğŸ®

### **ğŸ”¹ Ejemplo de Uso: EnvÃ­o de Datos a Kinesis**
Usando **Boto3 (SDK de Python)** para enviar datos a un stream:

```python
import boto3
import json
import time

# Crear cliente de Kinesis
kinesis_client = boto3.client('kinesis', region_name='us-east-1')

stream_name = 'mi-stream'

# Enviar datos a Kinesis
for i in range(10):
    data = {
        'event_id': i,
        'timestamp': time.time(),
        'message': 'Evento de prueba'
    }
    kinesis_client.put_record(
        StreamName=stream_name,
        Data=json.dumps(data),
        PartitionKey=str(i)  # Determina a quÃ© shard irÃ¡ el dato
    )
    print(f"Enviado evento {i}")
    time.sleep(1)  # SimulaciÃ³n de flujo de datos
```

### **ğŸ”¹ Integraciones Populares**
âœ… **AWS Lambda** para disparar funciones en tiempo real  
âœ… **Kinesis Data Firehose** para almacenar datos en S3, Redshift o OpenSearch  
âœ… **Kinesis Data Analytics** para anÃ¡lisis en tiempo real con SQL  
âœ… **DynamoDB, S3, ElasticSearch** para almacenamiento y anÃ¡lisis posterior  

**ğŸ”¥ AWS Kinesis Data Streams es una soluciÃ³n potente para manejar y procesar grandes volÃºmenes de datos en tiempo real, con alta escalabilidad y baja latencia.** ğŸš€

**Resumen**

- Tienes que pensar en procesar grandes cantidades de datos, desde TB hasta EB.
- Algunos casos de uso son para procesar informaciÃ³n de logs, social media, market data feeds y web clickstream.
- La unidad fundamental dentro de Kinesis se llama Data Record.
- La informaciÃ³n dentro de Kinesis por defecto solamente cuenta con un periodo de retenciÃ³n de 24 horas.
- El Shard es una secuencia de Data Records dentro de un stream.

**Lecturas recomendadas**

[Creating and Updating Data Streams - Amazon Kinesis Data Streams](https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-streams.html)