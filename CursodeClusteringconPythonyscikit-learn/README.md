# Curso de Clustering con Python y scikit-learn

## Â¿QuÃ© es el clustering en machine learning?

**Clustering** en *machine learning* es una tÃ©cnica de **aprendizaje no supervisado** que consiste en **agrupar datos similares** entre sÃ­ en grupos llamados **clÃºsters** (*clusters*, en inglÃ©s), **sin usar etiquetas o categorÃ­as predefinidas**.

### ğŸ” Â¿QuÃ© significa eso?

* El algoritmo **no sabe a quÃ© clase pertenece cada dato**.
* En lugar de eso, **descubre patrones o estructuras ocultas** en los datos.
* Los puntos que son **mÃ¡s similares entre sÃ­** (segÃºn una medida de distancia o similitud) se agrupan en el mismo clÃºster.

### âœ… Ejemplos de uso de clustering:

* Agrupar clientes con comportamientos de compra similares (segmentaciÃ³n de mercado).
* Identificar patrones en imÃ¡genes o documentos.
* DetecciÃ³n de anomalÃ­as o fraudes.
* AgrupaciÃ³n de genes con funciones similares en biologÃ­a.

### ğŸ”§ Algoritmos comunes de clustering:

| Algoritmo                   | CaracterÃ­stica principal                                                  |
| --------------------------- | ------------------------------------------------------------------------- |
| **K-Means**                 | Divide los datos en **K grupos** basados en distancias.                   |
| **DBSCAN**                  | Agrupa por **densidad de puntos** (detecta clÃºsters de forma arbitraria). |
| **Hierarchical Clustering** | Crea una **jerarquÃ­a de clÃºsters** (en forma de Ã¡rbol o dendrograma).     |

### ğŸ¯ Resumen:

> **Clustering** es un mÃ©todo de aprendizaje no supervisado que busca **descubrir grupos naturales** dentro de un conjunto de datos **sin necesidad de etiquetas previas**.

**Archivos de la clase**

[slides-curso-clustering-con-python-scikit-learn.pdf](https://static.platzi.com/media/public/uploads/slides-curso-clustering-con-python-scikit-learn_835791cf-0955-4487-8d59-987d91044699.pdf)

**Lecturas recomendadas**

[Curso de IntroducciÃ³n a Machine Learning por MindsDB - Platzi](https://platzi.com/cursos/machine-learning/)

## Tu primer clustering con scikit-learn

Â¡Perfecto! AquÃ­ tienes un ejemplo bÃ¡sico de cÃ³mo hacer tu **primer clustering** usando **`scikit-learn`** en Python, utilizando el algoritmo **K-Means**, uno de los mÃ¡s comunes y fÃ¡ciles de usar.

### ğŸ“¦ Requisitos previos

Primero, asegÃºrate de tener las siguientes librerÃ­as instaladas:

```bash
pip install scikit-learn matplotlib seaborn
```

### âœ… Ejemplo bÃ¡sico: Clustering con K-Means

```python
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
import seaborn as sns

# 1. Generamos datos de ejemplo
X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=42)

# 2. Aplicamos KMeans con 3 clÃºsters
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)

# 3. Obtenemos las predicciones
y_kmeans = kmeans.predict(X)

# 4. Visualizamos el resultado
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_kmeans, palette="Set1", s=60)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='black', marker='X', label='Centroides')
plt.title("Clustering con KMeans")
plt.legend()
plt.show()
```

### ğŸ” Â¿QuÃ© hace este cÃ³digo?

1. **Crea datos sintÃ©ticos** con 3 clÃºsters.
2. Aplica el algoritmo **K-Means** para agrupar los datos.
3. **Visualiza** los grupos asignados y los centroides de cada clÃºster.

### Â¿Y luego?

Puedes intentar con tus propios datos o probar otros algoritmos de clustering como:

* `DBSCAN` (detecta clÃºsters de forma irregular)
* `AgglomerativeClustering` (jerÃ¡rquico)
* `MeanShift`

## Â¿CuÃ¡ndo usar clustering?

Puedes usar **clustering** cuando quieres **descubrir grupos naturales o estructuras ocultas** en tus datos **sin tener etiquetas previas**. Es una tÃ©cnica de **aprendizaje no supervisado**, Ãºtil en muchas situaciones donde necesitas **explorar, segmentar o reducir complejidad**.

### ğŸ§  Â¿CuÃ¡ndo usar clustering? (Casos comunes)

#### 1. **SegmentaciÃ³n de clientes**

* Para agrupar clientes con comportamientos similares.
* Ejemplo: Marketing personalizado segÃºn hÃ¡bitos de compra.

#### 2. **AnÃ¡lisis exploratorio de datos (EDA)**

* Para **detectar patrones desconocidos** antes de aplicar modelos supervisados.
* Ãštil para descubrir **subgrupos naturales**.

#### 3. **AgrupaciÃ³n de documentos o textos**

* Agrupar artÃ­culos, reseÃ±as o noticias por tema sin etiquetarlos previamente.
* Ejemplo: Agrupar reseÃ±as similares de productos.

#### 4. **ReducciÃ³n de complejidad**

* Cuando tienes muchos datos y quieres entender su **estructura interna**.
* Puedes usar clustering como paso previo a modelos mÃ¡s complejos.

#### 5. **DetecciÃ³n de anomalÃ­as**

* Algunos outliers no pertenecen a ningÃºn clÃºster y pueden ser **anomalÃ­as o fraudes**.

#### 6. **Agrupamiento de imÃ¡genes**

* Por similitud de colores, formas o patrones visuales.
* Ejemplo: Clasificar fotos similares en galerÃ­as.

#### 7. **AgrupaciÃ³n geogrÃ¡fica**

* Agrupar ubicaciones por cercanÃ­a (ej. zonas de entrega, clientes cercanos).

### â— CuÃ¡ndo **NO** es recomendable

* Cuando **ya tienes etiquetas claras** para cada clase â†’ usa modelos **supervisados**.
* Si los datos **no tienen patrones claros o separables**.
* Si necesitas resultados **explicables y consistentes**: algunos algoritmos de clustering pueden ser sensibles a la inicializaciÃ³n (como K-Means).

### ğŸ§© Â¿QuÃ© necesitas para aplicar clustering?

* Datos **sin etiquetas**.
* Alguna idea de **cuÃ¡ntos grupos** esperas (aunque hay algoritmos que lo infieren).
* Una **mÃ©trica de distancia o similitud** que tenga sentido en tu dominio (euclidiana, coseno, etc.).

## Â¿CÃ³mo evaluar modelos de clustering?

Evaluar modelos de **clustering** puede ser un reto porque **no hay etiquetas verdaderas** (en aprendizaje no supervisado), pero existen mÃ©tricas **internas** y **externas** que nos ayudan a medir quÃ© tan buenos son los grupos que genera el algoritmo.

### âœ… Â¿CÃ³mo evaluar un modelo de clustering?

### ğŸ”¹ 1. **MÃ©tricas internas**

Usadas **cuando no hay etiquetas reales**. EvalÃºan la **cohesiÃ³n** (quÃ© tan compactos son los clÃºsters) y la **separaciÃ³n** (quÃ© tan distintos son entre sÃ­).

| MÃ©trica                     | Â¿QuÃ© mide?                                                   | Valor ideal        |
| --------------------------- | ------------------------------------------------------------ | ------------------ |
| **Silhouette Score**        | QuÃ© tan cerca estÃ¡ cada punto de su propio clÃºster vs otros. | Cerca de **1**     |
| **Davies-Bouldin Index**    | Ratio de dispersiÃ³n intra-clÃºster / distancia inter-clÃºster. | **MÃ¡s bajo mejor** |
| **Calinski-Harabasz Index** | VariaciÃ³n entre clÃºsters comparada con la interna.           | **MÃ¡s alto mejor** |

### ğŸ”¹ 2. **MÃ©tricas externas**

Usadas **cuando tienes etiquetas reales** (como en benchmarks).

| MÃ©trica                                 | Â¿QuÃ© compara?                                           |
| --------------------------------------- | ------------------------------------------------------- |
| **Adjusted Rand Index (ARI)**           | Compara similitud entre clÃºsters y etiquetas reales.    |
| **Normalized Mutual Information (NMI)** | Mide informaciÃ³n compartida entre clÃºsters y etiquetas. |
| **Fowlkes-Mallows Score**               | EvalÃºa precisiÃ³n entre pares de puntos.                 |

### ğŸ”¹ 3. **VisualizaciÃ³n**

Aunque no es una mÃ©trica numÃ©rica, **visualizar los clÃºsters** ayuda a:

* Ver si hay **solapamientos** o agrupaciones claras.
* Detectar **outliers**.
* Usar reducciÃ³n de dimensiones como **PCA** o **t-SNE** para representar datos en 2D/3D.

```python
from sklearn.metrics import silhouette_score

score = silhouette_score(X, labels)
print("Silhouette Score:", score)
```

### ğŸ§  Consejo:

Usa **Silhouette Score** cuando no tienes etiquetas, y si puedes comparar resultados con una verdad conocida, incluye tambiÃ©n **ARI** o **NMI**.

**Lecturas recomendadas**

[Selecting the number of clusters with silhouette analysis on KMeans clustering â€” scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)

## Â¿QuÃ© es el algoritmo de K-means y cÃ³mo funciona?

El **algoritmo de K-Means** es uno de los algoritmos de **clustering mÃ¡s populares** en *machine learning no supervisado*. Su objetivo es **dividir un conjunto de datos en *K* grupos (clÃºsters)**, donde cada grupo contiene puntos que son similares entre sÃ­ y diferentes de los de otros grupos.

### ğŸ§  Â¿QuÃ© hace K-Means?

Dado un nÃºmero **K** (el nÃºmero de clÃºsters deseado), el algoritmo agrupa los datos de forma que se **minimice la distancia** de cada punto a su centroide (el "centro" del clÃºster).

### âš™ï¸ Â¿CÃ³mo funciona K-Means? (Pasos)

1. **InicializaciÃ³n**:

   * Escoge **K puntos aleatorios** como **centroides iniciales** (uno por clÃºster).

2. **AsignaciÃ³n de clÃºsters**:

   * Cada punto se asigna al clÃºster **mÃ¡s cercano** (usando, por ejemplo, distancia euclidiana).

3. **ActualizaciÃ³n de centroides**:

   * Se recalcula el **centroide** de cada clÃºster como el **promedio** de los puntos asignados a Ã©l.

4. **Repetir**:

   * Repite los pasos 2 y 3 hasta que:

     * Los centroides **ya no cambian significativamente**, o
     * Se alcanza un nÃºmero mÃ¡ximo de iteraciones.

### ğŸ“ˆ Ejemplo visual

Imagina un conjunto de puntos dispersos. K-Means:

* Coloca 3 puntos aleatorios como centroides (si K = 3).
* Agrupa los puntos segÃºn cercanÃ­a a esos centroides.
* Recalcula los centroides.
* Repite hasta que las posiciones de los centroides se estabilicen.

### âœ… Ventajas

* **FÃ¡cil de entender e implementar.**
* Funciona bien con clÃºsters **esfÃ©ricos y separados**.
* **RÃ¡pido** incluso con grandes conjuntos de datos.

### âš ï¸ Limitaciones

* Tienes que **definir K** previamente.
* Es sensible a:

  * **Outliers**
  * **InicializaciÃ³n aleatoria** (puede converger a soluciones subÃ³ptimas).
* Asume que los clÃºsters son de forma redonda y de tamaÃ±o similar.

### ğŸ” Â¿CuÃ¡ndo usarlo?

* SegmentaciÃ³n de clientes.
* Agrupamiento de imÃ¡genes.
* AnÃ¡lisis exploratorio de datos.

**Lecturas recomendadas** 

[Visualizing K-Means Clustering](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)

[Clustering comparison | Cartography Playground](https://cartography-playground.gitlab.io/playgrounds/clustering-comparison/)

## Â¿CuÃ¡ndo usar K-means?

**K-means** es un algoritmo de aprendizaje no supervisado usado principalmente para **clustering** (agrupamiento). Es Ãºtil cuando se desea agrupar elementos en subconjuntos similares **sin etiquetas previas**.

### ğŸ“Œ CuÃ¡ndo usar K-means:

1. **Cuando no hay etiquetas (unsupervised learning)**:
   Tienes datos sin categorÃ­as asignadas y deseas encontrar patrones naturales en ellos.

2. **Cuando esperas grupos esfÃ©ricos y bien separados**:
   K-means funciona mejor cuando los clusters tienen forma circular o esfÃ©rica (por la forma como calcula distancias).

3. **Cuando conoces o puedes estimar el nÃºmero de clusters (k)**:
   Es necesario definir cuÃ¡ntos grupos esperas obtener.

4. **Cuando los datos no tienen muchos outliers**:
   K-means es sensible a valores atÃ­picos porque usa medias para agrupar.

5. **Para segmentaciÃ³n de clientes, compresiÃ³n de imÃ¡genes, detecciÃ³n de patrones, etc.**

### âŒ No se recomienda K-means cuando:

* Los clusters tienen formas irregulares o distintos tamaÃ±os.
* Hay muchos valores atÃ­picos.
* No tienes idea del nÃºmero adecuado de grupos (aunque puedes usar mÃ©todos como el codo/elbow para estimarlo).

## Implementando K-means

A continuaciÃ³n te muestro un ejemplo paso a paso de **cÃ³mo implementar K-means en Python** usando `scikit-learn`.

### âœ… **Paso 1: Importar librerÃ­as necesarias**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
```

### âœ… **Paso 2: Generar datos de ejemplo**

```python
# Creamos un conjunto de datos sintÃ©tico con 3 clusters
X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)

# Visualizamos los datos
plt.scatter(X[:, 0], X[:, 1], s=50)
plt.title("Datos sin clasificar")
plt.show()
```

### âœ… **Paso 3: Aplicar K-means**

```python
# Creamos y ajustamos el modelo
kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(X)

# Obtenemos las predicciones (a quÃ© grupo pertenece cada punto)
y_kmeans = kmeans.predict(X)

# Centros de los clusters
centros = kmeans.cluster_centers_
```

### âœ… **Paso 4: Visualizar los resultados**

```python
# Graficamos los datos clasificados por cluster
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')

# Mostramos los centros de cada cluster
plt.scatter(centros[:, 0], centros[:, 1], c='red', s=200, alpha=0.75, marker='X')
plt.title("Resultados del clustering con K-means")
plt.show()
```

### âœ… **Paso 5 (Opcional): Evaluar el nÃºmero Ã³ptimo de clusters (mÃ©todo del codo)**

```python
inertia = []

for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(X)
    inertia.append(kmeans.inertia_)

plt.plot(range(1, 10), inertia, marker='o')
plt.xlabel('NÃºmero de clusters')
plt.ylabel('Inercia')
plt.title('MÃ©todo del Codo')
plt.show()
```

**Archivos de la clase**

[clustering-kmeans.ipynb](https://static.platzi.com/media/public/uploads/clustering_kmeans_1aa7190f-894b-4ea5-b457-24fb419c7226.ipynb)

**Lecturas recomendadas**

[sklearn.cluster.KMeans â€” scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)

[clustering_kmeans.ipynb - Google Drive](https://drive.google.com/file/d/1MqZPEldivmSm0l3coh9APCDrA267sjKj/view?usp=sharing)

## Encontrando K

Para **encontrar el nÃºmero Ã³ptimo de clusters (K)** al usar K-Means, existen varios mÃ©todos. AquÃ­ te explico los principales y te doy ejemplos prÃ¡cticos para que los implementes fÃ¡cilmente:

### âœ… 1. **Elbow Method (MÃ©todo del Codo)**

### Â¿QuÃ© hace?

Mide la **inercia** (dentro del grupo de errores cuadrados - SSE) para distintos valores de K y busca el "codo" donde la mejora se estabiliza.

### CÃ³digo:

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# Datos simulados para el ejemplo
X, _ = make_blobs(n_samples=300, centers=5, random_state=42)

inertia = []

K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertia.append(kmeans.inertia_)

plt.plot(K_range, inertia, 'bo-')
plt.xlabel('NÃºmero de Clusters (k)')
plt.ylabel('Inercia')
plt.title('MÃ©todo del Codo')
plt.show()
```

> ğŸ” Busca el punto donde la curva se "dobla", como un codo.

### âœ… 2. **Silhouette Score**

### Â¿QuÃ© hace?

Mide quÃ© tan bien separados estÃ¡n los clusters. Cuanto mayor el valor (cerca de 1), mejor separados y definidos estÃ¡n.

### CÃ³digo:

```python
from sklearn.metrics import silhouette_score

silhouette_scores = []

K_range = range(2, 11)  # Comienza desde 2 porque Silhouette no estÃ¡ definido para k=1
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)

plt.plot(K_range, silhouette_scores, 'go-')
plt.xlabel('NÃºmero de Clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Puntaje de Silhouette')
plt.show()
```

> âœ… El valor Ã³ptimo de `k` es donde el **Silhouette Score es mÃ¡s alto**.

### âœ… 3. **Usando Yellowbrick (si lo logras instalar)**

```python
from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans

model = KMeans(random_state=42)
visualizer = KElbowVisualizer(model, k=(1,10), metric='silhouette', timings=False)
visualizer.fit(X)
visualizer.show()
```

### Â¿QuÃ© mÃ©todo elegir?

| MÃ©todo           | Ideal cuando...                                                             |
| ---------------- | --------------------------------------------------------------------------- |
| Elbow            | Quieres ver el punto donde el "beneficio" se estabiliza                     |
| Silhouette Score | Quieres evaluar la calidad de los clusters                                  |
| Yellowbrick      | Quieres automatizar visualmente el anÃ¡lisis (requiere instalaciÃ³n correcta) |

**Archivos de la clase**

[clustering-kmeans.ipynb](https://static.platzi.com/media/public/uploads/clustering_kmeans_e52e3f11-b422-4522-86b8-693f0cd189b8.ipynb)

**Lecturas recomendadas**

[clustering_kmeans.ipynb - Google Drive](https://drive.google.com/file/d/1MqZPEldivmSm0l3coh9APCDrA267sjKj/view?usp=sharing)

## Evaluando resultados de K-means

Una vez que ejecutas el algoritmo K-Means y encuentras el valor Ã³ptimo de **K**, es fundamental **evaluar los resultados** para asegurarte de que los clusters tengan sentido. AquÃ­ tienes **formas efectivas de evaluar K-Means**:

### âœ… 1. **Silhouette Score (EvaluaciÃ³n numÃ©rica)**

### Â¿QuÃ© mide?

* QuÃ© tan **bien separados** y **cohesivos** estÃ¡n los clusters.
* Va de `-1` (malo) a `+1` (muy bueno).

### CÃ³digo:

```python
from sklearn.metrics import silhouette_score

# Asumiendo que ya tienes `X` (datos) y `kmeans.labels_` (etiquetas)
score = silhouette_score(X, kmeans.labels_)
print(f"Silhouette Score: {score:.3f}")
```

> âœ… Un score cerca de 1 es excelente. Cerca de 0 indica clusters solapados. Negativo es mal clustering.

### âœ… 2. **VisualizaciÃ³n de Clusters (2D)**

### Â¿Para quÃ© sirve?

* Visualmente valida si los grupos son distinguibles.
* Solo se puede usar si tienes 2 o 3 dimensiones, o aplicas reducciÃ³n como PCA.

### CÃ³digo:

```python
import matplotlib.pyplot as plt

# Asumiendo que usaste kmeans = KMeans(n_clusters=k)
plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=300, c='red', marker='X', label='Centroides')
plt.title("VisualizaciÃ³n de Clusters K-Means")
plt.legend()
plt.show()
```

### âœ… 3. **Matriz de confusiÃ³n (si tienes etiquetas reales)**

> Si estÃ¡s haciendo clustering sobre datos **etiquetados (supervisado)**, puedes evaluar con exactitud y mÃ©tricas de clasificaciÃ³n:

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Supongamos que tienes y_true (etiquetas reales) y y_pred (kmeans.labels_)
sns.heatmap(confusion_matrix(y_true, kmeans.labels_), annot=True, fmt="d", cmap="Blues")
plt.xlabel("Etiquetas de KMeans")
plt.ylabel("Etiquetas Reales")
plt.title("Matriz de ConfusiÃ³n")
plt.show()
```

### âœ… 4. **Silhouette Visualizer (Yellowbrick - opcional)**

```python
from yellowbrick.cluster import SilhouetteVisualizer

model = KMeans(n_clusters=k, random_state=42)
visualizer = SilhouetteVisualizer(model)
visualizer.fit(X)
visualizer.show()
```

> ğŸ“Œ Esto muestra la forma y consistencia de cada cluster.

### âœ… 5. **ComparaciÃ³n Intra vs Inter ClÃºster**

* **Intra-cluster distance**: quÃ© tan compactos son los puntos de cada cluster.
* **Inter-cluster distance**: quÃ© tan separados estÃ¡n los centroides.

Si lo deseas, puedo ayudarte a programar un pequeÃ±o anÃ¡lisis de estas mÃ©tricas.

## Â¿QuÃ© es hierarchical clustering y cÃ³mo funciona?

El **Hierarchical Clustering** (o **clustering jerÃ¡rquico**) es un algoritmo de agrupamiento **no supervisado** que construye una jerarquÃ­a de clusters, es decir, una estructura en forma de **Ã¡rbol** que muestra cÃ³mo se agrupan los datos.

### ğŸ” Â¿CÃ³mo funciona?

Existen dos enfoques principales:

#### 1. **Agglomerative (aglomerativo)** â€“ de abajo hacia arriba ğŸ‘‡

* **El mÃ¡s comÃºn**.
* Cada punto de datos comienza como su propio cluster.
* En cada paso, los **dos clusters mÃ¡s cercanos se fusionan**.
* Esto se repite hasta que todos los puntos estÃ¡n en un solo cluster o se cumple un criterio.

#### 2. **Divisive (divisivo)** â€“ de arriba hacia abajo ğŸ‘†

* Todos los datos comienzan en un Ãºnico gran cluster.
* En cada paso, se **divide el cluster mÃ¡s grande** hasta que cada punto es su propio cluster.

### ğŸ§  Â¿CÃ³mo decide quÃ© clusters unir o dividir?

El algoritmo usa una **mÃ©trica de distancia** entre clusters. Algunas comunes son:

| MÃ©todo     | QuÃ© mide                                                      |
| ---------- | ------------------------------------------------------------- |
| `single`   | Distancia mÃ­nima entre puntos de clusters                     |
| `complete` | Distancia mÃ¡xima entre puntos de clusters                     |
| `average`  | Promedio de todas las distancias                              |
| `ward`     | Minimiza la varianza total dentro de los clusters (muy usada) |

### ğŸ“ˆ RepresentaciÃ³n: Dendrograma

El resultado se visualiza como un **dendrograma**, un diagrama en forma de Ã¡rbol que muestra cÃ³mo se fusionan (o dividen) los clusters a medida que cambia la distancia.

```python
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

linked = linkage(X, method='ward')  # o 'single', 'average', etc.
dendrogram(linked)
plt.title("Dendrograma de Clustering JerÃ¡rquico")
plt.xlabel("Puntos de datos")
plt.ylabel("Distancia")
plt.show()
```

### âœ… Â¿CuÃ¡ndo usarlo?

* Cuando quieres **ver la estructura jerÃ¡rquica** de los datos.
* Cuando el nÃºmero de clusters **no es conocido de antemano**.
* Para datos con **pocos puntos** (porque no escala bien con muchos datos).

### âŒ Desventajas

* **Lento** en datasets grandes (`O(n^3)`).
* Puede ser **sensible a ruido y outliers**.
* No permite reorganizar clusters una vez creados.

## Â¿CuÃ¡ndo usar hierarchical clustering?

Usar **clustering jerÃ¡rquico** (`Hierarchical Clustering`) es recomendable en los siguientes escenarios:

### âœ… **CUÃNDO USARLO**

#### 1. **Cuando no sabes cuÃ¡ntos clusters hay**

* El dendrograma te permite **explorar la estructura** y **elegir el nÃºmero de clusters visualmente** (cortando el Ã¡rbol).
* Ideal para **descubrimiento exploratorio**.

#### 2. **Cuando quieres una visiÃ³n jerÃ¡rquica de los datos**

* Si te interesa **ver relaciones padre-hijo** entre grupos (subgrupos dentro de grupos mÃ¡s grandes), este mÃ©todo es ideal.
* Ejemplo: taxonomÃ­a biolÃ³gica, estructura de carpetas, segmentaciÃ³n de clientes multinivel.

#### 3. **Para datasets pequeÃ±os o medianos**

* Funciona bien con **menos de \~1,000-5,000 puntos**. MÃ¡s allÃ¡ de eso, puede volverse muy lento y demandante en memoria.
* Es mejor para anÃ¡lisis en profundidad que para producciÃ³n a gran escala.

#### 4. **Cuando los clusters no son esfÃ©ricos ni del mismo tamaÃ±o**

* A diferencia de K-means, que asume clusters circulares del mismo tamaÃ±o, el clustering jerÃ¡rquico **no impone esa suposiciÃ³n**.

#### 5. **Cuando necesitas interpretar los resultados**

* El dendrograma es **intuitivo y visualmente explicativo**, muy Ãºtil en reportes o anÃ¡lisis descriptivos.

### âŒ **CUÃNDO EVITARLO**

| SituaciÃ³n                                | Por quÃ© evitarlo                                                      |
| ---------------------------------------- | --------------------------------------------------------------------- |
| Dataset muy grande                       | Tiene **complejidad O(nÂ³)** y **memoria O(nÂ²)**.                      |
| Necesitas clasificar nuevos datos rÃ¡pido | No es **incremental** ni rÃ¡pido para nuevos puntos.                   |
| Tienes muchos outliers                   | Puede generar **clusters distorsionados** si no hay preprocesamiento. |

### ğŸ”„ Alternativas en esos casos

| Necesidad                            | Alternativa                    |
| ------------------------------------ | ------------------------------ |
| Escalabilidad                        | K-means, MiniBatchKMeans       |
| DetecciÃ³n de formas arbitrarias      | DBSCAN, HDBSCAN                |
| Robustez a ruido                     | DBSCAN                         |
| ClasificaciÃ³n rÃ¡pida de nuevos datos | K-means o modelos supervisados |

## Implementando hierarchical clustering

AquÃ­ tienes una implementaciÃ³n completa de **Hierarchical Clustering (Clustering JerÃ¡rquico)** con `scikit-learn`, usando un dataset sintÃ©tico y visualizando tanto los clusters como el dendrograma:

### âœ… PASO A PASO: Hierarchical Clustering con `scikit-learn`

#### 1. Instalar librerÃ­as (si no las tienes):

```bash
pip install matplotlib seaborn scipy scikit-learn
```
#### 2. CÃ³digo completo:

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# Configurar estilo grÃ¡fico
sns.set(style="whitegrid")

# Generar datos artificiales
X, y_true = make_blobs(n_samples=150, centers=4, cluster_std=1.0, random_state=42)

# Estandarizar (muy recomendable)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Crear y ajustar modelo de clustering jerÃ¡rquico
hc = AgglomerativeClustering(n_clusters=4, linkage='ward')
clusters = hc.fit_predict(X_scaled)

# Visualizar los clusters
plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='viridis', s=50)
plt.title("Clusters jerÃ¡rquicos")
plt.xlabel("CaracterÃ­stica 1")
plt.ylabel("CaracterÃ­stica 2")
plt.show()
```

#### 3. Visualizar el dendrograma (opcional, pero muy Ãºtil):

```python
# Calcular matriz de enlaces
linked = linkage(X_scaled, method='ward')

# Graficar dendrograma
plt.figure(figsize=(10, 6))
dendrogram(linked, truncate_mode='level', p=5)  # p=5 muestra los niveles superiores
plt.title("Dendrograma")
plt.xlabel("Puntos de datos")
plt.ylabel("Distancia")
plt.show()
```

### ğŸ” Opciones importantes en `AgglomerativeClustering`:

| ParÃ¡metro    | DescripciÃ³n                                                     |
| ------------ | --------------------------------------------------------------- |
| `n_clusters` | NÃºmero de clusters finales                                      |
| `linkage`    | Tipo de enlace: `'ward'`, `'complete'`, `'average'`, `'single'` |
| `affinity`   | MÃ©trica de distancia (solo si `linkage â‰  'ward'`)               |

**Archivos de la clase**

[hierachical-clustering.ipynb](https://static.platzi.com/media/public/uploads/hierachical_clustering_e27a7035-5973-4cf5-be54-b7d27bd9f351.ipynb)

**Lecturas recomendadas**

[sklearn.cluster.AgglomerativeClustering â€” scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)

[hierachical_clustering.ipynb - Google Drive](https://drive.google.com/file/d/1luOzJwPqgSfEz2PHTg3PufcOyHP0imnO/view?usp=sharing)

## Evaluando resultados de hierarchical clustering

Para **evaluar los resultados del clustering jerÃ¡rquico (hierarchical clustering)**, puedes aplicar mÃ©tricas internas similares a las usadas en otros algoritmos como K-means. AquÃ­ te muestro cÃ³mo hacerlo y con quÃ© herramientas:

### ğŸ“Š **1. Silhouette Score**

Mide quÃ© tan bien estÃ¡n separados los clÃºsteres:

```python
from sklearn.metrics import silhouette_score

score = silhouette_score(X_scaled, y_hc)
print(f"Silhouette Score: {score:.3f}")
```

* **Rango:** de -1 a 1. Mientras mÃ¡s cercano a 1, mejor definidos estÃ¡n los grupos.

### ğŸ§® **2. Calinski-Harabasz Index**

Mide la dispersiÃ³n entre clÃºsteres frente a la dispersiÃ³n dentro del clÃºster:

```python
from sklearn.metrics import calinski_harabasz_score

score = calinski_harabasz_score(X_scaled, y_hc)
print(f"Calinski-Harabasz Index: {score:.2f}")
```

* **InterpretaciÃ³n:** cuanto mayor el valor, mejor separados estÃ¡n los clÃºsteres.

### ğŸ”¢ **3. Davies-Bouldin Index**

Mide la similitud entre clÃºsteres; valores mÃ¡s bajos son mejores:

```python
from sklearn.metrics import davies_bouldin_score

score = davies_bouldin_score(X_scaled, y_hc)
print(f"Davies-Bouldin Index: {score:.3f}")
```

### ğŸŒ³ **4. VisualizaciÃ³n con dendrograma**

Si usas `scipy`, puedes generar un dendrograma para inspecciÃ³n visual:

```python
import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title('Dendrograma')
plt.xlabel('Muestras')
plt.ylabel('Distancia')
plt.show()
```

### âœ… ConclusiÃ³n

| MÃ©trica                 | Â¿QuÃ© mide?                    | Ideal       |
| ----------------------- | ----------------------------- | ----------- |
| Silhouette Score        | SeparaciÃ³n y compactaciÃ³n     | Cercano a 1 |
| Calinski-Harabasz Index | DispersiÃ³n entre/intra grupos | Alto        |
| Davies-Bouldin Index    | Similitud entre clÃºsteres     | Bajo        |

**Lecturas recomendadas**

[Selecting the number of clusters with silhouette analysis on KMeans clustering â€” scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)

[hierachical_clustering.ipynb - Google Drive](https://drive.google.com/file/d/1luOzJwPqgSfEz2PHTg3PufcOyHP0imnO/view?usp=sharing)

## Â¿QuÃ© es DBSCAN y cÃ³mo funciona?

**DBSCAN** (*Density-Based Spatial Clustering of Applications with Noise*) es un algoritmo de clustering basado en densidad. A diferencia de K-means o el clustering jerÃ¡rquico, **no necesitas especificar el nÃºmero de clÃºsteres de antemano**, y es muy eficaz para encontrar **clÃºsteres de forma arbitraria y detectar ruido (outliers)**.

### ğŸ”§ Â¿CÃ³mo funciona DBSCAN?

Se basa en dos parÃ¡metros principales:

1. **Îµ (epsilon):** el radio para considerar vecinos cercanos.
2. **minPts:** el nÃºmero mÃ­nimo de puntos para formar un clÃºster denso.

### ğŸ§± ClasificaciÃ³n de puntos

DBSCAN clasifica los puntos en tres tipos:

* **Punto nÃºcleo:** tiene al menos `minPts` vecinos dentro de un radio `Îµ`.
* **Punto frontera:** estÃ¡ dentro del radio `Îµ` de un punto nÃºcleo, pero no tiene suficientes vecinos para ser nÃºcleo.
* **Ruido (outlier):** no es nÃºcleo ni frontera.

### ğŸ”„ Algoritmo paso a paso:

1. Elige un punto no visitado.
2. Si tiene suficientes vecinos dentro de `Îµ`, crea un nuevo clÃºster.
3. Expande el clÃºster agregando todos los puntos densamente conectados.
4. Si no tiene suficientes vecinos, mÃ¡rcalo como **ruido**.
5. Repite hasta visitar todos los puntos.

### ğŸŸ¢ Ventajas de DBSCAN

* No necesita saber el nÃºmero de clÃºsteres.
* Puede detectar **formas complejas** y **outliers**.
* Robusto al ruido.

### ğŸ”´ Desventajas

* DifÃ­cil de elegir los parÃ¡metros Ã³ptimos `Îµ` y `minPts`.
* No funciona bien si los clÃºsteres tienen **densidades muy diferentes**.
* Menos eficiente en conjuntos de datos muy grandes o de alta dimensiÃ³n.

### ğŸ“Œ Ejemplo visual

SupÃ³n un conjunto de datos con dos clÃºsteres curvados y algo de ruido. K-means probablemente divida mal los clÃºsteres porque supone formas circulares. DBSCAN, en cambio, los detecta correctamente y marca el ruido.

**Lecturas recomendadas**

[Visualizing DBSCAN Clustering](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

## Â¿CuÃ¡ndo usar DBSCAN?

Puedes usar **DBSCAN** cuando:

### âœ… **1. No conoces el nÃºmero de clÃºsteres de antemano**

* DBSCAN detecta automÃ¡ticamente cuÃ¡ntos clÃºsteres hay (siempre que la densidad lo permita).
* Ideal para exploraciÃ³n de datos sin conocimiento previo.

### âœ… **2. Quieres detectar *outliers* o ruido**

* DBSCAN clasifica naturalmente los puntos aislados como **ruido**, lo que es Ãºtil en anÃ¡lisis de anomalÃ­as o fraudes.

### âœ… \**3. Tus clÃºsteres tienen formas **no circulares** o **arbitrarias***

* A diferencia de K-means, que funciona bien solo con formas esfÃ©ricas, DBSCAN puede detectar clÃºsteres **curvados, alargados o irregulares**.

### âœ… **4. Tus datos tienen densidades consistentes**

* Si los clÃºsteres tienen **densidades similares**, DBSCAN puede separarlos bien.

### ğŸš« **CuÃ¡ndo **no** usar DBSCAN**

* Si los clÃºsteres tienen **densidades muy diferentes** â†’ DBSCAN puede fallar en separar bien los grupos.
* En **altas dimensiones**, la distancia euclidiana pierde significado â†’ puede que `Îµ` no funcione bien.
* Si necesitas explicaciones claras de los clÃºsteres â†’ K-means o clustering jerÃ¡rquico pueden ser mÃ¡s interpretables.

### ğŸ“Œ Ejemplos de aplicaciÃ³n:

* DetecciÃ³n de fraudes bancarios (puntos atÃ­picos).
* AnÃ¡lisis de trÃ¡fico GPS (trayectorias densas vs. aisladas).
* AgrupaciÃ³n de tweets o noticias similares (previo uso de reducciÃ³n de dimensionalidad).
* ImÃ¡genes satelitales donde los objetos tienen formas irregulares.

## Implementando DBSCAN

AquÃ­ tienes una implementaciÃ³n prÃ¡ctica de **DBSCAN** en Python usando `scikit-learn`, con visualizaciÃ³n incluida:

### âœ… **Paso 1: Cargar y preparar los datos**

Usaremos datos simulados con formas no circulares para mostrar la ventaja de DBSCAN.

```python
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

# Crear un conjunto de datos de ejemplo (dos medias lunas)
X, y = make_moons(n_samples=300, noise=0.1, random_state=42)

# Visualizar
plt.scatter(X[:, 0], X[:, 1])
plt.title("Datos de ejemplo: dos medias lunas")
plt.show()
```

### âœ… **Paso 2: Aplicar DBSCAN**

```python
from sklearn.cluster import DBSCAN

# Crear modelo DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=5)  # Puedes ajustar estos parÃ¡metros

# Ajustar modelo
y_dbscan = dbscan.fit_predict(X)

# Visualizar resultado
plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma')
plt.title("ClÃºsteres encontrados con DBSCAN")
plt.show()
```

### âš™ï¸ **ParÃ¡metros importantes:**

* `eps`: distancia mÃ¡xima entre dos puntos para que uno sea considerado vecino del otro.
* `min_samples`: nÃºmero mÃ­nimo de puntos para formar un clÃºster denso (incluye el punto central).

### âœ… **Paso 3 (opcional): Identificar *outliers***

```python
# Los puntos con etiqueta -1 son considerados ruido
import numpy as np

outliers = X[y_dbscan == -1]

plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma')
plt.scatter(outliers[:, 0], outliers[:, 1], c='red', label='Outliers')
plt.legend()
plt.title("DBSCAN con detecciÃ³n de outliers")
plt.show()
```

**Archivos de la clase**

[dbscan.ipynb](https://static.platzi.com/media/public/uploads/dbscan_f460079b-4fed-429f-9fdd-58b6aad8eb9b.ipynb)

**Lecturas recomendadas**

[sklearn.cluster.DBSCAN â€” scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[dbscan.ipynb - Google Drive](https://drive.google.com/file/d/1e0LRhK3k00yxHZy5Q9eXS0jTU0GdnYTq/view?usp=sharing)

## Encontrar hÃ­per-parÃ¡metros

Para encontrar los **hiperparÃ¡metros Ã³ptimos de DBSCAN**, especialmente `eps` (radio de vecindad), podemos usar un grÃ¡fico de **distancias de los vecinos mÃ¡s cercanos**. Esto ayuda a detectar el valor de `eps` a partir de una "rodilla" o cambio brusco en la curva.

### âœ… **Paso 1: Graficar las distancias del vecino mÃ¡s cercano**

```python
from sklearn.neighbors import NearestNeighbors
import numpy as np
import matplotlib.pyplot as plt

# Ajustar vecinos
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(X)
distances, indices = neighbors_fit.kneighbors(X)

# Ordenar las distancias del 4Âº vecino (porque n_neighbors=5)
distances = np.sort(distances[:, 4])  # el Ã­ndice es n_neighbors - 1

# Graficar
plt.plot(distances)
plt.ylabel("Distancia al 4Âº vecino mÃ¡s cercano")
plt.xlabel("Puntos ordenados por distancia")
plt.title("Curva de codo para encontrar eps")
plt.grid(True)
plt.show()
```

ğŸ§  **InterpretaciÃ³n**: Busca el punto donde la pendiente del grÃ¡fico cambia bruscamente ("codo"). Ese valor es una buena estimaciÃ³n para `eps`.

### âœ… **Paso 2: Aplicar DBSCAN con eps hallado**

SupÃ³n que el grÃ¡fico muestra un codo en `eps â‰ˆ 0.25`:

```python
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.25, min_samples=5)
y_dbscan = dbscan.fit_predict(X)

# Visualizar
plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma')
plt.title("DBSCAN con eps ajustado")
plt.show()
```

### âœ… **Paso 3 (opcional): Evaluar calidad con Silhouette Score**

```python
from sklearn.metrics import silhouette_score

# AsegÃºrate de que haya mÃ¡s de 1 clÃºster
n_clusters = len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)
if n_clusters > 1:
    score = silhouette_score(X, y_dbscan)
    print(f"Silhouette Score: {score:.3f}")
else:
    print("DBSCAN no encontrÃ³ suficientes clÃºsteres para calcular Silhouette Score.")
```

**Lecturas recomendadas**

[sklearn.cluster.DBSCAN â€” scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[dbscan.ipynb - Google Drive](https://drive.google.com/file/d/1e0LRhK3k00yxHZy5Q9eXS0jTU0GdnYTq/view?usp=sharing)

## Evaluando resultados de DBSCAN

Para **evaluar los resultados de DBSCAN**, puedes utilizar diversas mÃ©tricas y visualizaciones. AquÃ­ te explico los enfoques mÃ¡s comunes:

### âœ… 1. **VisualizaciÃ³n de los clÃºsteres**

La forma mÃ¡s directa de evaluar DBSCAN es visualizar los clÃºsteres:

```python
import matplotlib.pyplot as plt

plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma')
plt.title("Resultados de DBSCAN")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.grid(True)
plt.show()
```

* Los **ruidos** serÃ¡n etiquetados como `-1`.
* Colores diferentes representan diferentes clÃºsteres.

### âœ… 2. **Silhouette Score**

EvalÃºa quÃ© tan bien estÃ¡ cada punto dentro de su clÃºster:

```python
from sklearn.metrics import silhouette_score

labels = y_dbscan
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

if n_clusters > 1:
    score = silhouette_score(X, labels)
    print(f"Silhouette Score: {score:.3f}")
else:
    print("No se pueden calcular mÃ©tricas: hay menos de 2 clÃºsteres.")
```

> ğŸ¯ **InterpretaciÃ³n**: Cuanto mÃ¡s cercano a 1, mejor. Valores < 0 indican mala asignaciÃ³n.

### âœ… 3. **NÃºmero de clÃºsteres y ruido**

Puedes revisar cuÃ¡ntos clÃºsteres encontrÃ³ DBSCAN y cuÃ¡ntos puntos considerÃ³ como ruido:

```python
import numpy as np

n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)

print(f"NÃºmero de clÃºsteres encontrados: {n_clusters}")
print(f"NÃºmero de puntos de ruido: {n_noise}")
```

### âœ… 4. **ConfusiÃ³n con etiquetas reales (si existen)**

Si tienes etiquetas verdaderas (`y_true`), puedes usar mÃ©tricas como *Adjusted Rand Index (ARI)* o *Homogeneity Score*:

```python
from sklearn.metrics import adjusted_rand_score, homogeneity_score

print("ARI:", adjusted_rand_score(y_true, y_dbscan))
print("Homogeneidad:", homogeneity_score(y_true, y_dbscan))
```

### âœ… 5. **Silhouette Visualizer (opcional)**

Si tienes instalado `yellowbrick`, puedes usar un grÃ¡fico de silueta:

```python
from yellowbrick.cluster import SilhouetteVisualizer
from sklearn.cluster import DBSCAN

visualizer = SilhouetteVisualizer(DBSCAN(eps=0.3, min_samples=5))
visualizer.fit(X)
visualizer.show()
```

> AsegÃºrate de que haya mÃ¡s de un clÃºster para que esto funcione.

**Lecturas recomendadas**

[Selecting the number of clusters with silhouette analysis on KMeans clustering â€” scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)

[dbscan.ipynb - Google Drive](https://drive.google.com/file/d/1e0LRhK3k00yxHZy5Q9eXS0jTU0GdnYTq/view?usp=sharing)