# Curso de Clustering con Python y scikit-learn

## ¬øQu√© es el clustering en machine learning?

**Clustering** en *machine learning* es una t√©cnica de **aprendizaje no supervisado** que consiste en **agrupar datos similares** entre s√≠ en grupos llamados **cl√∫sters** (*clusters*, en ingl√©s), **sin usar etiquetas o categor√≠as predefinidas**.

### üîç ¬øQu√© significa eso?

* El algoritmo **no sabe a qu√© clase pertenece cada dato**.
* En lugar de eso, **descubre patrones o estructuras ocultas** en los datos.
* Los puntos que son **m√°s similares entre s√≠** (seg√∫n una medida de distancia o similitud) se agrupan en el mismo cl√∫ster.

### ‚úÖ Ejemplos de uso de clustering:

* Agrupar clientes con comportamientos de compra similares (segmentaci√≥n de mercado).
* Identificar patrones en im√°genes o documentos.
* Detecci√≥n de anomal√≠as o fraudes.
* Agrupaci√≥n de genes con funciones similares en biolog√≠a.

### üîß Algoritmos comunes de clustering:

| Algoritmo                   | Caracter√≠stica principal                                                  |
| --------------------------- | ------------------------------------------------------------------------- |
| **K-Means**                 | Divide los datos en **K grupos** basados en distancias.                   |
| **DBSCAN**                  | Agrupa por **densidad de puntos** (detecta cl√∫sters de forma arbitraria). |
| **Hierarchical Clustering** | Crea una **jerarqu√≠a de cl√∫sters** (en forma de √°rbol o dendrograma).     |

### üéØ Resumen:

> **Clustering** es un m√©todo de aprendizaje no supervisado que busca **descubrir grupos naturales** dentro de un conjunto de datos **sin necesidad de etiquetas previas**.

**Archivos de la clase**

[slides-curso-clustering-con-python-scikit-learn.pdf](https://static.platzi.com/media/public/uploads/slides-curso-clustering-con-python-scikit-learn_835791cf-0955-4487-8d59-987d91044699.pdf)

**Lecturas recomendadas**

[Curso de Introducci√≥n a Machine Learning por MindsDB - Platzi](https://platzi.com/cursos/machine-learning/)

## Tu primer clustering con scikit-learn

¬°Perfecto! Aqu√≠ tienes un ejemplo b√°sico de c√≥mo hacer tu **primer clustering** usando **`scikit-learn`** en Python, utilizando el algoritmo **K-Means**, uno de los m√°s comunes y f√°ciles de usar.

### üì¶ Requisitos previos

Primero, aseg√∫rate de tener las siguientes librer√≠as instaladas:

```bash
pip install scikit-learn matplotlib seaborn
```

### ‚úÖ Ejemplo b√°sico: Clustering con K-Means

```python
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
import seaborn as sns

# 1. Generamos datos de ejemplo
X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=42)

# 2. Aplicamos KMeans con 3 cl√∫sters
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)

# 3. Obtenemos las predicciones
y_kmeans = kmeans.predict(X)

# 4. Visualizamos el resultado
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y_kmeans, palette="Set1", s=60)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='black', marker='X', label='Centroides')
plt.title("Clustering con KMeans")
plt.legend()
plt.show()
```

### üîç ¬øQu√© hace este c√≥digo?

1. **Crea datos sint√©ticos** con 3 cl√∫sters.
2. Aplica el algoritmo **K-Means** para agrupar los datos.
3. **Visualiza** los grupos asignados y los centroides de cada cl√∫ster.

### ¬øY luego?

Puedes intentar con tus propios datos o probar otros algoritmos de clustering como:

* `DBSCAN` (detecta cl√∫sters de forma irregular)
* `AgglomerativeClustering` (jer√°rquico)
* `MeanShift`

## ¬øCu√°ndo usar clustering?

Puedes usar **clustering** cuando quieres **descubrir grupos naturales o estructuras ocultas** en tus datos **sin tener etiquetas previas**. Es una t√©cnica de **aprendizaje no supervisado**, √∫til en muchas situaciones donde necesitas **explorar, segmentar o reducir complejidad**.

### üß† ¬øCu√°ndo usar clustering? (Casos comunes)

#### 1. **Segmentaci√≥n de clientes**

* Para agrupar clientes con comportamientos similares.
* Ejemplo: Marketing personalizado seg√∫n h√°bitos de compra.

#### 2. **An√°lisis exploratorio de datos (EDA)**

* Para **detectar patrones desconocidos** antes de aplicar modelos supervisados.
* √ötil para descubrir **subgrupos naturales**.

#### 3. **Agrupaci√≥n de documentos o textos**

* Agrupar art√≠culos, rese√±as o noticias por tema sin etiquetarlos previamente.
* Ejemplo: Agrupar rese√±as similares de productos.

#### 4. **Reducci√≥n de complejidad**

* Cuando tienes muchos datos y quieres entender su **estructura interna**.
* Puedes usar clustering como paso previo a modelos m√°s complejos.

#### 5. **Detecci√≥n de anomal√≠as**

* Algunos outliers no pertenecen a ning√∫n cl√∫ster y pueden ser **anomal√≠as o fraudes**.

#### 6. **Agrupamiento de im√°genes**

* Por similitud de colores, formas o patrones visuales.
* Ejemplo: Clasificar fotos similares en galer√≠as.

#### 7. **Agrupaci√≥n geogr√°fica**

* Agrupar ubicaciones por cercan√≠a (ej. zonas de entrega, clientes cercanos).

### ‚ùó Cu√°ndo **NO** es recomendable

* Cuando **ya tienes etiquetas claras** para cada clase ‚Üí usa modelos **supervisados**.
* Si los datos **no tienen patrones claros o separables**.
* Si necesitas resultados **explicables y consistentes**: algunos algoritmos de clustering pueden ser sensibles a la inicializaci√≥n (como K-Means).

### üß© ¬øQu√© necesitas para aplicar clustering?

* Datos **sin etiquetas**.
* Alguna idea de **cu√°ntos grupos** esperas (aunque hay algoritmos que lo infieren).
* Una **m√©trica de distancia o similitud** que tenga sentido en tu dominio (euclidiana, coseno, etc.).

## ¬øC√≥mo evaluar modelos de clustering?

Evaluar modelos de **clustering** puede ser un reto porque **no hay etiquetas verdaderas** (en aprendizaje no supervisado), pero existen m√©tricas **internas** y **externas** que nos ayudan a medir qu√© tan buenos son los grupos que genera el algoritmo.

### ‚úÖ ¬øC√≥mo evaluar un modelo de clustering?

### üîπ 1. **M√©tricas internas**

Usadas **cuando no hay etiquetas reales**. Eval√∫an la **cohesi√≥n** (qu√© tan compactos son los cl√∫sters) y la **separaci√≥n** (qu√© tan distintos son entre s√≠).

| M√©trica                     | ¬øQu√© mide?                                                   | Valor ideal        |
| --------------------------- | ------------------------------------------------------------ | ------------------ |
| **Silhouette Score**        | Qu√© tan cerca est√° cada punto de su propio cl√∫ster vs otros. | Cerca de **1**     |
| **Davies-Bouldin Index**    | Ratio de dispersi√≥n intra-cl√∫ster / distancia inter-cl√∫ster. | **M√°s bajo mejor** |
| **Calinski-Harabasz Index** | Variaci√≥n entre cl√∫sters comparada con la interna.           | **M√°s alto mejor** |

### üîπ 2. **M√©tricas externas**

Usadas **cuando tienes etiquetas reales** (como en benchmarks).

| M√©trica                                 | ¬øQu√© compara?                                           |
| --------------------------------------- | ------------------------------------------------------- |
| **Adjusted Rand Index (ARI)**           | Compara similitud entre cl√∫sters y etiquetas reales.    |
| **Normalized Mutual Information (NMI)** | Mide informaci√≥n compartida entre cl√∫sters y etiquetas. |
| **Fowlkes-Mallows Score**               | Eval√∫a precisi√≥n entre pares de puntos.                 |

### üîπ 3. **Visualizaci√≥n**

Aunque no es una m√©trica num√©rica, **visualizar los cl√∫sters** ayuda a:

* Ver si hay **solapamientos** o agrupaciones claras.
* Detectar **outliers**.
* Usar reducci√≥n de dimensiones como **PCA** o **t-SNE** para representar datos en 2D/3D.

```python
from sklearn.metrics import silhouette_score

score = silhouette_score(X, labels)
print("Silhouette Score:", score)
```

### üß† Consejo:

Usa **Silhouette Score** cuando no tienes etiquetas, y si puedes comparar resultados con una verdad conocida, incluye tambi√©n **ARI** o **NMI**.

**Lecturas recomendadas**

[Selecting the number of clusters with silhouette analysis on KMeans clustering ‚Äî scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)

## ¬øQu√© es el algoritmo de K-means y c√≥mo funciona?

El **algoritmo de K-Means** es uno de los algoritmos de **clustering m√°s populares** en *machine learning no supervisado*. Su objetivo es **dividir un conjunto de datos en *K* grupos (cl√∫sters)**, donde cada grupo contiene puntos que son similares entre s√≠ y diferentes de los de otros grupos.

### üß† ¬øQu√© hace K-Means?

Dado un n√∫mero **K** (el n√∫mero de cl√∫sters deseado), el algoritmo agrupa los datos de forma que se **minimice la distancia** de cada punto a su centroide (el "centro" del cl√∫ster).

### ‚öôÔ∏è ¬øC√≥mo funciona K-Means? (Pasos)

1. **Inicializaci√≥n**:

   * Escoge **K puntos aleatorios** como **centroides iniciales** (uno por cl√∫ster).

2. **Asignaci√≥n de cl√∫sters**:

   * Cada punto se asigna al cl√∫ster **m√°s cercano** (usando, por ejemplo, distancia euclidiana).

3. **Actualizaci√≥n de centroides**:

   * Se recalcula el **centroide** de cada cl√∫ster como el **promedio** de los puntos asignados a √©l.

4. **Repetir**:

   * Repite los pasos 2 y 3 hasta que:

     * Los centroides **ya no cambian significativamente**, o
     * Se alcanza un n√∫mero m√°ximo de iteraciones.

### üìà Ejemplo visual

Imagina un conjunto de puntos dispersos. K-Means:

* Coloca 3 puntos aleatorios como centroides (si K = 3).
* Agrupa los puntos seg√∫n cercan√≠a a esos centroides.
* Recalcula los centroides.
* Repite hasta que las posiciones de los centroides se estabilicen.

### ‚úÖ Ventajas

* **F√°cil de entender e implementar.**
* Funciona bien con cl√∫sters **esf√©ricos y separados**.
* **R√°pido** incluso con grandes conjuntos de datos.

### ‚ö†Ô∏è Limitaciones

* Tienes que **definir K** previamente.
* Es sensible a:

  * **Outliers**
  * **Inicializaci√≥n aleatoria** (puede converger a soluciones sub√≥ptimas).
* Asume que los cl√∫sters son de forma redonda y de tama√±o similar.

### üîç ¬øCu√°ndo usarlo?

* Segmentaci√≥n de clientes.
* Agrupamiento de im√°genes.
* An√°lisis exploratorio de datos.

**Lecturas recomendadas** 

[Visualizing K-Means Clustering](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/)

[Clustering comparison | Cartography Playground](https://cartography-playground.gitlab.io/playgrounds/clustering-comparison/)

## ¬øCu√°ndo usar K-means?

**K-means** es un algoritmo de aprendizaje no supervisado usado principalmente para **clustering** (agrupamiento). Es √∫til cuando se desea agrupar elementos en subconjuntos similares **sin etiquetas previas**.

### üìå Cu√°ndo usar K-means:

1. **Cuando no hay etiquetas (unsupervised learning)**:
   Tienes datos sin categor√≠as asignadas y deseas encontrar patrones naturales en ellos.

2. **Cuando esperas grupos esf√©ricos y bien separados**:
   K-means funciona mejor cuando los clusters tienen forma circular o esf√©rica (por la forma como calcula distancias).

3. **Cuando conoces o puedes estimar el n√∫mero de clusters (k)**:
   Es necesario definir cu√°ntos grupos esperas obtener.

4. **Cuando los datos no tienen muchos outliers**:
   K-means es sensible a valores at√≠picos porque usa medias para agrupar.

5. **Para segmentaci√≥n de clientes, compresi√≥n de im√°genes, detecci√≥n de patrones, etc.**

### ‚ùå No se recomienda K-means cuando:

* Los clusters tienen formas irregulares o distintos tama√±os.
* Hay muchos valores at√≠picos.
* No tienes idea del n√∫mero adecuado de grupos (aunque puedes usar m√©todos como el codo/elbow para estimarlo).

## Implementando K-means

A continuaci√≥n te muestro un ejemplo paso a paso de **c√≥mo implementar K-means en Python** usando `scikit-learn`.

### ‚úÖ **Paso 1: Importar librer√≠as necesarias**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
```

### ‚úÖ **Paso 2: Generar datos de ejemplo**

```python
# Creamos un conjunto de datos sint√©tico con 3 clusters
X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)

# Visualizamos los datos
plt.scatter(X[:, 0], X[:, 1], s=50)
plt.title("Datos sin clasificar")
plt.show()
```

### ‚úÖ **Paso 3: Aplicar K-means**

```python
# Creamos y ajustamos el modelo
kmeans = KMeans(n_clusters=3, random_state=0)
kmeans.fit(X)

# Obtenemos las predicciones (a qu√© grupo pertenece cada punto)
y_kmeans = kmeans.predict(X)

# Centros de los clusters
centros = kmeans.cluster_centers_
```

### ‚úÖ **Paso 4: Visualizar los resultados**

```python
# Graficamos los datos clasificados por cluster
plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')

# Mostramos los centros de cada cluster
plt.scatter(centros[:, 0], centros[:, 1], c='red', s=200, alpha=0.75, marker='X')
plt.title("Resultados del clustering con K-means")
plt.show()
```

### ‚úÖ **Paso 5 (Opcional): Evaluar el n√∫mero √≥ptimo de clusters (m√©todo del codo)**

```python
inertia = []

for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(X)
    inertia.append(kmeans.inertia_)

plt.plot(range(1, 10), inertia, marker='o')
plt.xlabel('N√∫mero de clusters')
plt.ylabel('Inercia')
plt.title('M√©todo del Codo')
plt.show()
```

**Archivos de la clase**

[clustering-kmeans.ipynb](https://static.platzi.com/media/public/uploads/clustering_kmeans_1aa7190f-894b-4ea5-b457-24fb419c7226.ipynb)

**Lecturas recomendadas**

[sklearn.cluster.KMeans ‚Äî scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)

[clustering_kmeans.ipynb - Google Drive](https://drive.google.com/file/d/1MqZPEldivmSm0l3coh9APCDrA267sjKj/view?usp=sharing)

## Encontrando K

Para **encontrar el n√∫mero √≥ptimo de clusters (K)** al usar K-Means, existen varios m√©todos. Aqu√≠ te explico los principales y te doy ejemplos pr√°cticos para que los implementes f√°cilmente:

### ‚úÖ 1. **Elbow Method (M√©todo del Codo)**

### ¬øQu√© hace?

Mide la **inercia** (dentro del grupo de errores cuadrados - SSE) para distintos valores de K y busca el "codo" donde la mejora se estabiliza.

### C√≥digo:

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# Datos simulados para el ejemplo
X, _ = make_blobs(n_samples=300, centers=5, random_state=42)

inertia = []

K_range = range(1, 11)
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X)
    inertia.append(kmeans.inertia_)

plt.plot(K_range, inertia, 'bo-')
plt.xlabel('N√∫mero de Clusters (k)')
plt.ylabel('Inercia')
plt.title('M√©todo del Codo')
plt.show()
```

> üîç Busca el punto donde la curva se "dobla", como un codo.

### ‚úÖ 2. **Silhouette Score**

### ¬øQu√© hace?

Mide qu√© tan bien separados est√°n los clusters. Cuanto mayor el valor (cerca de 1), mejor separados y definidos est√°n.

### C√≥digo:

```python
from sklearn.metrics import silhouette_score

silhouette_scores = []

K_range = range(2, 11)  # Comienza desde 2 porque Silhouette no est√° definido para k=1
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    labels = kmeans.fit_predict(X)
    score = silhouette_score(X, labels)
    silhouette_scores.append(score)

plt.plot(K_range, silhouette_scores, 'go-')
plt.xlabel('N√∫mero de Clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Puntaje de Silhouette')
plt.show()
```

> ‚úÖ El valor √≥ptimo de `k` es donde el **Silhouette Score es m√°s alto**.

### ‚úÖ 3. **Usando Yellowbrick (si lo logras instalar)**

```python
from yellowbrick.cluster import KElbowVisualizer
from sklearn.cluster import KMeans

model = KMeans(random_state=42)
visualizer = KElbowVisualizer(model, k=(1,10), metric='silhouette', timings=False)
visualizer.fit(X)
visualizer.show()
```

### ¬øQu√© m√©todo elegir?

| M√©todo           | Ideal cuando...                                                             |
| ---------------- | --------------------------------------------------------------------------- |
| Elbow            | Quieres ver el punto donde el "beneficio" se estabiliza                     |
| Silhouette Score | Quieres evaluar la calidad de los clusters                                  |
| Yellowbrick      | Quieres automatizar visualmente el an√°lisis (requiere instalaci√≥n correcta) |

**Archivos de la clase**

[clustering-kmeans.ipynb](https://static.platzi.com/media/public/uploads/clustering_kmeans_e52e3f11-b422-4522-86b8-693f0cd189b8.ipynb)

**Lecturas recomendadas**

[clustering_kmeans.ipynb - Google Drive](https://drive.google.com/file/d/1MqZPEldivmSm0l3coh9APCDrA267sjKj/view?usp=sharing)

## Evaluando resultados de K-means

Una vez que ejecutas el algoritmo K-Means y encuentras el valor √≥ptimo de **K**, es fundamental **evaluar los resultados** para asegurarte de que los clusters tengan sentido. Aqu√≠ tienes **formas efectivas de evaluar K-Means**:

### ‚úÖ 1. **Silhouette Score (Evaluaci√≥n num√©rica)**

### ¬øQu√© mide?

* Qu√© tan **bien separados** y **cohesivos** est√°n los clusters.
* Va de `-1` (malo) a `+1` (muy bueno).

### C√≥digo:

```python
from sklearn.metrics import silhouette_score

# Asumiendo que ya tienes `X` (datos) y `kmeans.labels_` (etiquetas)
score = silhouette_score(X, kmeans.labels_)
print(f"Silhouette Score: {score:.3f}")
```

> ‚úÖ Un score cerca de 1 es excelente. Cerca de 0 indica clusters solapados. Negativo es mal clustering.

### ‚úÖ 2. **Visualizaci√≥n de Clusters (2D)**

### ¬øPara qu√© sirve?

* Visualmente valida si los grupos son distinguibles.
* Solo se puede usar si tienes 2 o 3 dimensiones, o aplicas reducci√≥n como PCA.

### C√≥digo:

```python
import matplotlib.pyplot as plt

# Asumiendo que usaste kmeans = KMeans(n_clusters=k)
plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],
            s=300, c='red', marker='X', label='Centroides')
plt.title("Visualizaci√≥n de Clusters K-Means")
plt.legend()
plt.show()
```

### ‚úÖ 3. **Matriz de confusi√≥n (si tienes etiquetas reales)**

> Si est√°s haciendo clustering sobre datos **etiquetados (supervisado)**, puedes evaluar con exactitud y m√©tricas de clasificaci√≥n:

```python
from sklearn.metrics import confusion_matrix
import seaborn as sns

# Supongamos que tienes y_true (etiquetas reales) y y_pred (kmeans.labels_)
sns.heatmap(confusion_matrix(y_true, kmeans.labels_), annot=True, fmt="d", cmap="Blues")
plt.xlabel("Etiquetas de KMeans")
plt.ylabel("Etiquetas Reales")
plt.title("Matriz de Confusi√≥n")
plt.show()
```

### ‚úÖ 4. **Silhouette Visualizer (Yellowbrick - opcional)**

```python
from yellowbrick.cluster import SilhouetteVisualizer

model = KMeans(n_clusters=k, random_state=42)
visualizer = SilhouetteVisualizer(model)
visualizer.fit(X)
visualizer.show()
```

> üìå Esto muestra la forma y consistencia de cada cluster.

### ‚úÖ 5. **Comparaci√≥n Intra vs Inter Cl√∫ster**

* **Intra-cluster distance**: qu√© tan compactos son los puntos de cada cluster.
* **Inter-cluster distance**: qu√© tan separados est√°n los centroides.

Si lo deseas, puedo ayudarte a programar un peque√±o an√°lisis de estas m√©tricas.

## ¬øQu√© es hierarchical clustering y c√≥mo funciona?

El **Hierarchical Clustering** (o **clustering jer√°rquico**) es un algoritmo de agrupamiento **no supervisado** que construye una jerarqu√≠a de clusters, es decir, una estructura en forma de **√°rbol** que muestra c√≥mo se agrupan los datos.

### üîç ¬øC√≥mo funciona?

Existen dos enfoques principales:

#### 1. **Agglomerative (aglomerativo)** ‚Äì de abajo hacia arriba üëá

* **El m√°s com√∫n**.
* Cada punto de datos comienza como su propio cluster.
* En cada paso, los **dos clusters m√°s cercanos se fusionan**.
* Esto se repite hasta que todos los puntos est√°n en un solo cluster o se cumple un criterio.

#### 2. **Divisive (divisivo)** ‚Äì de arriba hacia abajo üëÜ

* Todos los datos comienzan en un √∫nico gran cluster.
* En cada paso, se **divide el cluster m√°s grande** hasta que cada punto es su propio cluster.

### üß† ¬øC√≥mo decide qu√© clusters unir o dividir?

El algoritmo usa una **m√©trica de distancia** entre clusters. Algunas comunes son:

| M√©todo     | Qu√© mide                                                      |
| ---------- | ------------------------------------------------------------- |
| `single`   | Distancia m√≠nima entre puntos de clusters                     |
| `complete` | Distancia m√°xima entre puntos de clusters                     |
| `average`  | Promedio de todas las distancias                              |
| `ward`     | Minimiza la varianza total dentro de los clusters (muy usada) |

### üìà Representaci√≥n: Dendrograma

El resultado se visualiza como un **dendrograma**, un diagrama en forma de √°rbol que muestra c√≥mo se fusionan (o dividen) los clusters a medida que cambia la distancia.

```python
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

linked = linkage(X, method='ward')  # o 'single', 'average', etc.
dendrogram(linked)
plt.title("Dendrograma de Clustering Jer√°rquico")
plt.xlabel("Puntos de datos")
plt.ylabel("Distancia")
plt.show()
```

### ‚úÖ ¬øCu√°ndo usarlo?

* Cuando quieres **ver la estructura jer√°rquica** de los datos.
* Cuando el n√∫mero de clusters **no es conocido de antemano**.
* Para datos con **pocos puntos** (porque no escala bien con muchos datos).

### ‚ùå Desventajas

* **Lento** en datasets grandes (`O(n^3)`).
* Puede ser **sensible a ruido y outliers**.
* No permite reorganizar clusters una vez creados.

## ¬øCu√°ndo usar hierarchical clustering?

Usar **clustering jer√°rquico** (`Hierarchical Clustering`) es recomendable en los siguientes escenarios:

### ‚úÖ **CU√ÅNDO USARLO**

#### 1. **Cuando no sabes cu√°ntos clusters hay**

* El dendrograma te permite **explorar la estructura** y **elegir el n√∫mero de clusters visualmente** (cortando el √°rbol).
* Ideal para **descubrimiento exploratorio**.

#### 2. **Cuando quieres una visi√≥n jer√°rquica de los datos**

* Si te interesa **ver relaciones padre-hijo** entre grupos (subgrupos dentro de grupos m√°s grandes), este m√©todo es ideal.
* Ejemplo: taxonom√≠a biol√≥gica, estructura de carpetas, segmentaci√≥n de clientes multinivel.

#### 3. **Para datasets peque√±os o medianos**

* Funciona bien con **menos de \~1,000-5,000 puntos**. M√°s all√° de eso, puede volverse muy lento y demandante en memoria.
* Es mejor para an√°lisis en profundidad que para producci√≥n a gran escala.

#### 4. **Cuando los clusters no son esf√©ricos ni del mismo tama√±o**

* A diferencia de K-means, que asume clusters circulares del mismo tama√±o, el clustering jer√°rquico **no impone esa suposici√≥n**.

#### 5. **Cuando necesitas interpretar los resultados**

* El dendrograma es **intuitivo y visualmente explicativo**, muy √∫til en reportes o an√°lisis descriptivos.

### ‚ùå **CU√ÅNDO EVITARLO**

| Situaci√≥n                                | Por qu√© evitarlo                                                      |
| ---------------------------------------- | --------------------------------------------------------------------- |
| Dataset muy grande                       | Tiene **complejidad O(n¬≥)** y **memoria O(n¬≤)**.                      |
| Necesitas clasificar nuevos datos r√°pido | No es **incremental** ni r√°pido para nuevos puntos.                   |
| Tienes muchos outliers                   | Puede generar **clusters distorsionados** si no hay preprocesamiento. |

### üîÑ Alternativas en esos casos

| Necesidad                            | Alternativa                    |
| ------------------------------------ | ------------------------------ |
| Escalabilidad                        | K-means, MiniBatchKMeans       |
| Detecci√≥n de formas arbitrarias      | DBSCAN, HDBSCAN                |
| Robustez a ruido                     | DBSCAN                         |
| Clasificaci√≥n r√°pida de nuevos datos | K-means o modelos supervisados |

## Implementando hierarchical clustering

Aqu√≠ tienes una implementaci√≥n completa de **Hierarchical Clustering (Clustering Jer√°rquico)** con `scikit-learn`, usando un dataset sint√©tico y visualizando tanto los clusters como el dendrograma:

### ‚úÖ PASO A PASO: Hierarchical Clustering con `scikit-learn`

#### 1. Instalar librer√≠as (si no las tienes):

```bash
pip install matplotlib seaborn scipy scikit-learn
```
#### 2. C√≥digo completo:

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# Configurar estilo gr√°fico
sns.set(style="whitegrid")

# Generar datos artificiales
X, y_true = make_blobs(n_samples=150, centers=4, cluster_std=1.0, random_state=42)

# Estandarizar (muy recomendable)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Crear y ajustar modelo de clustering jer√°rquico
hc = AgglomerativeClustering(n_clusters=4, linkage='ward')
clusters = hc.fit_predict(X_scaled)

# Visualizar los clusters
plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='viridis', s=50)
plt.title("Clusters jer√°rquicos")
plt.xlabel("Caracter√≠stica 1")
plt.ylabel("Caracter√≠stica 2")
plt.show()
```

#### 3. Visualizar el dendrograma (opcional, pero muy √∫til):

```python
# Calcular matriz de enlaces
linked = linkage(X_scaled, method='ward')

# Graficar dendrograma
plt.figure(figsize=(10, 6))
dendrogram(linked, truncate_mode='level', p=5)  # p=5 muestra los niveles superiores
plt.title("Dendrograma")
plt.xlabel("Puntos de datos")
plt.ylabel("Distancia")
plt.show()
```

### üîç Opciones importantes en `AgglomerativeClustering`:

| Par√°metro    | Descripci√≥n                                                     |
| ------------ | --------------------------------------------------------------- |
| `n_clusters` | N√∫mero de clusters finales                                      |
| `linkage`    | Tipo de enlace: `'ward'`, `'complete'`, `'average'`, `'single'` |
| `affinity`   | M√©trica de distancia (solo si `linkage ‚â† 'ward'`)               |

**Archivos de la clase**

[hierachical-clustering.ipynb](https://static.platzi.com/media/public/uploads/hierachical_clustering_e27a7035-5973-4cf5-be54-b7d27bd9f351.ipynb)

**Lecturas recomendadas**

[sklearn.cluster.AgglomerativeClustering ‚Äî scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)

[hierachical_clustering.ipynb - Google Drive](https://drive.google.com/file/d/1luOzJwPqgSfEz2PHTg3PufcOyHP0imnO/view?usp=sharing)

## Evaluando resultados de hierarchical clustering

Para **evaluar los resultados del clustering jer√°rquico (hierarchical clustering)**, puedes aplicar m√©tricas internas similares a las usadas en otros algoritmos como K-means. Aqu√≠ te muestro c√≥mo hacerlo y con qu√© herramientas:

### üìä **1. Silhouette Score**

Mide qu√© tan bien est√°n separados los cl√∫steres:

```python
from sklearn.metrics import silhouette_score

score = silhouette_score(X_scaled, y_hc)
print(f"Silhouette Score: {score:.3f}")
```

* **Rango:** de -1 a 1. Mientras m√°s cercano a 1, mejor definidos est√°n los grupos.

### üßÆ **2. Calinski-Harabasz Index**

Mide la dispersi√≥n entre cl√∫steres frente a la dispersi√≥n dentro del cl√∫ster:

```python
from sklearn.metrics import calinski_harabasz_score

score = calinski_harabasz_score(X_scaled, y_hc)
print(f"Calinski-Harabasz Index: {score:.2f}")
```

* **Interpretaci√≥n:** cuanto mayor el valor, mejor separados est√°n los cl√∫steres.

### üî¢ **3. Davies-Bouldin Index**

Mide la similitud entre cl√∫steres; valores m√°s bajos son mejores:

```python
from sklearn.metrics import davies_bouldin_score

score = davies_bouldin_score(X_scaled, y_hc)
print(f"Davies-Bouldin Index: {score:.3f}")
```

### üå≥ **4. Visualizaci√≥n con dendrograma**

Si usas `scipy`, puedes generar un dendrograma para inspecci√≥n visual:

```python
import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title('Dendrograma')
plt.xlabel('Muestras')
plt.ylabel('Distancia')
plt.show()
```

### ‚úÖ Conclusi√≥n

| M√©trica                 | ¬øQu√© mide?                    | Ideal       |
| ----------------------- | ----------------------------- | ----------- |
| Silhouette Score        | Separaci√≥n y compactaci√≥n     | Cercano a 1 |
| Calinski-Harabasz Index | Dispersi√≥n entre/intra grupos | Alto        |
| Davies-Bouldin Index    | Similitud entre cl√∫steres     | Bajo        |

**Lecturas recomendadas**

[Selecting the number of clusters with silhouette analysis on KMeans clustering ‚Äî scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)

[hierachical_clustering.ipynb - Google Drive](https://drive.google.com/file/d/1luOzJwPqgSfEz2PHTg3PufcOyHP0imnO/view?usp=sharing)

## ¬øQu√© es DBSCAN y c√≥mo funciona?

**DBSCAN** (*Density-Based Spatial Clustering of Applications with Noise*) es un algoritmo de clustering basado en densidad. A diferencia de K-means o el clustering jer√°rquico, **no necesitas especificar el n√∫mero de cl√∫steres de antemano**, y es muy eficaz para encontrar **cl√∫steres de forma arbitraria y detectar ruido (outliers)**.

### üîß ¬øC√≥mo funciona DBSCAN?

Se basa en dos par√°metros principales:

1. **Œµ (epsilon):** el radio para considerar vecinos cercanos.
2. **minPts:** el n√∫mero m√≠nimo de puntos para formar un cl√∫ster denso.

### üß± Clasificaci√≥n de puntos

DBSCAN clasifica los puntos en tres tipos:

* **Punto n√∫cleo:** tiene al menos `minPts` vecinos dentro de un radio `Œµ`.
* **Punto frontera:** est√° dentro del radio `Œµ` de un punto n√∫cleo, pero no tiene suficientes vecinos para ser n√∫cleo.
* **Ruido (outlier):** no es n√∫cleo ni frontera.

### üîÑ Algoritmo paso a paso:

1. Elige un punto no visitado.
2. Si tiene suficientes vecinos dentro de `Œµ`, crea un nuevo cl√∫ster.
3. Expande el cl√∫ster agregando todos los puntos densamente conectados.
4. Si no tiene suficientes vecinos, m√°rcalo como **ruido**.
5. Repite hasta visitar todos los puntos.

### üü¢ Ventajas de DBSCAN

* No necesita saber el n√∫mero de cl√∫steres.
* Puede detectar **formas complejas** y **outliers**.
* Robusto al ruido.

### üî¥ Desventajas

* Dif√≠cil de elegir los par√°metros √≥ptimos `Œµ` y `minPts`.
* No funciona bien si los cl√∫steres tienen **densidades muy diferentes**.
* Menos eficiente en conjuntos de datos muy grandes o de alta dimensi√≥n.

### üìå Ejemplo visual

Sup√≥n un conjunto de datos con dos cl√∫steres curvados y algo de ruido. K-means probablemente divida mal los cl√∫steres porque supone formas circulares. DBSCAN, en cambio, los detecta correctamente y marca el ruido.

**Lecturas recomendadas**

[Visualizing DBSCAN Clustering](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)

## ¬øCu√°ndo usar DBSCAN?

Puedes usar **DBSCAN** cuando:

### ‚úÖ **1. No conoces el n√∫mero de cl√∫steres de antemano**

* DBSCAN detecta autom√°ticamente cu√°ntos cl√∫steres hay (siempre que la densidad lo permita).
* Ideal para exploraci√≥n de datos sin conocimiento previo.

### ‚úÖ **2. Quieres detectar *outliers* o ruido**

* DBSCAN clasifica naturalmente los puntos aislados como **ruido**, lo que es √∫til en an√°lisis de anomal√≠as o fraudes.

### ‚úÖ \**3. Tus cl√∫steres tienen formas **no circulares** o **arbitrarias***

* A diferencia de K-means, que funciona bien solo con formas esf√©ricas, DBSCAN puede detectar cl√∫steres **curvados, alargados o irregulares**.

### ‚úÖ **4. Tus datos tienen densidades consistentes**

* Si los cl√∫steres tienen **densidades similares**, DBSCAN puede separarlos bien.

### üö´ **Cu√°ndo **no** usar DBSCAN**

* Si los cl√∫steres tienen **densidades muy diferentes** ‚Üí DBSCAN puede fallar en separar bien los grupos.
* En **altas dimensiones**, la distancia euclidiana pierde significado ‚Üí puede que `Œµ` no funcione bien.
* Si necesitas explicaciones claras de los cl√∫steres ‚Üí K-means o clustering jer√°rquico pueden ser m√°s interpretables.

### üìå Ejemplos de aplicaci√≥n:

* Detecci√≥n de fraudes bancarios (puntos at√≠picos).
* An√°lisis de tr√°fico GPS (trayectorias densas vs. aisladas).
* Agrupaci√≥n de tweets o noticias similares (previo uso de reducci√≥n de dimensionalidad).
* Im√°genes satelitales donde los objetos tienen formas irregulares.

## Implementando DBSCAN

Aqu√≠ tienes una implementaci√≥n pr√°ctica de **DBSCAN** en Python usando `scikit-learn`, con visualizaci√≥n incluida:

### ‚úÖ **Paso 1: Cargar y preparar los datos**

Usaremos datos simulados con formas no circulares para mostrar la ventaja de DBSCAN.

```python
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

# Crear un conjunto de datos de ejemplo (dos medias lunas)
X, y = make_moons(n_samples=300, noise=0.1, random_state=42)

# Visualizar
plt.scatter(X[:, 0], X[:, 1])
plt.title("Datos de ejemplo: dos medias lunas")
plt.show()
```

### ‚úÖ **Paso 2: Aplicar DBSCAN**

```python
from sklearn.cluster import DBSCAN

# Crear modelo DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=5)  # Puedes ajustar estos par√°metros

# Ajustar modelo
y_dbscan = dbscan.fit_predict(X)

# Visualizar resultado
plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma')
plt.title("Cl√∫steres encontrados con DBSCAN")
plt.show()
```

### ‚öôÔ∏è **Par√°metros importantes:**

* `eps`: distancia m√°xima entre dos puntos para que uno sea considerado vecino del otro.
* `min_samples`: n√∫mero m√≠nimo de puntos para formar un cl√∫ster denso (incluye el punto central).

### ‚úÖ **Paso 3 (opcional): Identificar *outliers***

```python
# Los puntos con etiqueta -1 son considerados ruido
import numpy as np

outliers = X[y_dbscan == -1]

plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma')
plt.scatter(outliers[:, 0], outliers[:, 1], c='red', label='Outliers')
plt.legend()
plt.title("DBSCAN con detecci√≥n de outliers")
plt.show()
```

**Archivos de la clase**

[dbscan.ipynb](https://static.platzi.com/media/public/uploads/dbscan_f460079b-4fed-429f-9fdd-58b6aad8eb9b.ipynb)

**Lecturas recomendadas**

[sklearn.cluster.DBSCAN ‚Äî scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[dbscan.ipynb - Google Drive](https://drive.google.com/file/d/1e0LRhK3k00yxHZy5Q9eXS0jTU0GdnYTq/view?usp=sharing)

## Encontrar h√≠per-par√°metros

Para encontrar los **hiperpar√°metros √≥ptimos de DBSCAN**, especialmente `eps` (radio de vecindad), podemos usar un gr√°fico de **distancias de los vecinos m√°s cercanos**. Esto ayuda a detectar el valor de `eps` a partir de una "rodilla" o cambio brusco en la curva.

### ‚úÖ **Paso 1: Graficar las distancias del vecino m√°s cercano**

```python
from sklearn.neighbors import NearestNeighbors
import numpy as np
import matplotlib.pyplot as plt

# Ajustar vecinos
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(X)
distances, indices = neighbors_fit.kneighbors(X)

# Ordenar las distancias del 4¬∫ vecino (porque n_neighbors=5)
distances = np.sort(distances[:, 4])  # el √≠ndice es n_neighbors - 1

# Graficar
plt.plot(distances)
plt.ylabel("Distancia al 4¬∫ vecino m√°s cercano")
plt.xlabel("Puntos ordenados por distancia")
plt.title("Curva de codo para encontrar eps")
plt.grid(True)
plt.show()
```

üß† **Interpretaci√≥n**: Busca el punto donde la pendiente del gr√°fico cambia bruscamente ("codo"). Ese valor es una buena estimaci√≥n para `eps`.

### ‚úÖ **Paso 2: Aplicar DBSCAN con eps hallado**

Sup√≥n que el gr√°fico muestra un codo en `eps ‚âà 0.25`:

```python
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(eps=0.25, min_samples=5)
y_dbscan = dbscan.fit_predict(X)

# Visualizar
plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma')
plt.title("DBSCAN con eps ajustado")
plt.show()
```

### ‚úÖ **Paso 3 (opcional): Evaluar calidad con Silhouette Score**

```python
from sklearn.metrics import silhouette_score

# Aseg√∫rate de que haya m√°s de 1 cl√∫ster
n_clusters = len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)
if n_clusters > 1:
    score = silhouette_score(X, y_dbscan)
    print(f"Silhouette Score: {score:.3f}")
else:
    print("DBSCAN no encontr√≥ suficientes cl√∫steres para calcular Silhouette Score.")
```

**Lecturas recomendadas**

[sklearn.cluster.DBSCAN ‚Äî scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)

[dbscan.ipynb - Google Drive](https://drive.google.com/file/d/1e0LRhK3k00yxHZy5Q9eXS0jTU0GdnYTq/view?usp=sharing)

## Evaluando resultados de DBSCAN

Para **evaluar los resultados de DBSCAN**, puedes utilizar diversas m√©tricas y visualizaciones. Aqu√≠ te explico los enfoques m√°s comunes:

### ‚úÖ 1. **Visualizaci√≥n de los cl√∫steres**

La forma m√°s directa de evaluar DBSCAN es visualizar los cl√∫steres:

```python
import matplotlib.pyplot as plt

plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap='plasma')
plt.title("Resultados de DBSCAN")
plt.xlabel("Componente 1")
plt.ylabel("Componente 2")
plt.grid(True)
plt.show()
```

* Los **ruidos** ser√°n etiquetados como `-1`.
* Colores diferentes representan diferentes cl√∫steres.

### ‚úÖ 2. **Silhouette Score**

Eval√∫a qu√© tan bien est√° cada punto dentro de su cl√∫ster:

```python
from sklearn.metrics import silhouette_score

labels = y_dbscan
n_clusters = len(set(labels)) - (1 if -1 in labels else 0)

if n_clusters > 1:
    score = silhouette_score(X, labels)
    print(f"Silhouette Score: {score:.3f}")
else:
    print("No se pueden calcular m√©tricas: hay menos de 2 cl√∫steres.")
```

> üéØ **Interpretaci√≥n**: Cuanto m√°s cercano a 1, mejor. Valores < 0 indican mala asignaci√≥n.

### ‚úÖ 3. **N√∫mero de cl√∫steres y ruido**

Puedes revisar cu√°ntos cl√∫steres encontr√≥ DBSCAN y cu√°ntos puntos consider√≥ como ruido:

```python
import numpy as np

n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
n_noise = list(labels).count(-1)

print(f"N√∫mero de cl√∫steres encontrados: {n_clusters}")
print(f"N√∫mero de puntos de ruido: {n_noise}")
```

### ‚úÖ 4. **Confusi√≥n con etiquetas reales (si existen)**

Si tienes etiquetas verdaderas (`y_true`), puedes usar m√©tricas como *Adjusted Rand Index (ARI)* o *Homogeneity Score*:

```python
from sklearn.metrics import adjusted_rand_score, homogeneity_score

print("ARI:", adjusted_rand_score(y_true, y_dbscan))
print("Homogeneidad:", homogeneity_score(y_true, y_dbscan))
```

### ‚úÖ 5. **Silhouette Visualizer (opcional)**

Si tienes instalado `yellowbrick`, puedes usar un gr√°fico de silueta:

```python
from yellowbrick.cluster import SilhouetteVisualizer
from sklearn.cluster import DBSCAN

visualizer = SilhouetteVisualizer(DBSCAN(eps=0.3, min_samples=5))
visualizer.fit(X)
visualizer.show()
```

> Aseg√∫rate de que haya m√°s de un cl√∫ster para que esto funcione.

**Lecturas recomendadas**

[Selecting the number of clusters with silhouette analysis on KMeans clustering ‚Äî scikit-learn 1.1.3 documentation](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)

[dbscan.ipynb - Google Drive](https://drive.google.com/file/d/1e0LRhK3k00yxHZy5Q9eXS0jTU0GdnYTq/view?usp=sharing)

## Preparar datos para clusterizar

Preparar los datos adecuadamente es **clave para obtener buenos resultados de clustering**. Aqu√≠ tienes una gu√≠a paso a paso para preparar tus datos antes de aplicar algoritmos como K-means, DBSCAN o Hierarchical Clustering:

### üßπ 1. Cargar y explorar los datos

```python
import pandas as pd

df = pd.read_csv("tu_archivo.csv")  # o usa otro m√©todo de carga
print(df.head())
print(df.info())
```

### üîç 2. Seleccionar las columnas relevantes

El clustering se basa en distancias, por lo tanto, es importante **seleccionar solo las caracter√≠sticas num√©ricas** (o convertir las categ√≥ricas).

```python
features = df[['columna1', 'columna2', 'columna3']]  # ajusta seg√∫n tu dataset
```

### üßº 3. Limpiar datos

* **Eliminar nulos o imputar valores**
* **Eliminar duplicados si es necesario**

```python
features = features.dropna()
features = features.drop_duplicates()
```

### üßÆ 4. Escalar los datos (muy importante)

Los algoritmos de clustering **dependen de la escala** de los datos. Se recomienda usar `StandardScaler` o `MinMaxScaler`.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)
```

### ‚úÖ 5. (Opcional) Reducir dimensiones para visualizar

Si tienes m√°s de 2 dimensiones, puedes usar PCA o t-SNE para reducir a 2D y visualizar los cl√∫steres.

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
```

### üì¶ Resultado final listo para clusterizar

Ahora `X_scaled` (o `X_pca`) est√° listo para pasar a:

* `KMeans().fit(X_scaled)`
* `DBSCAN().fit(X_scaled)`
* `AgglomerativeClustering().fit(X_scaled)`

**Lecturas recomendadas**

[Unsupervised Learning on Country Data | Kaggle](https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data)

[proyecto_countries_clustering.ipynb - Google Drive](https://drive.google.com/file/d/10ybq0nOtmjpYyy6tLie1StUbZ3YXxGf_/view?usp=sharing)

## Aplicando PCA para clustering

Aplicar **PCA (An√°lisis de Componentes Principales)** antes del clustering puede ayudarte a:

* **Reducir la dimensionalidad** del dataset.
* **Eliminar ruido** y redundancia.
* **Mejorar visualizaci√≥n** de los grupos.
* Hacer que algoritmos como **K-means** o **DBSCAN** funcionen mejor si los datos tienen muchas dimensiones.

### üîß Pasos para aplicar PCA antes de clustering:

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 1. Escalar los datos
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_country.drop('country', axis=1))  # Aseg√∫rate que 'country' existe

# 2. Aplicar PCA
pca = PCA(n_components=2)  # Elige 2 o m√°s componentes para visualizaci√≥n o clustering
X_pca = pca.fit_transform(X_scaled)

# 3. Visualizaci√≥n opcional
import matplotlib.pyplot as plt

plt.figure(figsize=(8,6))
plt.scatter(X_pca[:, 0], X_pca[:, 1])
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('Datos reducidos con PCA')
plt.grid(True)
plt.show()
```

### üß™ Usar PCA con K-means (ejemplo):

```python
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, random_state=42)
clusters = kmeans.fit_predict(X_pca)

# Visualizar clustering
plt.figure(figsize=(8,6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('Clustering con K-means despu√©s de PCA')
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()
```

### ‚úÖ ¬øCu√°ntos componentes usar?

Puedes revisar cu√°nta **varianza explica cada componente**:

```python
print(pca.explained_variance_ratio_)
print("Varianza acumulada:", sum(pca.explained_variance_ratio_))
```

Si los primeros 2 o 3 componentes explican >80% de la varianza, puedes usarlos sin perder mucha informaci√≥n.

**Archivos de la clase** 

[proyecto-countries-clustering.ipynb](https://static.platzi.com/media/public/uploads/proyecto_countries_clustering_95a989f9-43df-4a04-a70d-842608fa304d.ipynb)

**Lecturas recomendadas**

[Principle Components Analysis - Learn Machine Learning The fun way](https://mlplaygrounds.com/machine/learning/PCA.html)

[Platzi: Cursos online profesionales de tecnolog√≠a](https://platzi.com/clases/2353-estadistica-descriptiva/38412-pca-analisis-de-componentes-principales/)

[proyecto_countries_clustering.ipynb - Google Drive](https://drive.google.com/file/d/10ybq0nOtmjpYyy6tLie1StUbZ3YXxGf_/view?usp=sharing)

## Resolviendo con K-means

Perfecto, resolver un problema de **clustering** con **K-means** implica los siguientes pasos clave:

### ‚úÖ **1. Preparar los datos**

Aseg√∫rate de escalar los datos y eliminar columnas no num√©ricas como `'country'`.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_country.drop('country', axis=1))  # Solo si 'country' est√° presente
```

### ‚úÖ **2. Elegir el n√∫mero de clusters (K)**

Puedes usar el **m√©todo del codo** para encontrar un buen valor de K:

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia = []
K_range = range(1, 11)

for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)

plt.plot(K_range, inertia, marker='o')
plt.xlabel('N√∫mero de clusters (K)')
plt.ylabel('Inercia')
plt.title('M√©todo del codo')
plt.grid(True)
plt.show()
```

> Busca el "codo" en la curva: el punto donde la inercia deja de disminuir r√°pidamente.

### ‚úÖ **3. Ajustar el modelo con el K elegido**

```python
k = 3  # por ejemplo, seg√∫n el codo
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_scaled)
```

### ‚úÖ **4. Visualizar resultados (opcional con PCA)**

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis')
plt.xlabel('PCA 1')
plt.ylabel('PCA 2')
plt.title('Clustering con K-means')
plt.colorbar(label='Cluster')
plt.grid(True)
plt.show()
```

### ‚úÖ **5. Analizar los grupos**

Puedes agregar los resultados de los clusters al DataFrame original:

```python
df_country['cluster'] = clusters
```

Luego analizar, por ejemplo, con `.groupby('cluster').mean()`.

**Lecturas recomendadas**

[Unsupervised Learning on Country Data | Kaggle](https://www.kaggle.com/datasets/rohan0301/unsupervised-learning-on-country-data)

[proyecto_countries_clustering.ipynb - Google Drive](https://drive.google.com/file/d/10ybq0nOtmjpYyy6tLie1StUbZ3YXxGf_/view?usp=sharing)

## Resolviendo con hierarchical clustering

Para resolver un problema de **segmentaci√≥n o clasificaci√≥n de veh√≠culos** (o cualquier conjunto de datos similar) usando **hierarchical clustering (clustering jer√°rquico)**, se siguen estos pasos generales. A continuaci√≥n te muestro un ejemplo completo con Python aplicado a un conjunto de veh√≠culos con caracter√≠sticas como precio, consumo, y tama√±o.

### ‚úÖ 1. **Importar librer√≠as**

```python
import pandas as pd
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.preprocessing import StandardScaler
```

### ‚úÖ 2. **Crear o cargar el dataset**

Ejemplo de veh√≠culos (puedes usar tus propios datos):

```python
data = {
    'Modelo': ['Onix', 'Kia Soluto', 'Swift', 'S-Cross', 'Picanto', 'K3 Cross', 'Kardian'],
    'Precio': [82320000, 69990000, 85000000, 125000000, 65000000, 120000000, 110000000],
    'Consumo_kmgl': [65, 60, 70, 55, 68, 58, 62],
    'Longitud_mm': [4474, 4300, 3840, 4300, 3595, 4365, 4115]
}
df = pd.DataFrame(data)
```

### ‚úÖ 3. **Normalizar los datos**

```python
scaler = StandardScaler()
X = scaler.fit_transform(df[['Precio', 'Consumo_kmgl', 'Longitud_mm']])
```

### ‚úÖ 4. **Aplicar Clustering Jer√°rquico**

```python
linked = linkage(X, method='ward')  # Tambi√©n puedes usar 'single', 'complete', 'average'
```

### ‚úÖ 5. **Graficar el dendrograma**

```python
plt.figure(figsize=(10, 6))
dendrogram(linked,
           labels=df['Modelo'].values,
           orientation='top',
           distance_sort='descending',
           show_leaf_counts=True)
plt.title('Dendrograma de veh√≠culos')
plt.xlabel('Modelos')
plt.ylabel('Distancia')
plt.show()
```

### ‚úÖ 6. **Formar grupos (clusters)**

Por ejemplo, 3 grupos:

```python
clusters = fcluster(linked, 3, criterion='maxclust')
df['Cluster'] = clusters
print(df.sort_values('Cluster'))
```

### üß† Interpretaci√≥n

Al formar 2‚Äì4 clusters, puedes analizar qu√© veh√≠culos est√°n en el mismo grupo y por qu√© (precio similar, tama√±o, consumo, etc). Por ejemplo:

* **Cluster 1**: Autos compactos y econ√≥micos (Picanto, Soluto)
* **Cluster 2**: Autos medianos con mejor equipamiento (Onix, Kardian)
* **Cluster 3**: SUV o crossover (S-Cross, K3 Cross)

**Lecturas recomendadas**

[proyecto_countries_clustering.ipynb - Google Drive](https://drive.google.com/file/d/10ybq0nOtmjpYyy6tLie1StUbZ3YXxGf_/view?usp=sharing)

## Resolviendo con DBSCAN

Resolver un problema con **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** es ideal cuando esperas encontrar **clusters de forma irregular** o quieres detectar **outliers** (valores at√≠picos) de forma autom√°tica. Aqu√≠ te muestro c√≥mo implementarlo paso a paso en Python, usando como ejemplo autos disponibles en Colombia bajo ciertos criterios.

### ‚úÖ 1. **Importar librer√≠as**

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
```

### ‚úÖ 2. **Datos de ejemplo**

Simulamos datos representativos de autos:

```python
data = {
    'Modelo': ['Onix', 'Kia Soluto', 'Swift', 'S-Cross', 'Picanto', 'K3 Cross', 'Kardian'],
    'Precio': [82320000, 69990000, 85000000, 125000000, 65000000, 120000000, 110000000],
    'Consumo_kmgl': [65, 60, 70, 55, 68, 58, 62],
    'Longitud_mm': [4474, 4300, 3840, 4300, 3595, 4365, 4115]
}
df = pd.DataFrame(data)
```

### ‚úÖ 3. **Normalizaci√≥n**

```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[['Precio', 'Consumo_kmgl', 'Longitud_mm']])
```

### ‚úÖ 4. **Aplicar DBSCAN**

```python
dbscan = DBSCAN(eps=1.0, min_samples=2)
labels = dbscan.fit_predict(X_scaled)
df['Cluster'] = labels
```

### ‚úÖ 5. **Visualizar resultados**

```python
plt.figure(figsize=(8, 5))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='Set1')
for i, modelo in enumerate(df['Modelo']):
    plt.annotate(modelo, (X_scaled[i, 0], X_scaled[i, 1]))
plt.title('Clusters con DBSCAN')
plt.xlabel('Precio (normalizado)')
plt.ylabel('Consumo (normalizado)')
plt.grid(True)
plt.show()
```

### ‚úÖ 6. **Interpretar resultados**

* El campo `Cluster` en `df` indica el grupo al que pertenece cada auto.
* Los valores `-1` son **outliers**: DBSCAN los identifica como puntos que no pertenecen a ning√∫n cluster.
* Puedes ajustar `eps` y `min_samples` para modificar la densidad m√≠nima requerida para formar un grupo.

### üîß Ajuste de Hiperpar√°metros

Para encontrar buenos valores de `eps` y `min_samples`, puedes usar:

```python
from sklearn.neighbors import NearestNeighbors

neighbors = NearestNeighbors(n_neighbors=2)
neighbors_fit = neighbors.fit(X_scaled)
distances, indices = neighbors_fit.kneighbors(X_scaled)

distances = np.sort(distances[:, 1])
plt.plot(distances)
plt.title('K-distance plot')
plt.xlabel('Puntos ordenados')
plt.ylabel('Distancia al 2¬∞ vecino m√°s cercano')
plt.grid(True)
plt.show()
```

Busca el ‚Äúcodo‚Äù en el gr√°fico para elegir el valor de `eps`.

## Resolviendo con DBSCAN (sin PCA)

Perfecto. Vamos a resolver un problema de **clustering con DBSCAN sin usar PCA**, es decir, aplicando el algoritmo directamente sobre los datos normalizados, tal como se recomienda en muchos escenarios pr√°cticos.

### üß™ **Paso a paso para clustering con DBSCAN (sin PCA)**

Supongamos que tienes un conjunto de datos `df` con variables como:

* Precio del veh√≠culo
* Consumo de combustible
* Dimensiones
* Otros atributos num√©ricos √∫tiles para el an√°lisis

### ‚úÖ 1. **Importar librer√≠as necesarias**

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
import seaborn as sns
```

### ‚úÖ 2. **Cargar o crear tus datos**

Aqu√≠ usamos un ejemplo simulado (puedes reemplazar por tu DataFrame real):

```python
df = pd.DataFrame({
    'Modelo': ['Kia Picanto', 'Renault Kwid', 'Chevrolet Onix', 'Suzuki Swift', 'Renault Kardian'],
    'Precio': [65000000, 58000000, 82300000, 85000000, 105000000],
    'Consumo_kmgl': [65, 70, 60, 68, 58],
    'Longitud_mm': [3595, 3731, 4474, 3840, 4115]
})
```

### ‚úÖ 3. **Preprocesamiento (normalizaci√≥n)**

Normalizamos solo las columnas num√©ricas (sin aplicar reducci√≥n de dimensionalidad):

```python
X = df[['Precio', 'Consumo_kmgl', 'Longitud_mm']]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### ‚úÖ 4. **Aplicar DBSCAN**

```python
dbscan = DBSCAN(eps=1.0, min_samples=2)
labels = dbscan.fit_predict(X_scaled)
df['Cluster'] = labels
```

### ‚úÖ 5. **Visualizaci√≥n 2D**

Como no usamos PCA, puedes graficar solo 2 dimensiones directamente:

```python
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=labels, palette='tab10', s=100)
for i, txt in enumerate(df['Modelo']):
    plt.annotate(txt, (X_scaled[i, 0]+0.02, X_scaled[i, 1]+0.02), fontsize=9)
plt.title("Clustering con DBSCAN (sin PCA)")
plt.xlabel("Precio (normalizado)")
plt.ylabel("Consumo (normalizado)")
plt.grid(True)
plt.show()
```

### ‚úÖ 6. **Resultados**

```python
print(df)
```

Esto te dar√° un DataFrame con los modelos y su **asignaci√≥n de cl√∫ster** o `-1` si fue identificado como **outlier**.

### üéØ Consejo: ¬øC√≥mo elegir el mejor `eps`?

Usa el m√©todo del gr√°fico de la ‚Äúk-distancia‚Äù:

```python
from sklearn.neighbors import NearestNeighbors

neighbors = NearestNeighbors(n_neighbors=2)
neighbors_fit = neighbors.fit(X_scaled)
distances, indices = neighbors_fit.kneighbors(X_scaled)
distances = np.sort(distances[:, 1])
plt.plot(distances)
plt.title("Gr√°fico de k-distancia")
plt.xlabel("Puntos ordenados")
plt.ylabel("Distancia al 2¬∞ vecino")
plt.grid(True)
plt.show()
```

El punto donde la curva se eleva bruscamente es un buen candidato para `eps`.

**Lecturas recomendadas**

[proyecto_countries_clustering.ipynb - Google Drive](https://drive.google.com/file/d/10ybq0nOtmjpYyy6tLie1StUbZ3YXxGf_/view?usp=sharing)

## Evaluaci√≥n resultados de distintos modelos de clustering

Evaluar los resultados de distintos **modelos de clustering** (K-means, DBSCAN, Hierarchical Clustering, etc.) es esencial para identificar cu√°l ofrece la mejor segmentaci√≥n seg√∫n tus datos. A diferencia de clasificaci√≥n supervisada, en clustering no tenemos etiquetas verdaderas, por lo que usamos **m√©tricas internas** o **evaluaciones visuales**.

### ‚úÖ Evaluaci√≥n de Modelos de Clustering

### üîπ 1. **Silhouette Score**

Mide cu√°n bien est√°n separados los cl√∫steres y cu√°n compactos son.

* Rango: **-1 a 1**
* Valores cercanos a **1** son mejores.
* Se puede usar con **KMeans**, **Hierarchical**, y **DBSCAN** (aunque este √∫ltimo puede tener `-1` como outliers).

```python
from sklearn.metrics import silhouette_score

score = silhouette_score(X_scaled, labels)
print(f'Silhouette Score: {score:.3f}')
```

> Nota: Para DBSCAN, aseg√∫rate de excluir los outliers (`labels != -1`), si es necesario.

### üîπ 2. **Davies-Bouldin Index**

Mide la dispersi√≥n intra-cl√∫ster y separaci√≥n inter-cl√∫ster.

* **Menor es mejor**
* Funciona con cualquier m√©todo de clustering.

```python
from sklearn.metrics import davies_bouldin_score

dbi = davies_bouldin_score(X_scaled, labels)
print(f'Davies-Bouldin Index: {dbi:.3f}')
```

### üîπ 3. **Calinski-Harabasz Index**

Cuantifica la varianza entre los cl√∫steres y dentro de los cl√∫steres.

* **Mayor es mejor**
* Bueno para comparar modelos con distintos `k`.

```python
from sklearn.metrics import calinski_harabasz_score

ch_score = calinski_harabasz_score(X_scaled, labels)
print(f'Calinski-Harabasz Index: {ch_score:.3f}')
```

### üîπ 4. **Comparaci√≥n visual 2D**

Reduce los datos a 2 dimensiones (por ejemplo, con PCA) y visualiza:

```python
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=labels, palette='tab10', s=100)
plt.title("Clustering Visual")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.grid(True)
plt.show()
```

Esto permite **comparar visualmente** c√≥mo los modelos separan los datos.

### üß™ Comparaci√≥n entre modelos

Aqu√≠ un resumen de c√≥mo podr√≠as comparar tres modelos:

| Modelo              | Silhouette Score | Davies-Bouldin | Calinski-Harabasz |
| ------------------- | ---------------- | -------------- | ----------------- |
| K-means (k=3)       | 0.52             | 0.88           | 137.6             |
| Hierarchical (ward) | 0.47             | 0.91           | 120.3             |
| DBSCAN (eps=1.2)    | 0.60             | 0.77           | 150.8             |

> Puedes construir esta tabla autom√°ticamente si guardas los resultados de cada modelo.

### üß† Consejo

* **K-means** funciona mejor con grupos esf√©ricos bien definidos.
* **Hierarchical** es √∫til si te interesa una estructura jer√°rquica (como dendrogramas).
* **DBSCAN** detecta outliers y cl√∫steres de forma arbitraria (ideal en datos con ruido).

## Proyecto final y cierre

**Lecturas recomendadas**

[Customer Personality Analysis | Kaggle](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis)