# Curso de √Ålgebra Lineal Aplicada para Machine Learning

## Descomposici√≥n de Matrices y Su Aplicaci√≥n en Machine Learning

La **descomposici√≥n de matrices** es una herramienta fundamental en **√°lgebra lineal aplicada al Machine Learning**, ya que permite simplificar c√°lculos complejos, reducir la dimensionalidad y extraer informaci√≥n estructural de los datos. A continuaci√≥n te explico los **principales tipos**, sus **aplicaciones** y ejemplos pr√°cticos.

### ‚úÖ ¬øQu√© es la Descomposici√≥n de Matrices?

Consiste en **descomponer una matriz en varios factores** o submatrices con propiedades especiales que facilitan:

* La soluci√≥n de sistemas de ecuaciones
* La reducci√≥n de dimensiones
* La compresi√≥n de datos
* La mejora del rendimiento en modelos de machine learning

### üîç Tipos Principales de Descomposici√≥n

### 1. **Descomposici√≥n LU (Lower-Upper)**

* Descompone una matriz cuadrada A en:

  $$
  A = L \cdot U
  $$

  donde `L` es triangular inferior y `U` es triangular superior.

* ‚úÖ **Aplicaciones**:

  * Resolver sistemas de ecuaciones lineales.
  * Acelerar algoritmos num√©ricos.

### 2. **Descomposici√≥n QR**

* Descompone A en:

  $$
  A = Q \cdot R
  $$

  donde `Q` es ortogonal (o unitario) y `R` es triangular superior.

* ‚úÖ **Aplicaciones**:

  * Soluciones num√©ricas estables.
  * Regresi√≥n lineal.

### 3. **Descomposici√≥n SVD (Singular Value Decomposition)**

* Factoriza A en:

  $$
  A = U \cdot \Sigma \cdot V^T
  $$

  donde `U` y `V` son ortogonales y `Œ£` contiene los valores singulares.

* ‚úÖ **Aplicaciones**:

  * **Reducci√≥n de dimensionalidad (PCA)**
  * **Recomendadores**
  * **Compresi√≥n de im√°genes**
  * **Detecci√≥n de patrones latentes**

### 4. **Descomposici√≥n Eig (de autovalores)**

* Para matrices cuadradas:

  $$
  A = V \cdot D \cdot V^{-1}
  $$

  donde `D` es diagonal (autovalores) y `V` contiene los autovectores.

* ‚úÖ **Aplicaciones**:

  * An√°lisis de componentes principales (PCA)
  * Estabilidad de sistemas
  * M√©todos espectrales

### üí° Ejemplo Pr√°ctico en Python (SVD con NumPy)

```python
import numpy as np

A = np.array([[3, 2], [2, 3]])
U, S, VT = np.linalg.svd(A)

print("Matriz U:\n", U)
print("Valores singulares Œ£:\n", S)
print("Matriz V^T:\n", VT)
```

### ü§ñ Aplicaciones en Machine Learning

| Tipo de descomposici√≥n | Aplicaci√≥n ML destacada                          |
| ---------------------- | ------------------------------------------------ |
| **SVD**                | Recomendadores, PCA                              |
| **LU / QR**            | Soluci√≥n eficiente de sistemas, regresi√≥n lineal |
| **Eig**                | PCA, clustering espectral                        |
| **NMF (No Negativa)**  | Modelado de temas (topic modeling)               |

### Resumen

#### ¬øPor qu√© es importante entender las matrices en data science?

Comprender el uso de las matrices en data science es fundamental para abordar problemas complejos y optimizar procesos. Las matrices permiten realizar transformaciones lineales, facilitando la manipulaci√≥n y el an√°lisis de datos en gran escala. En muchos casos, especialmente en √°reas como machine learning, entender las matrices es clave para mejorar la eficiencia computacional debido a la reducci√≥n de dimensiones y al manejo de datos de alta densidad.

#### ¬øQu√© conceptos previos necesitas?

Es crucial recordar ciertos conceptos que ser√°n tu base para avanzar en este curso. Entre estos:

- **Matrices e Identidad**: Comprender qu√© es una matriz y las operaciones b√°sicas que puedes realizar.
- **Inversa de una matriz cuadrada**: Saber c√≥mo calcularla y las condiciones bajo las cuales existe.

Estos fundamentos te permitir√°n ir m√°s all√° y aventurarte en el c√°lculo de autovalores y autovectores, y c√≥mo estos permiten descomponer una matriz. Adem√°s, entender√°s qu√© es el SVD y la descomposici√≥n en valores singulares.

#### ¬øC√≥mo se relaciona el √Ålgebra Lineal con Machine Learning?

La relaci√≥n del √°lgebra lineal con el machine learning es directa, ya que muchos de los algoritmos utilizados en esta √°rea requieren manipular y transformar grandes vol√∫menes de datos. Aqu√≠ algunos puntos clave:

- **Reducci√≥n de dimensionalidad:** Disminuir el n√∫mero de dimensiones puede llevar a procesos m√°s eficientes sin perder informaci√≥n significativa.
- **Optimizaci√≥n de algoritmos**: Al reducir dimensionalidades, disminuye el tiempo computacional necesario, lo cual es esencial cuando se manejan grandes conjuntos de datos.
- **Transformaciones lineales**: Las matrices permiten transformar y manipular datos eficazmente, lo que es crucial para entrenar modelos de machine learning.

Trabajar con matrices y entender su aplicaci√≥n pr√°ctica te dar√° ventaja al manejar sistemas de machine learning m√°s complejos, asegurando que tu enfoque sea tanto preciso como eficiente.

#### ¬øQu√© m√°s aprender√°s en este curso?

El prop√≥sito de este curso es ir m√°s all√° de los fundamentos y explorar temas avanzados de √°lgebra lineal aplicados a data science. Esto incluye:

- **C√°lculo de Pseudo-inversas**: O inversas generalizadas, √∫tiles en sistemas que no tienen una soluci√≥n √∫nica o bien definida.
- **Algoritmo PCA (An√°lisis de Componentes Principales)**: Este es un m√©todo muy utilizado para la reducci√≥n de dimensionalidad y an√°lisis exploratorio de datos.
- **Aplicaciones pr√°cticas**: Implementaci√≥n de estos conceptos en problemas reales, que te permitir√° ver en acci√≥n las t√©cnicas aprendidas.

Este curso est√° dise√±ado no solo para enriquecer tu conocimiento te√≥rico, sino para empoderarte a aplicar estas herramientas de manera efectiva en tus proyectos de ciencia de datos. ¬°Sigue adelante y descubre el potencial del √°lgebra lineal en el mundo del machine learning y data science!

## Transformaciones Lineales con Matrices en Python: Visualizaci√≥n y An√°lisis

Vamos a abordar el tema **Transformaciones Lineales con Matrices en Python** con un enfoque pr√°ctico: entender, visualizar y analizar c√≥mo una matriz puede transformar vectores en el plano.


### üß† ¬øQu√© es una transformaci√≥n lineal?

Una **transformaci√≥n lineal** es una funci√≥n que lleva vectores de un espacio a otro respetando suma y multiplicaci√≥n escalar. Se representa mediante **multiplicaci√≥n de una matriz por un vector**.

Si $A$ es una matriz y $\vec{v}$ un vector, entonces:

$$
\text{Transformaci√≥n: } T(\vec{v}) = A \cdot \vec{v}
$$

### üõ†Ô∏è Herramientas en Python

Usaremos:

```python
import numpy as np
import matplotlib.pyplot as plt
```

### üß™ Ejemplo 1: Transformaci√≥n en 2D

```python
# Vectores originales (cuadrado unitario)
original = np.array([
    [0, 0], [1, 0], [1, 1], [0, 1], [0, 0]
]).T

# Matriz de transformaci√≥n
A = np.array([
    [2, 1],
    [1, 3]
])

# Aplicar la transformaci√≥n
transformado = A @ original

# Visualizar
plt.figure(figsize=(6,6))
plt.plot(*original, label='Original', color='blue')
plt.plot(*transformado, label='Transformado', color='red')
plt.axhline(0, color='gray', lw=0.5)
plt.axvline(0, color='gray', lw=0.5)
plt.grid(True)
plt.axis('equal')
plt.legend()
plt.title('Transformaci√≥n Lineal con Matriz A')
plt.show()
```

### üîç An√°lisis

* La matriz **A** cambia la forma del cuadrado unitario.
* Esta transformaci√≥n puede **escalar, rotar, reflejar o sesgar** los vectores originales dependiendo de los valores de la matriz.

### üß≠ Ejemplo 2: Escalamiento y rotaci√≥n

```python
from math import cos, sin, pi

# Escalamiento
S = np.array([
    [2, 0],
    [0, 0.5]
])

# Rotaci√≥n 45 grados
theta = pi / 4
R = np.array([
    [cos(theta), -sin(theta)],
    [sin(theta), cos(theta)]
])

# Combinar: escalar y luego rotar
T = R @ S
resultado = T @ original

plt.figure(figsize=(6,6))
plt.plot(*original, label='Original', color='blue')
plt.plot(*resultado, label='Escala + Rotaci√≥n', color='green')
plt.grid(True)
plt.axis('equal')
plt.legend()
plt.title('Transformaci√≥n Lineal: Escala y Rotaci√≥n')
plt.show()
```

### üìö Aplicaciones

* **Compresi√≥n de im√°genes** (PCA).
* **Animaci√≥n y gr√°ficos por computadora**.
* **Simulaci√≥n f√≠sica** y geometr√≠a computacional.
* **Machine learning** (transformaciones en espacios latentes).

### Resumen

#### ¬øC√≥mo entendemos las matrices como transformaciones lineales?

Las matrices pueden entenderse como transformaciones lineales que, al aplicarse a un espacio o un vector, generan una transformaci√≥n. Cuando aplicamos una matriz, podemos afectar a un vector modificando su tama√±o o incluso rot√°ndolo. En el mundo de la programaci√≥n, podemos llevar esto a la pr√°ctica utilizando Python y librer√≠as como NumPy y Matplotlib para representar gr√°ficamente estos cambios.

#### ¬øC√≥mo configuramos nuestro entorno en Python para visualizaciones?

Para empezar, necesitamos importar las librer√≠as necesarias. Aqu√≠ va un peque√±o fragmento de c√≥digo en Python que nos permitir√° ver los gr√°ficos debajo de cada celda de nuestro notebook:

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
```

Posteriormente, definimos nuestras matrices y vectores usando `numpy`.

#### ¬øC√≥mo definimos y aplicamos una transformaci√≥n con matrices?

Supongamos que tenemos la siguiente matriz:

`A = np.array([[-1, 3], [2, -2]])`

Queremos investigar qu√© transformaci√≥n genera esta matriz al aplicarla al siguiente vector:

`v = np.array([2, 1])`

La transformaci√≥n de un vector `v` usando una matriz `A` se realiza a trav√©s del producto interno de la matriz y el vector. Pero antes de eso, definamos una funci√≥n para graficar los vectores.

#### ¬øC√≥mo graficamos vectores en Python?

Es √∫til tener una funci√≥n vers√°til para graficar m√∫ltiples vectores. Aqu√≠ hay una base de c√≥mo podemos definir y utilizar esta funci√≥n:

```python
def graficar_vectores(vectores, colores, alpha=1):
    plt.figure()
    plt.axvline(x=0, color='grey', lw=1)
    plt.axhline(y=0, color='grey', lw=1)
    for i in range(len(vectores)):
        x = np.concatenate([[0, 0], vectores[i]])
        plt.quiver(*x[::2], *x[1::2], angles='xy', scale_units='xy', scale=1, color=colores[i], alpha=alpha)
    plt.xlim(-3, 3)
    plt.ylim(-3, 3)
    plt.show()

v_flaten = v.flatten()
graficar_vectores([v_flaten], ['blue'])
```

Esta funci√≥n ayuda a visualizar c√≥mo han cambiado los vectores al ser transformados por la matriz. Los ejes cruzan en `x=0` y el color de las l√≠neas es `gris`.

#### ¬øC√≥mo se transforman los vectores utilizando matrices?

Cuando aplicamos la matriz `A` al vector `v`, podemos ver el cambio que se produce. Primero, realizamos el c√°lculo del producto interno:

```python
v_transformado = A.dot(v)
graficar_vectores([v.flatten(), v_transformado.flatten()], ['blue', 'orange'])
```

Aqu√≠, graficamos el vector original `v` junto con el vector `v_transformado` para observar la transformaci√≥n visualmente, comparando sus posiciones y direcciones.

#### ¬øPor qu√© es importante entender estas transformaciones en Aprendizaje Autom√°tico?

Las transformaciones de matrices son fundamentales en el aprendizaje autom√°tico, especialmente cuando trabajamos con im√°genes o datos que tienen representaciones matriciales. Las matrices permiten transformar estos datos de manera que pueden ser procesados m√°s eficientemente por algoritmos de Deep Learning o Machine Learning.

Entender las representaciones de vectores y matrices y c√≥mo podemos alargar, rotar o modificar su escala es clave para manipular datos estructurados como im√°genes, en las cuales cada pixel puede ser parte de una matriz mayor. Cuando llevamos matrices a vectores, este proceso se llama "flatten" y es crucial para el tratamiento de datos en modelos computacionales.

#### ¬øQu√© papel juegan los determinantes en estas transformaciones?

El determinante de una matriz nos ofrece informaci√≥n valiosa sobre la transformaci√≥n. Un determinante negativo (como el `-4` en nuestro ejemplo) puede indicarnos que la transformaci√≥n involucra una inversi√≥n o un giro. Por otro lado, si las normas de los vectores antes y despu√©s de la transformaci√≥n se mantienen iguales, puede se√±alar que hay vectores en el espacio que no cambian su longitud.

```python
determinante = np.linalg.det(A)
print(f"Determinante de la matriz A: {determinante}")
```

En esta exploraci√≥n, continuamos profundizando en c√≥mo las operaciones de matriz y producto escalar nos ayudan a dar forma a datos y patrones, sentando las bases para descubrimientos m√°s intrincados en el vasto campo del an√°lisis de datos y Machine Learning.

## Autovalores y autovectores en transformaciones lineales

Los **autovalores** y **autovectores** son conceptos fundamentales en **√°lgebra lineal**, especialmente √∫tiles en el an√°lisis de **transformaciones lineales**. Aqu√≠ te explico de forma clara qu√© son, para qu√© sirven y c√≥mo se usan en transformaciones lineales, incluyendo ejemplos en **Python**.

### üî∑ ¬øQu√© son?

#### ‚úÖ **Autovector (Eigenvector)**

Es un vector que **no cambia de direcci√≥n** cuando se le aplica una transformaci√≥n lineal, solo puede ser **escalado** (alargado o acortado).

#### ‚úÖ **Autovalor (Eigenvalue)**

Es el **escalar** que indica cu√°nto se escala el autovector despu√©s de aplicar la transformaci√≥n.

### üîπ Definici√≥n matem√°tica

Si $A$ es una matriz cuadrada, un **autovector** $\vec{v} \neq 0$ y su correspondiente **autovalor** $\lambda$ satisfacen:

$$
A \vec{v} = \lambda \vec{v}
$$

### üî∏ Interpretaci√≥n en transformaciones lineales

Cuando aplicamos una transformaci√≥n lineal representada por una matriz $A$, los autovectores son **direcciones que no rotan**. Solo se **escalan** por su autovalor correspondiente.

Ejemplo:
Si una matriz representa una rotaci√≥n o estiramiento, los autovectores indican las **direcciones "invariantes"**, y los autovalores cu√°nto se **alargan o acortan** esas direcciones.

### üìå Aplicaciones

* Compresi√≥n de datos (PCA)
* Din√°mica de sistemas
* Computaci√≥n gr√°fica
* An√°lisis de redes
* Machine Learning

### üêç Ejemplo en Python con NumPy

```python
import numpy as np

# Matriz de transformaci√≥n
A = np.array([[2, 1],
              [1, 2]])

# Autovalores y autovectores
autovalores, autovectores = np.linalg.eig(A)

print("Autovalores:")
print(autovalores)

print("\nAutovectores (columnas):")
print(autovectores)
```

### üìà Visualizaci√≥n (opcional con matplotlib)

Si quieres visualizar c√≥mo act√∫an los autovectores en una transformaci√≥n, puedo ayudarte a generar un gr√°fico de vectores antes y despu√©s de aplicar la matriz $A$.

### Resumen

#### ¬øQu√© son las transformaciones lineales y c√≥mo afectan a los vectores?

Las transformaciones lineales son un concepto fundamental en √°lgebra lineal, el cual describe c√≥mo un vector puede ser manipulado por una matriz para cambiar de direcci√≥n o magnitud. Este proceso es crucial en muchos campos, como la f√≠sica, la computaci√≥n gr√°fica o la inteligencia artificial. Un auto-v√≠ctor particular es un vector que, cuando se le aplica una transformaci√≥n, mantiene su direcci√≥n original, aunque su amplitud puede variar tras ser multiplicado por un autovalor.

#### ¬øC√≥mo podemos graficar transformaciones lineales?

Para visualizar c√≥mo una matriz transforma un vector, podemos utilizar herramientas de gr√°ficos en Python. Aqu√≠, importamos las bibliotecas necesarias para esta tarea, que incluyen `numpy` como `np` y `matplotlib`.

```python
import numpy as np
import matplotlib.pyplot as plt

def graficar_vectores(vectores, colores, l√≠mites):
    plt.figure()
    plt.axvline(x=0, color='grey', lw=2)
    plt.axhline(y=0, color='grey', lw=2)
    for i in range(len(vectores)):
        x = np.array([0, vectores[i][0]])
        y = np.array([0, vectores[i][1]])
        plt.quiver(x[0], y[0], x[1], y[1], angles='xy', scale_units='xy', scale=1, color=colores[i])
    plt.xlim(l√≠mites['x'])
    plt.ylim(l√≠mites['y'])
    plt.grid()
    plt.show()
```

#### ¬øC√≥mo encontrar autovectores y autovalores?

Para hallar un autovector, debemos encontrar un vector que no cambie su direcci√≥n tras aplicarle una matriz de transformaci√≥n. Este proceso tambi√©n implica determinar el autovalor asociado.

##### Ejemplo de c√°lculo

Supongamos que tenemos la siguiente matriz y vector:

```python
A = np.array([[3, 2], [4, 1]])
v = np.array([1, 1])
```

Para encontrar el vector transformado, aplicamos el producto interno a `v`:

`v_transformado = np.dot(A, v)`

Esto nos devuelve un nuevo vector que podemos graficar junto al original para observar las diferencias.

##### C√°lculo del autovalor

Los resultados del producto pueden interpretarse para encontrar el autovalor:

- Si `v_transformado` es un m√∫ltiplo de `v`, entonces el factor de multiplicaci√≥n es el autovalor.

En este ejemplo, si `v_transformado` resulta ser `[5, 5]`, entonces el autovalor ser√≠a 5.

##### Visualizaci√≥n de vectores originales y transformados

Para demostrar la teor√≠a, graficamos los vectores usando colores distintos para diferenciar entre vectores originales y transformados:

```python
colores = ['#FF9A13', '#1190FF']  # Naranja y azul claro
l√≠mites = {'x': [-1, 6], 'y': [-1, 6]}
graficar_vectores([v, v_transformado], colores, l√≠mites)
```

Esta representaci√≥n visual muestra claramente el cambio en la magnitud o sentido del vector tras la transformaci√≥n.

#### ¬øCu√°ntos autovectores puede tener una matriz?

En una matriz 2x2, como en nuestro ejemplo, podemos encontrar hasta dos autovectores con sus respectivos autovalores. Esto significa que hay dos direcciones distintas que, al ser transformadas, conservan su direcci√≥n dentro de la misma transformaci√≥n.

Al explorar estos conceptos, enriquecemos nuestra comprensi√≥n del √°lgebra lineal y su aplicaci√≥n pr√°ctica en la resoluci√≥n de problemas complejos, alentando a los estudiantes a continuar mejorando su habilidad y abriendo la puerta a m√°s aplicaciones matem√°ticas.

## C√°lculo de Autovalores y Autovectores con NumPy en Python

Vamos a ver c√≥mo calcular **autovalores (eigenvalues)** y **autovectores (eigenvectors)** en **Python** usando la librer√≠a **NumPy**, paso a paso.

### üß† ¬øQu√© necesitas?

Una **matriz cuadrada** $A \in \mathbb{R}^{n \times n}$, y quieres encontrar $\lambda$ y $\vec{v}$ tales que:

$$
A \vec{v} = \lambda \vec{v}
$$

### ‚úÖ Paso 1: Importar NumPy

```python
import numpy as np
```

### ‚úÖ Paso 2: Definir la matriz

Por ejemplo:

```python
A = np.array([[4, 2],
              [1, 3]])
```

### ‚úÖ Paso 3: Calcular autovalores y autovectores

```python
autovalores, autovectores = np.linalg.eig(A)
```

* `autovalores` es un array con los $\lambda_1, \lambda_2, \ldots$
* `autovectores` es una matriz donde **cada columna** es un autovector asociado al autovalor correspondiente

### ‚úÖ Paso 4: Mostrar resultados

```python
print("Autovalores:")
print(autovalores)

print("\nAutovectores (cada columna es uno):")
print(autovectores)
```

### üéØ Ejemplo completo

```python
import numpy as np

# Matriz cuadrada
A = np.array([[4, 2],
              [1, 3]])

# C√°lculo
autovalores, autovectores = np.linalg.eig(A)

print("Autovalores:")
print(autovalores)

print("\nAutovectores:")
print(autovectores)
```

### ‚ú® Resultado esperado (aproximado):

```plaintext
Autovalores:
[5. 2.]

Autovectores:
[[ 0.894  -0.707 ]
 [ 0.447   0.707 ]]
```

### üìå Nota importante

* Puedes verificar que:

  $$
  A \cdot \vec{v}_i \approx \lambda_i \cdot \vec{v}_i
  $$

  con `np.allclose(A @ v, l * v)` para cada par $(\lambda_i, \vec{v}_i)$

### Resumen

#### ¬øC√≥mo calcular autovalores y autovectores con Python?

Para aquellos interesados en profundizar en el √°lgebra lineal y su aplicaci√≥n a trav√©s de la programaci√≥n, comprender c√≥mo calcular autovalores y autovectores es esencial. Python, con sus poderosas bibliotecas de c√°lculo, ofrece una manera eficiente de realizar estos c√°lculos. En este art√≠culo, abordaremos c√≥mo puedes usar la biblioteca NumPy para hallar autovalores y autovectores de una matriz dada.

#### ¬øQu√© necesitamos para empezar?

Primero, aseg√∫rate de tener instaladas las bibliotecas necesarias en tu entorno de programaci√≥n. Las herramientas principales ser√°n:

- **NumPy**: Para c√°lculos num√©ricos.
- **Matplotlib**: Para visualizar los vectores y su transformaci√≥n gr√°fica.

#### ¬øC√≥mo definir y calcular con NumPy?

Comencemos por definir una matriz utilizando NumPy. Supongamos que quinemos encontrar los autovalores y autovectores de la matriz X:

```python
import numpy as np

# Definici√≥n de la matriz X
X = np.array([[3, 2],
              [4, 1]])
```

Para obtener los autovalores y autovectores, usaremos la funci√≥n `eig` de NumPy:

`autovalores, autovectores = np.linalg.eig(X)`

#### ¬øQu√© nos devuelven las funciones de NumPy?

La funci√≥n `np.linalg.eig()` devuelve dos elementos:

1. Un arreglo con los autovalores de la matriz.
2. Una matriz con los autovectores asociados, donde cada columna representa un autovector.

#### An√°lisis visual de los autovectores

Para comprender mejor el resultado, podemos graficar los vectores utilizando Matplotlib. He aqu√≠ un ejemplo de c√≥mo hacerlo:

```python
import matplotlib.pyplot as plt

# Funci√≥n para graficar vectores desde el origen
def graficar_vectores(vectores, colores):
    plt.figure()
    plt.axvline(x=0, color='grey', lw=1)
    plt.axhline(y=0, color='grey', lw=1)
    for i in range(len(vectores)):
        plt.quiver(0, 0, vectores[i][0], vectores[i][1], 
                   angles='xy', scale_units='xy', scale=1, color=colores[i])
    plt.xlim(-4, 4)
    plt.ylim(-4, 4)
    plt.grid()
    plt.show()

# Graficar los autovectores
graficar_vectores(autovectores.T, ['green', 'orange'])
```

#### ¬øQu√© observar al visualizar?

Cuando visualizas los resultados, notar√°s que los autovectores que calcula NumPy son m√∫ltiples de los que podr√≠as haber calculado manualmente. Esto es completamente v√°lido en matem√°tica, ya que lo que define a un autovector es su direcci√≥n, no su magnitud espec√≠fica.

#### Importancia de los autovalores y autovectores

Los autovalores y autovectores son herramientas cr√≠ticas en muchos campos de la ciencia e ingenier√≠a, incluida la descomposici√≥n espectral, an√°lisis de estabilidad y reducci√≥n dimensional (como en PCA). A pesar de que el proceso pueda parecer t√©cnico, su comprensi√≥n y correcta implementaci√≥n abre puertas a soluciones de problemas mucho m√°s complejos.

Con este conocimiento, est√°s bien equipado para explorar y experimentar con la descomposici√≥n de matrices en Python. La pr√°ctica te ayudar√° a internalizar estos conceptos, as√≠ que te alentamos a continuar explorando y ampliando tus habilidades. ¬°Sigue aprendiendo!

## Descomposici√≥n de matrices: valores y vectores propios

La **descomposici√≥n en valores y vectores propios** (tambi√©n llamada **descomposici√≥n espectral**) es una t√©cnica fundamental del **√°lgebra lineal**, con m√∫ltiples aplicaciones en machine learning, f√≠sica, estad√≠sticas, compresi√≥n de datos y m√°s.

### üî∑ ¬øQu√© es la descomposici√≥n en valores y vectores propios?

Dada una **matriz cuadrada** $A$, si es **diagonalizable**, podemos escribirla como:

$$
A = V \Lambda V^{-1}
$$

Donde:

* $V$: matriz cuyos **columnas son los autovectores** de $A$
* $\Lambda$: **matriz diagonal** con los **autovalores** de $A$
* $V^{-1}$: **inversa** de $V$

Esta descomposici√≥n permite entender c√≥mo $A$ transforma el espacio.

### ‚úÖ Condiciones

* **Solo funciona** para matrices cuadradas.
* Requiere que $A$ tenga un conjunto completo de autovectores linealmente independientes (es decir, que sea diagonalizable).

### üß† ¬øPor qu√© es √∫til?

* **Reduce la complejidad computacional** de operaciones (como exponenciar matrices).
* Permite **entender geometr√≠a de transformaciones**.
* Se usa en PCA, sistemas din√°micos, compresi√≥n de im√°genes, etc.

### üêç Ejemplo en Python con NumPy

```python
import numpy as np

# Matriz cuadrada A
A = np.array([[4, 2],
              [1, 3]])

# Autovalores y autovectores
eigenvalues, eigenvectors = np.linalg.eig(A)

# Matriz diagonal de autovalores
Lambda = np.diag(eigenvalues)

# Matriz de autovectores
V = eigenvectors

# Verificaci√≥n de la descomposici√≥n
A_reconstructed = V @ Lambda @ np.linalg.inv(V)

print("Matriz original A:")
print(A)

print("\nMatriz reconstruida A_reconstructed:")
print(A_reconstructed)
```

### üßæ Salida esperada

```plaintext
Matriz original A:
[[4 2]
 [1 3]]

Matriz reconstruida A_reconstructed:
[[4. 2.]
 [1. 3.]]
```

> Si `A_reconstructed` ‚âà `A`, entonces la descomposici√≥n es correcta ‚úÖ

### üìå ¬øQu√© pasa si la matriz no es diagonalizable?

Si no se puede escribir como $V \Lambda V^{-1}$, a√∫n puedes usar la **descomposici√≥n de Schur** o la **SVD (Singular Value Decomposition)**, m√°s general para matrices no cuadradas o no diagonalizables.

### Resumen

#### ¬øQu√© significa descomponer una matriz?

Descomponer una matriz implica encontrar una o m√°s matrices que, al multiplicarlas, nos permitan recrear la matriz original, cumpliendo con ciertas propiedades. Un ejemplo sencillo es el n√∫mero 6, que puede descomponerse como 3x2, donde 3 y 2 tienen la propiedad de ser n√∫meros primos. Aplicado a matrices, buscamos matrices componentes que nos faciliten ciertos c√°lculos o an√°lisis.

#### ¬øC√≥mo se realiza la descomposici√≥n usando autovalores y autovectores?

Para realizar la descomposici√≥n de una matriz utilizando autovalores y autovectores, seguimos un proceso sistem√°tico. Supongamos que tenemos una matriz A. Este proceso consta de varios pasos:

1. **Determinaci√≥n de autovalores y autovectores**:

- Calculamos los autovalores ((\lambda)) y los autovectores (v) de la matriz.

2. **Construcci√≥n de la matriz diagonal**:

- Creamos una matriz diagonal que contiene todos los autovalores.

3. **Composici√≥n de la matriz original**:

- Escribimos la matriz A como el producto de la matriz de autovectores, la matriz diagonal de autovalores, y la inversa de la matriz de autovectores.

#### Ejemplo pr√°ctico con c√≥digo

```python
import numpy as np

# Definimos la matriz A
A = np.array([[3, 2], [4, 1]])

# Calculamos autovalores y autovectores
autovalores, autovectores = np.linalg.eig(A)

# Mostramos resultados
print("Autovalores:", autovalores)
print("Autovectores:\n", autovectores)

# Composici√≥n de la matriz original
matrizA_calculada = np.dot(autovectores, np.dot(np.diag(autovalores), np.linalg.inv(autovectores)))
print("Matriz Calculada:\n", matrizA_calculada)
```

Al ejecutar este c√≥digo, deber√≠as observar que la matriz calculada es id√©ntica a la matriz original, lo que confirma que la descomposici√≥n ha sido exitosa.

#### ¬øQu√© beneficios tiene usar matrices sim√©tricas reales?

Cuando trabajamos con matrices sim√©tricas reales, las propiedades de estas matrices nos ofrecen ventajas computacionales importantes. Estas matrices cumplen con que A es igual a su transpuesta ((A = A^T)), lo que implica que en lugar de calcular su inversa, podemos trabajar con su transpuesta.

#### Uso de matrices sim√©tricas en descomposiciones

1. **Renovaci√≥n del proceso de descomposici√≥n**:

- Si la matriz es sim√©trica, podemos reformular nuestra descomposici√≥n usando la transpuesta de los autovectores en lugar de su inversa.

2. **Ejecuci√≥n del c√°lculo**:

- Esta forma es no solo m√°s f√°cil de calcular, sino tambi√©n m√°s eficiente en t√©rminos computacionales.

#### Ejemplo de matriz sim√©trica

```python
# Definimos una matriz sim√©trica
A_sim = np.array([[3, 2], [2, 3]])

# Calculamos autovalores y autovectores
autovalores, autovectores = np.linalg.eig(A_sim)

# Composici√≥n con transpuesta
resultante_sim = np.dot(autovectores, np.dot(np.diag(autovalores), autovectores.T))
print("Matriz Sim√©trica Calculada:\n", resultante_sim)
```

Con este m√©todo, verificamos que obtenemos la matriz original sin tener que calcular una inversa, lo cual es especialmente √∫til para aplicaciones que requieren rapidez y eficiencia.

#### Ventajas y recomendaciones

- **Eficiencia Computacional**: Utilizar matrices sim√©tricas o trabajar con matrices que nos permitan evitar la inversa nos brinda ventajas de velocidad y precisi√≥n.
- **Simplicidad de C√°lculo**: Usar la transpuesta es m√°s sencillo y fiable que la calculadora inversa de matrices.

Aquellos que buscan optimizar procesos donde las matrices juegan un papel crucial deben considerar estas metodolog√≠as para lograr resultados efectivos y robustos. Contin√∫a explorando esta √°rea fascinante y sigue fortaleciendo tus habilidades matem√°ticas y computacionales para destacar en el an√°lisis de matrices.

## Descomposici√≥n de Matrices en Valores Singulares

La **Descomposici√≥n en Valores Singulares** (SVD, por sus siglas en ingl√©s: *Singular Value Decomposition*) es una de las herramientas m√°s poderosas y vers√°tiles del √°lgebra lineal aplicada a datos, machine learning, compresi√≥n, procesamiento de im√°genes, recomendadores, y m√°s.

### üî∑ ¬øQu√© es la descomposici√≥n SVD?

Para **cualquier matriz** $A \in \mathbb{R}^{m \times n}$, se puede descomponer como:

$$
A = U \Sigma V^T
$$

Donde:

* $U \in \mathbb{R}^{m \times m}$: matriz ortogonal con vectores propios de $A A^T$
* $\Sigma \in \mathbb{R}^{m \times n}$: matriz **diagonal rectangular** con los **valores singulares** de $A$
* $V^T \in \mathbb{R}^{n \times n}$: transpuesta de matriz ortogonal con vectores propios de $A^T A$

### üìå ¬øPara qu√© sirve?

* **Reducci√≥n de dimensiones** (PCA usa SVD)
* **Compresi√≥n de im√°genes**
* **Eliminaci√≥n de ruido**
* **Sistemas de recomendaci√≥n** (Netflix, Amazon)
* Resolver **sistemas sobredeterminados o mal condicionados**

### üêç Ejemplo en Python con NumPy

```python
import numpy as np

# Matriz original (puede no ser cuadrada)
A = np.array([[3, 1, 1],
              [-1, 3, 1]])

# Descomposici√≥n SVD
U, S, VT = np.linalg.svd(A)

print("Matriz U:")
print(U)

print("\nValores singulares (S):")
print(S)

print("\nMatriz V transpuesta (V^T):")
print(VT)

# Para reconstruir A:
Sigma = np.zeros((U.shape[0], VT.shape[0]))
np.fill_diagonal(Sigma, S)

A_reconstructed = U @ Sigma @ VT

print("\nMatriz A reconstruida:")
print(A_reconstructed)
```

### üìä ¬øQu√© representan los valores singulares?

Los elementos de $\Sigma$ (valores singulares) indican **cu√°nta informaci√≥n** (energ√≠a, varianza) est√° contenida en cada componente. Se ordenan de mayor a menor.

### üéØ Reducci√≥n de Dimensi√≥n con SVD

Si solo usas los primeros $k$ valores singulares:

$$
A_k = U_k \Sigma_k V_k^T
$$

Esto da una **aproximaci√≥n de menor rango**, ideal para:

* Visualizaci√≥n
* Compresi√≥n
* Velocidad de c√≥mputo

### üìå Resumen

| Componente | Significado                                       |
| ---------- | ------------------------------------------------- |
| $U$        | Direcciones principales de las filas (izquierda)  |
| $\Sigma$   | Importancia relativa (energ√≠a) de cada direcci√≥n  |
| $V^T$      | Direcciones principales de las columnas (derecha) |

### Resumen

#### ¬øQu√© es la descomposici√≥n en valores singulares?

La descomposici√≥n en valores singulares (SVD por sus siglas en ingl√©s) es una herramienta matem√°tica esencial, especialmente √∫til en el an√°lisis de datos y procesamiento de im√°genes. Cuando nos enfrentamos a una matriz que no es cuadrada, la SVD nos permite extraer y condensar informaci√≥n esencial. En esta t√©cnica, una matriz se descompone en tres componentes: dos matrices ortogonales y una matriz diagonal.

#### ¬øComo se representa la descomposici√≥n?

Dentro de la descomposici√≥n en valores singulares, una matriz A se descompone en:

- **U**: Una matriz ortogonal donde todos sus vectores son ortonormales. Contiene los vectores singulares izquierdos.
- **Œ£**: Una matriz diagonal que tiene en su diagonal los valores singulares de la matriz original, y fuera de la diagonal, ceros.
- **V^T**: La transpuesta de una matriz ortogonal que alberga los vectores singulares derechos.

#### ¬øCu√°l es la importancia de cada componente?

- **Vectores singulares (izquierdos y derechos)**: Son esenciales para comprender c√≥mo se transforma la informaci√≥n en su representaci√≥n simplificada. Los vectores singulares derechos (V) y los vectores singulares izquierdos (U) ofrecen un sistema de coordenadas para visualizar el efecto de la transformaci√≥n.

- **Valores singulares**: Estos est√°n contenidos en la matriz diagonal Œ£ y determinan la influencia de los vectores singulares. Cuanto m√°s grande es un valor singular, m√°s impacto tiene el correspondiente vector singular en la reconstrucci√≥n de la matriz original.

#### ¬øC√≥mo se ejecuta la descomposici√≥n en Python?

La descomposici√≥n de matrices en valores singulares se puede realizar f√°cilmente en Python usando la biblioteca NumPy. Aqu√≠ se muestra un ejemplo de c√≥mo llevarlo a cabo:

```python
import numpy as np

# Definiendo la matriz A
A = np.array([[1, 2, 3], [3, 4, 5]])

# Calculando la descomposici√≥n en valores singulares
U, S, Vt = np.linalg.svd(A)

print("Matriz U:")
print(U)

print("Valores singulares:")
print(S)

print("Matriz V^T:")
print(Vt)
```

#### ¬øQu√© nos muestra el resultado?

1. **Matriz U**: Muestra los vectores singulares izquierdos, cada columna es un vector.

2. **Valores singulares en S**: En su forma econ√≥mica, muestra solo los valores singulares no nulos.

3. **Matriz V^T**: Contiene los vectores singulares derechos en sus filas.

#### Aplicaciones y recomendaciones para el uso de SVD

La descomposici√≥n en valores singulares es ampliamente utilizada en diferentes √°reas, tales como:

- Procesamiento de imagen: Para reducir el ruido y comprimir im√°genes.
- An√°lisis de datos: En reducci√≥n dimensional o en la recomendaci√≥n de sistemas para identificar patrones significativos.

#### Leer m√°s y experimentar

La SVD es un concepto fundamental para los cient√≠ficos de datos e ingenieros que buscan optimizar recursos al trabajar con grandes conjuntos de datos. Se recomienda explorar distintas bibliotecas y entornos de programaci√≥n que proporcionan funcionalidades avanzadas para operaciones con matrices, tales como TensorFlow o SciPy, para obtener una experiencia pr√°ctica profunda.

No se quede solo en la teor√≠a; la pr√°ctica es clave. Experimente con matrices de diferentes tama√±os y observe el comportamiento de la descomposici√≥n para entender c√≥mo sus datos pueden ser manipulados y analizados de manera m√°s eficiente.

¬°Continue explorando el mundo de las matem√°ticas y el √°lgebra lineal para mejorar sus habilidades en ciencia de datos y programaci√≥n!

## Transformaciones Lineales con Matrices: Efectos en el C√≠rculo Unitario

Analizar **transformaciones lineales** a trav√©s de sus **efectos sobre el c√≠rculo unitario** es una forma visual y poderosa de entender c√≥mo act√∫an las matrices sobre los vectores del espacio.

### üßÆ ¬øQu√© es una transformaci√≥n lineal?

Una **transformaci√≥n lineal** $T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ es una funci√≥n que puede representarse como:

$$
T(\vec{x}) = A \vec{x}
$$

donde $A$ es una **matriz** y $\vec{x}$ un **vector**.

### üîµ ¬øQu√© pasa si aplicas una transformaci√≥n al **c√≠rculo unitario**?

El **c√≠rculo unitario** en $\mathbb{R}^2$ est√° formado por todos los vectores $\vec{x}$ tal que:

$$
\|\vec{x}\| = 1 \Rightarrow x^2 + y^2 = 1
$$

Cuando aplicas una matriz $A$ al c√≠rculo, lo transformas en una **elipse**. Esta elipse te muestra visualmente:

* La **direcci√≥n de mayor estiramiento** (autovector con mayor autovalor o valor singular)
* La **direcci√≥n de compresi√≥n**
* Si la matriz **rota**, **refleja**, **escalona**, etc.

### üêç Ejemplo en Python con visualizaci√≥n

```python
import numpy as np
import matplotlib.pyplot as plt

# C√≠rculo unitario
theta = np.linspace(0, 2 * np.pi, 100)
circle = np.array([np.cos(theta), np.sin(theta)])

# Matriz de transformaci√≥n
A = np.array([[2, 1],
              [1, 3]])

# Transformaci√≥n lineal
transformed = A @ circle

# Gr√°fico
plt.figure(figsize=(6, 6))
plt.plot(circle[0], circle[1], label='C√≠rculo Unitario', color='blue')
plt.plot(transformed[0], transformed[1], label='Transformado', color='red')
plt.axhline(0, color='black', linewidth=0.5)
plt.axvline(0, color='black', linewidth=0.5)
plt.axis('equal')
plt.grid(True)
plt.title('Transformaci√≥n Lineal del C√≠rculo Unitario')
plt.legend()
plt.show()
```

### üß† ¬øQu√© observas?

* Si $A$ **rota**, la elipse se inclina.
* Si $A$ **refleja**, cambia de orientaci√≥n.
* Si $A$ **es diagonal**, estira el c√≠rculo en direcciones de los ejes.
* Si $A$ tiene **autovectores**, las direcciones principales de la elipse coinciden con ellos.

### üîé Conexi√≥n con valores singulares

La forma de la elipse tambi√©n est√° **relacionada con los valores singulares de $A$**:

* El **eje largo** de la elipse ‚Üí **mayor valor singular**.
* El **eje corto** de la elipse ‚Üí **menor valor singular**.

### Resumen

#### ¬øC√≥mo se descomponen las matrices en transformaciones lineales?

Las matrices, cuando se piensan como transformaciones lineales, ofrecen una herramienta poderosa para manipular diferentes vectores en un espacio. Una matriz A puede descomponerse en otras tres matrices, cada una representando su propia transformaci√≥n lineal. Entender sus efectos es crucial, ya que estas transformaciones repercuten de la misma manera sin importar los vectores a los que se apliquen. Vamos a sumergirnos en el mundo de las matrices y su relaci√≥n con el c√≠rculo unitario.

#### ¬øQu√© es el c√≠rculo unitario y por qu√© lo utilizamos?

El c√≠rculo unitario es una herramienta gr√°fica que ayuda a visualizar los efectos de estas transformaciones. Dicho de otro modo, se trata de un c√≠rculo centrado en el origen (0,0) con radio 1. Su papel es esencial en el estudio de las transformaciones lineales, ya que permite observar de forma clara los cambios que dichas transformaciones generan, tal como la rotaci√≥n o el escalado de vectores dentro del espacio.

#### ¬øC√≥mo graficar el c√≠rculo unitario?

Para graficar el c√≠rculo unitario y aplicar transformaciones, necesitamos Python y la biblioteca `numpy` para c√°lculos y `matplotlib` para la visualizaci√≥n. El siguiente c√≥digo muestra c√≥mo lograr esto:

```python
import numpy as np
import matplotlib.pyplot as plt

def graficar_matriz(matriz, vector_colores=['r', 'b']):
    # Definimos el c√≠rculo unitario
    x = np.linspace(-1, 1, 1000)
    y = np.sqrt(1 - x**2)
    
    # Transformaci√≥n del c√≠rculo por la matriz
    x1 = matriz[0, 0] * x + matriz[0, 1] * y
    y1 = matriz[1, 0] * x + matriz[1, 1] * y
    
    x1_neg = matriz[0, 0] * x - matriz[0, 1] * y
    y1_neg = matriz[1, 0] * x - matriz[1, 1] * y
    
    # Graficamos los vectores transformados
    plt.plot(x1, y1, color='g', alpha=0.7)
    plt.plot(x1_neg, y1_neg, color='g', alpha=0.7)
    
    plt.axhline(0, color='black', lw=1)
    plt.axvline(0, color='black', lw=1)
    plt.xlim(-1.5, 1.5)
    plt.ylim(-1.5, 1.5)
    plt.gca().set_aspect('equal', adjustable='box')
    plt.show()

# Ejemplo con una matriz de identidad
matriz_identidad = np.array([[1, 0], [0, 1]])
graficar_matriz(matriz_identidad)
```

#### ¬øC√≥mo aplicar transformaciones sin modificar el gr√°fico original?

Al probar el efecto de diferentes matrices, es esencial luego visualizar c√≥mo estas cambian el c√≠rculo unitario en tiempo real. Las matrices de diferentes valores cambian la forma del c√≠rculo original, visualizando el impacto de la transformaci√≥n lineal aplicada. A continuaci√≥n, se muestra c√≥mo se implementa este concepto:

```python
# Definimos una matriz
A = np.array([[3, 7], [5, 2]])

# Graficamos el c√≠rculo unitario original
graficar_matriz(np.eye(2))  # Matriz identidad

# Graficamos el c√≠rculo unitario transformado
graficar_matriz(A)
```

#### ¬øQu√© observamos con las matrices aplicadas?

Al aplicar la matriz A, observamos c√≥mo los vectores base del c√≠rculo unitario original (los puntos cardinales) se transforman. Por ejemplo:

- El vector (1, 0) se desplaza de su posici√≥n original.
- El vector (0, 1) tambi√©n cambia, conforme a los componentes de A, obteniendo un nuevo espacio transformado.

Esta aplicaci√≥n pr√°ctica nos permite ver c√≥mo act√∫a la matriz sobre los vectores del c√≠rculo y nos ofrece una comprensi√≥n palpable de las transformaciones lineales.

#### Recomendaciones pr√°cticas

Para entender y visualizar de manera efectiva las transformaciones lineales mediante matrices:

1. **Practica con diversas matrices**: Cambia los valores de las matrices para observar distintos efectos en el c√≠rculo unitario.
2. **Documenta tus observaciones**: Mant√©n un registro de c√≥mo cambian los vectores con diferentes matrices, te ayudar√° a entender patrones.
3. **Explora visualmente y anal√≠ticamente**: Usa las gr√°ficas para ver el cambio y luego calcula los valores para confirmar lo que ves.
4. **Profundiza en la teor√≠a**: Revisa la teor√≠a matem√°tica detr√°s de estas transformaciones para una comprensi√≥n m√°s s√≥lida.

Estas recomendaciones no solo abrillantan tus habilidades matem√°ticas sino tambi√©n potencian tu capacidad de programaci√≥n aplicada a problemas matem√°ticos.

¬°Contin√∫a explorando y ampliando tu conocimiento en transformaciones lineales!

## Descomposici√≥n SVD: Transformaciones de Matrices y C√≠rculo Unitario

Relacionar la **descomposici√≥n en valores singulares (SVD)** con las **transformaciones del c√≠rculo unitario** es una de las formas m√°s potentes de entender visualmente c√≥mo una matriz transforma el espacio.

### üßÆ ¬øQu√© es la Descomposici√≥n SVD?

La descomposici√≥n SVD (Singular Value Decomposition) de una matriz $A \in \mathbb{R}^{m \times n}$ se define como:

$$
A = U \Sigma V^T
$$

Donde:

* $U$ es una matriz ortogonal de $m \times m$
* $\Sigma$ es una matriz diagonal (o rectangular) con valores singulares no negativos
* $V^T$ es la transpuesta de una matriz ortogonal $V \in \mathbb{R}^{n \times n}$

### üîµ ¬øC√≥mo transforma el c√≠rculo unitario?

Sup√≥n que partes de un **c√≠rculo unitario** $x \in \mathbb{R}^2$. La transformaci√≥n $A = U \Sigma V^T$ act√∫a en tres pasos:

1. **$V^T$**: Rota (o refleja) el c√≠rculo.
2. **$\Sigma$**: Estira o comprime a lo largo de los ejes.
3. **$U$**: Vuelve a rotar (o reflejar) el resultado.

### üêç Ejemplo visual en Python

```python
import numpy as np
import matplotlib.pyplot as plt

# 1. Generar c√≠rculo unitario
theta = np.linspace(0, 2 * np.pi, 200)
circle = np.array([np.cos(theta), np.sin(theta)])

# 2. Matriz A
A = np.array([[2, 1],
              [1, 3]])

# 3. SVD
U, S, VT = np.linalg.svd(A)

# 4. Aplicar cada paso del SVD
circle_VT = VT @ circle
circle_S = np.diag(S) @ circle_VT
circle_USV = U @ circle_S

# 5. Graficar
fig, axs = plt.subplots(1, 4, figsize=(16, 4))
titles = ['C√≠rculo Unitario', 'Aplicar V·µó', 'Aplicar Œ£', 'Aplicar U (Transformaci√≥n Final)']
steps = [circle, circle_VT, circle_S, circle_USV]
colors = ['blue', 'orange', 'green', 'red']

for i in range(4):
    axs[i].plot(steps[i][0], steps[i][1], color=colors[i])
    axs[i].set_title(titles[i])
    axs[i].axhline(0, color='gray', linestyle='--', linewidth=0.5)
    axs[i].axvline(0, color='gray', linestyle='--', linewidth=0.5)
    axs[i].axis('equal')
    axs[i].grid(True)

plt.tight_layout()
plt.show()
```

### üß† Interpretaci√≥n:

* La **forma final (elipse)** muestra el efecto completo de la transformaci√≥n $A$.
* Los **ejes de la elipse** son las **direcciones principales** (columnas de $U$), y sus longitudes son los **valores singulares** $\sigma_1, \sigma_2$.
* Este an√°lisis es √∫til para entender la **compresi√≥n de im√°genes**, **reducci√≥n de dimensionalidad**, y **rangos de matrices**.

### Resumen

#### ¬øC√≥mo funciona la descomposici√≥n de matrices en transformaciones?

La descomposici√≥n de matrices es una herramienta poderosa que nos permite entender c√≥mo una matriz original se transforma a trav√©s de distintas operaciones. En este caso, nos enfocaremos en la descomposici√≥n en valores singulares (SVD, por sus siglas en ingl√©s) y c√≥mo se refleja en transformaciones concretas. La representaci√≥n de una matriz mediante SVD nos retorna tres matrices y podemos aplicar cada transformaci√≥n paso a paso.

#### ¬øQu√© es la transformaci√≥n inicial de rotaci√≥n por la matriz B?

La primera transformaci√≥n que aplicamos a nuestra matriz se asocia con una rotaci√≥n y es efectuada por la matriz B. Imaginemos que iniciamos con un c√≠rculo unitario para visualizarlo mejor. Esto representa un punto de partida b√°sico y sim√©trico que al aplicar B, produce una rotaci√≥n del espacio.

Al hacer esta rotaci√≥n:

- El vector en el eje Y gira.
- El vector en el eje X tambi√©n gira.

Podemos calcular el √°ngulo de rotaci√≥n utilizando conceptos como el producto interno y la norma. Esta rotaci√≥n nos ayuda a comprender c√≥mo B reorienta nuestro espacio en la dimensi√≥n que estamos analizando.

#### ¬øQu√© efecto tiene la matriz diagonal D en la transformaci√≥n?

La siguiente transformaci√≥n que debemos considerar es la producida por la matriz diagonal D. Este paso se conoce como "escalado", y se encarga de amplificar o reducir nuestro espacio de trabajo seg√∫n las dimensiones:

1. **Amplificaci√≥n o reducci√≥n diferencial**: D ajusta la escala de nuestro sistema, pero no de manera uniforme en todas las direcciones.

Por ejemplo:

- En la direcci√≥n del eje Y, puede que se amplifique m√°s que en la direcci√≥n del eje X, o viceversa.

Para graficar y observar estos cambios, ajustamos el √°rea de visualizaci√≥n y analizamos c√≥mo cada vector es alargado o acortado de acuerdo con los valores de D.

#### ¬øC√≥mo finaliza el proceso de transformaci√≥n con una segunda rotaci√≥n?

La transformaci√≥n final la realiza la segunda matriz de rotaci√≥n, U. Este paso acaba de rotar el espacio despu√©s del escalado y nos devuelve un estado transformado que es fiel a la matriz original:

- **Rotaci√≥n final**: U se aplica al espacio ya ajustado, recolocando los vectores posiblemente al lado positivo (o negativo) del eje X o Y.

Anal√≠sticamente se puede comprobar que, al finalizar el proceso, el resultado de las transformaciones con A (la matriz original) coincide con las transformaciones sucedidas por SVD. Cada parte de la descomposici√≥n act√∫a en armon√≠a para que, al final, el efecto total de la matriz A se vea reflejado en los cambios integrados por sus componentes singulares.

Al estudiar el SVD, nos damos cuenta de cu√°nto podemos aprender de una matriz sencilla. Esto no solo nos proporciona mayor comprensi√≥n de la matem√°tica detr√°s de las transformaciones lineales, sino que tambi√©n nos dota de herramientas para realizar c√°lculos geom√©tricos complejos, maximizando nuestras capacidades en programaci√≥n y an√°lisis de datos.

## Impacto de los Valores Singulares en Transformaciones Matriciales

Los **valores singulares** tienen un impacto fundamental en las **transformaciones matriciales**, ya que determinan **c√≥mo se deforma el espacio** cuando se aplica una matriz a un conjunto de vectores.

### üßÆ ¬øQu√© son los valores singulares?

Dada una matriz $A \in \mathbb{R}^{m \times n}$, su **descomposici√≥n en valores singulares (SVD)** es:

$$
A = U \Sigma V^T
$$

* $\Sigma$ es una matriz diagonal con los **valores singulares** $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r > 0$
* Estos valores indican **cu√°nto se estira o comprime el espacio** en direcciones espec√≠ficas.

### üéØ ¬øC√≥mo impactan en una transformaci√≥n?

### 1. **Estiramiento/Escalamiento**

Cada valor singular $\sigma_i$ indica cu√°nto se **alarga o achica** el vector en la direcci√≥n del **autovector** correspondiente.

* Si $\sigma_1 = 3$: la matriz triplica la magnitud en esa direcci√≥n.
* Si $\sigma_2 = 0.5$: la matriz comprime a la mitad en esa otra direcci√≥n.

### 2. **Rango de la matriz**

El n√∫mero de valores singulares no nulos indica el **rango de la matriz**, es decir, cu√°ntas dimensiones del espacio est√°n preservadas.

### 3. **Condici√≥n num√©rica**

$$
\text{condici√≥n} = \frac{\sigma_1}{\sigma_r}
$$

Una gran diferencia entre el mayor y el menor valor singular indica una matriz **mal condicionada** (potencialmente inestable para inversi√≥n o resoluci√≥n de sistemas lineales).

### üîµ Ejemplo visual con el c√≠rculo unitario

La transformaci√≥n de un c√≠rculo unitario por una matriz $A$ lo convierte en una **elipse**:

* **Ejes de la elipse**: dados por los vectores propios de $A^TA$ (columnas de $V$)
* **Longitud de los ejes**: igual a los valores singulares $\sigma_i$
* **Orientaci√≥n**: definida por las matrices $U$ y $V$

### üß† Aplicaciones:

| Aplicaci√≥n                       | Impacto de los valores singulares                                               |
| -------------------------------- | ------------------------------------------------------------------------------- |
| **Compresi√≥n de im√°genes**       | Usar solo los $k$ mayores $\sigma_i$                                            |
| **Reducci√≥n de dimensionalidad** | PCA se basa en valores singulares                                               |
| **Detecci√≥n de redundancia**     | $\sigma_i \approx 0$ indica dependencia lineal                                  |
| **Estabilidad num√©rica**         | Matrices con valores singulares muy peque√±os pueden generar errores de redondeo |

### üêç En Python (ejemplo r√°pido)

```python
import numpy as np
A = np.array([[3, 1], [1, 3]])
U, S, VT = np.linalg.svd(A)
print("Valores singulares:", S)
```

Esto te dice cu√°nto **deforma** tu matriz el espacio y en qu√© direcciones principales.

### Resumen

#### ¬øC√≥mo intervienen los valores singulares en una transformaci√≥n?

Entender la importancia de los valores singulares es fundamental para cualquier analista de datos que busque profundizar en la descomposici√≥n de matrices y sus aplicaciones pr√°cticas. Al descomponer una matriz, obtenemos tres matrices: U, D y V, donde D es diagonal y est√° compuesta por los valores singulares. Pero, ¬øqu√© provoca realmente la aplicaci√≥n de estos valores en una transformaci√≥n? Descubrir esto nos permite analizar c√≥mo los valores singulares influyen en las direcciones principales y la extensi√≥n en las que el transformador impacta al espacio de datos.

#### ¬øC√≥mo se implementa la descomposici√≥n de matrices en Python?

El trabajo comienza llamando a las bibliotecas necesarias y definiendo una matriz a descomponer. Utilizando numpy (alias `np`) y `matplotlib`, podemos no solo ejecutar operaciones matem√°ticas complejas, sino tambi√©n visualizar el efecto de estas descomposiciones:

```python
import numpy as np
from matplotlib import pyplot as plt

# Definimos la matriz a descomponer
A = np.array([[3, 7], [5, 2]])

# Calculamos la descomposici√≥n en valores singulares
U, D, Vt = np.linalg.svd(A)
```

Con `numpy.linalg.svd()`, se realiza la descomposici√≥n SVD de la matriz, devolviendo las matrices U, D (en forma de un vector diagonal) y V transpuesta, permitiendo as√≠ el an√°lisis de sus efectos individuales.

#### ¬øC√≥mo afectan los valores singulares a los vectores?

Una vez que tenemos los valores singulares, podemos ver su influencia directa sobre los vectores al aplicar las matrices resultantes de la descomposici√≥n. Por ejemplo, al definir un nuevo conjunto de vectores, observamos c√≥mo se transforman al aplicar los valores singulares:

```python
# Definici√≥n de vectores
vector_1 = np.array([1, 0])
vector_2 = np.array([0, 1])

# Aplicaci√≥n de D a los vectores
d_vector_1 = D[0] * np.array([U[0,0], U[1,0]])
d_vector_2 = D[1] * np.array([U[0,1], U[1,1]])
```

Los valores de la matriz diagonal `D` escalan los vectores direccionales transform√°ndolos, ampliando o reduciendo su longitud seg√∫n su magnitud.

#### ¬øC√≥mo visualizamos las transformaciones de los vectores?

Para comprender mejor estas transformaciones, utilizamos gr√°ficos en `matplotlib` para visualizar tanto la matriz original como los vectores transformados por los valores singulares. Al trazar estos en un gr√°fico, podemos contrastar c√≥mo cambian tanto el tama√±o como la direcci√≥n de los vectores iniciales y transformados:

```python
# Visualizaci√≥n del efecto de la descomposici√≥n
plt.quiver(0, 0, vector_1[0], vector_1[1], angles='xy', scale_units='xy', scale=1, color='r', label='Vector Original 1')
plt.quiver(0, 0, d_vector_1[0], d_vector_1[1], angles='xy', scale_units='xy', scale=1, color='b', label='Vector Transformado 1')
plt.quiver(0, 0, vector_2[0], vector_2[1], angles='xy', scale_units='xy', scale=1, color='g', label='Vector Original 2')
plt.quiver(0, 0, d_vector_2[0], d_vector_2[1], angles='xy', scale_units='xy', scale=1, color='y', label='Vector Transformado 2')

plt.xlim(-8, 8)
plt.ylim(-8, 8)
plt.grid()
plt.legend()
plt.show()
```

Al visualizar estos cambios, se obtiene una imagen clara de c√≥mo los valores singulares transforman los vectores, revelando sus efectos de escala y rotaci√≥n que impactan desde distintas direcciones.

#### ¬øPor qu√© es importante el an√°lisis de valores singulares?

El an√°lisis de valores singulares es esencial para diversas aplicaciones, desde la compresi√≥n de im√°genes hasta el reconocimiento de patrones. Este enfoque permite descomponer las transformaciones complejas en alteraciones manejables en escala y direcci√≥n. Adem√°s, comprender estos conceptos abre nuevas oportunidades para mejorar procesos anal√≠ticos y modelar datos con precisi√≥n superior.

En conclusi√≥n, el estudio y la aplicaci√≥n pr√°ctica de valores singulares nos proporcionan una herramienta extraordinariamente potente para manipular y comprender nuestras matrices y sus transformaciones.

## Procesamiento de Im√°genes: Escala de Grises y Normalizaci√≥n

El **procesamiento de im√°genes** es fundamental en visi√≥n por computadora y machine learning. Dos pasos clave para preparar im√°genes son:

### üñ§ Escala de Grises

### ‚úÖ ¬øQu√© es?

Transformar una imagen en color (RGB) a una imagen en **escala de grises** significa reducir los 3 canales de color a un solo canal, que representa **intensidades de luz (luminancia)**.

### üéØ ¬øPor qu√© usarla?

* Reduce la complejidad computacional.
* Es suficiente para tareas donde el color no aporta valor (detecci√≥n de bordes, reconocimiento facial, etc.).
* Facilita algoritmos como filtrado, segmentaci√≥n y detecci√≥n de contornos.

### üî£ F√≥rmula t√≠pica (luminancia):

$$
\text{Gris} = 0.2989 \cdot R + 0.5870 \cdot G + 0.1140 \cdot B
$$

### üßÆ Normalizaci√≥n

### ‚úÖ ¬øQu√© es?

Es el proceso de **escalar los valores de p√≠xeles** a un rango est√°ndar, generalmente entre **0 y 1** o **-1 y 1**.

### üéØ ¬øPor qu√© usarla?

* Mejora la **convergencia** y **estabilidad** en modelos de machine learning.
* Evita que valores grandes dominen sobre los peque√±os.
* Permite comparar im√°genes con diferentes rangos de iluminaci√≥n.

### üìå M√©todos comunes:

* **Min-Max Scaling**:

  $$
  x_{\text{norm}} = \frac{x - \min(x)}{\max(x) - \min(x)}
  $$
* **Dividir entre 255**: si los valores de p√≠xeles van de 0 a 255, entonces:

  $$
  x_{\text{norm}} = \frac{x}{255}
  $$

### üêç Ejemplo en Python (usando `OpenCV` y `NumPy`):

```python
import cv2
import numpy as np

# Leer imagen en escala de grises
img_color = cv2.imread('imagen.jpg')        # Imagen en color
img_gray = cv2.cvtColor(img_color, cv2.COLOR_BGR2GRAY)  # Escala de grises

# Normalizaci√≥n a rango [0, 1]
img_normalized = img_gray / 255.0

# Ver dimensiones y valores
print(img_gray.shape, img_normalized.min(), img_normalized.max())
```

### üîç Aplicaciones clave:

| Tarea                    | ¬øGrises? | ¬øNormalizaci√≥n? |
| ------------------------ | -------- | --------------- |
| Detecci√≥n de bordes      | ‚úÖ        | Opcional        |
| Reconocimiento facial    | ‚úÖ        | ‚úÖ               |
| Segmentaci√≥n de im√°genes | ‚úÖ/‚ùå      | ‚úÖ               |
| Modelos de deep learning | ‚úÖ/‚ùå      | ‚úÖ               |

### Resumen

#### ¬øC√≥mo procesar im√°genes en programaci√≥n utilizando matrices?

Procesar im√°genes en programaci√≥n no es solo una habilidad t√©cnica, sino una puerta de entrada al entendimiento profundo del reconocimiento de patrones y la compresi√≥n de datos. Al analizar una imagen, en realidad, lo que estamos observando son matrices de datos que representan los colores y la intensidad de cada p√≠xel. Exploraremos c√≥mo emplear herramientas como Python y sus bibliotecas para manipular im√°genes de manera efectiva.

#### ¬øQu√© bibliotecas se necesitan?

Para trabajar con im√°genes en Python, utilizaremos las siguientes bibliotecas:

- **PIL (Pillow)**: Permite la apertura, manipulaci√≥n y guardado de diferentes formatos de imagen.
- **Numpy**: Facilita operaciones matem√°ticas y es extremadamente √∫til para convertir im√°genes en matrices y viceversa.
- **Matplotlib**: Nos ayuda a visualizar im√°genes matrices al igual que sus alteraciones.

```python
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
```

#### ¬øC√≥mo abrir y mostrar una imagen?

Para abrir y mostrar una imagen, primero la cargamos usando `PIL` y luego la visualizamos usando `Matplotlib`. Aseg√∫rate de especificar la ruta correcta a tu imagen y las barras dobles si trabajas en Windows.

```python
imagen = Image.open("Imagenes\\cabra.jpg")
plt.imshow(imagen)
plt.show()
```

#### ¬øC√≥mo transformar una imagen a escala de grises?

La conversi√≥n a escala de grises es un paso crucial en preprocesamiento. Para hacerlo, utilizamos la funci√≥n `convert('L')` de Pillow, que transforma la imagen conservando solo la luminancia.

```python
imagen_gris = imagen.convert("L")
plt.imshow(imagen_gris, cmap='gray')
plt.show()
```

#### ¬øC√≥mo convertir una imagen en una matriz?

Convertimos nuestra imagen en escala de grises a una matriz utilizando `Numpy`. Esto nos permitir√° analizar y modificar directamente los valores de los p√≠xeles.

```python
matriz_imagen = np.array(imagen_gris)
print(matriz_imagen.shape) # Visualizar la forma de la matriz
```

#### ¬øC√≥mo aplicar operaciones sobre matrices?

Trabajar directamente con los datos nos permite realizar operaciones matem√°ticas avanzadas. Por ejemplo, dividir los valores de la matriz para normalizar los datos entre 0 y 1, lo que es com√∫n en Machine Learning.

```python
matriz_normalizada = matriz_imagen / 255.0
plt.imshow(matriz_normalizada, cmap='gray')
plt.show()
```

#### ¬øPor qu√© normalizar la matriz de la imagen?

La normalizaci√≥n ayuda en:

- **Optimizaci√≥n**: Al tener todos los valores entre 0 y 1, se facilitan c√°lculos y converge m√°s r√°pido en procesos de aprendizaje de m√°quina.
- **Comparabilidad**: Nos permite mantener relaciones proporcionales y consistentes entre distintos datos.

#### ¬øCu√°l es el impacto de alterar la matriz?

Modificando directamente la matriz, podemos cambiar c√≥mo se presenta una imagen. Por ejemplo, dividir todos los valores por 10 alterar√° la percepci√≥n de brillo en la imagen. Esto es muy √∫til para experimentar y entender c√≥mo las transformaciones afectan la imagen final.

```python
matriz_alterada = matriz_imagen / 10.0
plt.imshow(matriz_alterada, cmap='gray')
plt.show()
```

Este enfoque nos permite un amplio control en el preprocesamiento de im√°genes y es un paso vital hacia t√©cnicas m√°s avanzadas como la reducci√≥n dimensional y la compresi√≥n de im√°genes. Contin√∫a explorando este mundo fascinante y convi√©rtete en un experto en procesamiento de im√°genes en Python. Siempre hay nuevas t√©cnicas y herramientas emergentes que enriquecen el panorama de la ciencia de datos y la inteligencia artificial cada d√≠a.

## Descomposici√≥n de im√°genes: reducci√≥n de tama√±o y reconstrucci√≥n eficaz

La **descomposici√≥n de im√°genes** mediante t√©cnicas como **SVD (Singular Value Decomposition)** permite **reducir el tama√±o** de una imagen y luego **reconstruirla** de manera eficaz, preservando la mayor parte de la informaci√≥n visual.

### üß† ¬øQu√© es la descomposici√≥n SVD?

La descomposici√≥n SVD de una matriz `A` (por ejemplo, la matriz de una imagen en escala de grises) permite escribirla como:

$$
A = U \cdot \Sigma \cdot V^T
$$

* `U`: matriz de vectores propios (izquierda)
* `Œ£` (Sigma): matriz diagonal con los **valores singulares**
* `V^T`: matriz transpuesta de vectores propios (derecha)

### üì∑ Aplicaci√≥n en Im√°genes

Las im√°genes (en escala de grises) pueden representarse como matrices $m \times n$, donde cada valor representa la intensidad de un p√≠xel.

Al aplicar SVD a esa matriz, podemos:

1. **Guardar solo los primeros $k$ valores singulares m√°s grandes**, reduciendo datos.
2. **Reconstruir la imagen** con aproximaci√≥n aceptable mediante:

$$
A_k \approx U_k \cdot \Sigma_k \cdot V_k^T
$$

### ‚úÖ Ventajas

* **Compresi√≥n eficiente**: con solo unos pocos componentes se puede recrear bien la imagen.
* **Reducci√≥n de ruido**: al ignorar los valores singulares m√°s peque√±os.

### üß™ Ejemplo pr√°ctico en Python

```python
import numpy as np
import matplotlib.pyplot as plt
from skimage.color import rgb2gray
from skimage.io import imread

# Cargar imagen y convertir a escala de grises
imagen = rgb2gray(imread('imagen.jpg'))

# Aplicar SVD
U, S, Vt = np.linalg.svd(imagen, full_matrices=False)

# Reconstrucci√≥n con k componentes
k = 50
S_k = np.diag(S[:k])
U_k = U[:, :k]
Vt_k = Vt[:k, :]
reconstruida = U_k @ S_k @ Vt_k

# Mostrar imagen original y reconstruida
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.title("Original")
plt.imshow(imagen, cmap='gray')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.title(f"Reconstruida con k={k}")
plt.imshow(reconstruida, cmap='gray')
plt.axis('off')

plt.show()
```

### üéØ Conclusi√≥n

La descomposici√≥n SVD:

* Es una herramienta poderosa para **compresi√≥n y an√°lisis de im√°genes**.
* Permite ajustar la cantidad de informaci√≥n retenida usando el par√°metro $k$.
* Es √∫til en tareas de **reconstrucci√≥n, reducci√≥n de ruido y compresi√≥n sin p√©rdida significativa**.

### Resumen

#### ¬øC√≥mo aplicar la descomposici√≥n SVD a una imagen?

La descomposici√≥n en valores singulares (SVD) es una t√©cnica matem√°tica potente que nos permite reducir la dimensionalidad de una matriz sin perder informaci√≥n esencial. A menudo, se aplica en el procesamiento de im√°genes para comprimir archivos manteniendo una buena calidad visual. Pero, ¬øc√≥mo aplicamos esta t√©cnica a una imagen concreta? Veamos el proceso completo y c√≥mo esto afecta la reconstrucci√≥n de la imagen.

#### ¬øQu√© librer√≠as son esenciales para el procesamiento de im√°genes?

Para comenzar con nuestro an√°lisis, es necesario importar las librer√≠as adecuadas. Utilizaremos principalmente matplotlib para graficar y tratar im√°genes, y numpy para manejar n√∫meros y realizar operaciones matem√°ticas:

```python
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
```

Estas librer√≠as nos permitir√°n cargar, manipular y visualizar nuestra imagen de manera eficiente.

#### ¬øC√≥mo cargar y preparar la imagen?

Primero, cargamos nuestra imagen y la convertimos a escala de grises. Esto simplifica el proceso al reducir la informaci√≥n de color a una sola banda:

```python
img_path = "ruta/a/tu/imagen.jpg"
imagen_color = Image.open(img_path)
imagen_gray = imagen_color.convert('L')
imagen_array = np.array(imagen_gray, dtype=float)import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
```

Esta conversi√≥n a escala de grises tambi√©n nos facilita realizar la descomposici√≥n SVD, ya que trabajaremos con una matriz m√°s sencilla.

#### ¬øC√≥mo se realiza la descomposici√≥n en valores singulares (SVD)?

Utilizamos numpy para calcular la descomposici√≥n SVD de la matriz que representa nuestra imagen:

`U, S, Vt = np.linalg.svd(imagen_array, full_matrices=False)`

Aqu√≠, `U` y `Vt` son matrices ortogonales, mientras que `S` es un vector que contiene los valores singulares. Estos valores singulares est√°n ordenados de mayor a menor, identificando los componentes m√°s significativos de la imagen.

#### ¬øC√≥mo reconstruir la imagen utilizando SVD?

La reconstrucci√≥n de la imagen usando una cantidad reducida de valores singulares es clave para la compresi√≥n. Utilizamos solo los valores singulares m√°s grandes, ya que representan la mayor parte de la informaci√≥n visual:

```python
img_path = "ruta/a/tu/imagen.jpg"
imagen_color = Image.open(img_path)
imagen_gray = imagen_color.convert('L')
imagen_array = np.array(imagen_gray, dtype=float)k = 50  # n√∫mero de valores singulares considerados
S_k = np.diag(S[:k])
U_k = U[:, :k]
Vt_k = Vt[:k, :]
imagen_reconstruida = np.dot(U_k, np.dot(S_k, Vt_k))import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
```

Al variar `k`, podemos observar el efecto que tiene en la calidad de la imagen reconstruida. Cuanto mayor sea k, mejor ser√° la calidad visual pero tambi√©n mayor el tama√±o del archivo.

#### ¬øQu√© tama√±o de archivo y calidad de imagen obtenemos al variar k?

La elecci√≥n de `k` afecta directamente al tama√±o final de nuestro archivo y la claridad de la imagen reconstruida. Aqu√≠ algunas consideraciones:

- **Con pocos valores singulares**: Tendremos un archivo muy comprimido pero con p√©rdida de detalles. La imagen ser√° menos clara.
- **Con muchos valores singulares**: Conservamos m√°s detalles finos pero el tama√±o del archivo es mayor.

La habilidad para elegir el `k` √≥ptimo depende del prop√≥sito: si se necesita reconocimiento m√°s que calidad visual, menos valores ser√°n suficientes.

Concluyendo, la t√©cnica SVD aplicada a im√°genes logra un equilibrio √∫nico entre compresi√≥n y calidad, permitiendo optimizar recursos de almacenamiento sin sacrificar demasiada informaci√≥n visual. Te invitamos a experimentar y determinar cu√°l es el valor `k` ideal para tus necesidades espec√≠ficas. ¬°Explora nuevas formas de eficientizar tus proyectos de im√°genes!

## Compresi√≥n de Im√°genes Usando Descomposici√≥n en Valores Singulares

La **compresi√≥n de im√°genes usando Descomposici√≥n en Valores Singulares (SVD)** es una t√©cnica poderosa que reduce el tama√±o de una imagen manteniendo su calidad visual con una aproximaci√≥n eficiente.

### üîç ¬øQu√© es la compresi√≥n con SVD?

Dada una imagen como una matriz $A$, aplicamos la descomposici√≥n SVD:

$$
A = U \cdot \Sigma \cdot V^T
$$

Para comprimir:

* Nos quedamos solo con los **primeros $k$** valores singulares de $\Sigma$, que contienen la mayor parte de la energ√≠a (informaci√≥n) de la imagen.
* Esto reduce dr√°sticamente el n√∫mero de elementos necesarios para representar la imagen.

### üìâ ¬øCu√°nto se reduce?

Si la imagen es de tama√±o $m \times n$, almacenar todo requiere $m \cdot n$ elementos.
Con SVD, almacenamos:

$$
k \cdot (m + n + 1)
$$

¬°Para un $k$ peque√±o, la reducci√≥n puede ser de m√°s del 90%!

### üíª Ejemplo en Python

```python
import numpy as np
import matplotlib.pyplot as plt
from skimage.color import rgb2gray
from skimage.io import imread

# Leer y convertir a escala de grises
imagen = rgb2gray(imread('paisaje.jpg'))

# SVD
U, S, Vt = np.linalg.svd(imagen, full_matrices=False)

# Compresi√≥n con diferentes k
ks = [5, 20, 50, 100]

plt.figure(figsize=(12, 8))

for i, k in enumerate(ks, 1):
    # Aproximar imagen
    Uk = U[:, :k]
    Sk = np.diag(S[:k])
    Vk = Vt[:k, :]
    A_k = Uk @ Sk @ Vk
    
    # Mostrar
    plt.subplot(2, 3, i)
    plt.imshow(A_k, cmap='gray')
    plt.title(f'k = {k}')
    plt.axis('off')

# Imagen original
plt.subplot(2, 3, 6)
plt.imshow(imagen, cmap='gray')
plt.title('Original')
plt.axis('off')

plt.tight_layout()
plt.show()
```

### üß† Ventajas

* üîπ **Compresi√≥n sin p√©rdidas perceptibles** (para valores bajos de $k$)
* üîπ **Reducci√≥n de ruido**
* üîπ **F√°cil de implementar con NumPy**

### üìä Visualizaci√≥n de error

Puedes calcular el error de reconstrucci√≥n con:

```python
error = np.linalg.norm(imagen - A_k) / np.linalg.norm(imagen)
```

Esto te permite elegir un $k$ que balancee calidad y compresi√≥n.

### üéØ Conclusi√≥n

La SVD permite:

* **Reducir almacenamiento**
* **Transmitir im√°genes comprimidas**
* **Controlar la calidad vs tama√±o con $k$**

## Compresi√≥n de Im√°genes Usando Descomposici√≥n en Valores Singulares

La **compresi√≥n de im√°genes usando Descomposici√≥n en Valores Singulares (SVD)** es una t√©cnica poderosa que reduce el tama√±o de una imagen manteniendo su calidad visual con una aproximaci√≥n eficiente.

### üîç ¬øQu√© es la compresi√≥n con SVD?

Dada una imagen como una matriz $A$, aplicamos la descomposici√≥n SVD:

$$
A = U \cdot \Sigma \cdot V^T
$$

Para comprimir:

* Nos quedamos solo con los **primeros $k$** valores singulares de $\Sigma$, que contienen la mayor parte de la energ√≠a (informaci√≥n) de la imagen.
* Esto reduce dr√°sticamente el n√∫mero de elementos necesarios para representar la imagen.

### üìâ ¬øCu√°nto se reduce?

Si la imagen es de tama√±o $m \times n$, almacenar todo requiere $m \cdot n$ elementos.
Con SVD, almacenamos:

$$
k \cdot (m + n + 1)
$$

¬°Para un $k$ peque√±o, la reducci√≥n puede ser de m√°s del 90%!

### üíª Ejemplo en Python

```python
import numpy as np
import matplotlib.pyplot as plt
from skimage.color import rgb2gray
from skimage.io import imread

# Leer y convertir a escala de grises
imagen = rgb2gray(imread('paisaje.jpg'))

# SVD
U, S, Vt = np.linalg.svd(imagen, full_matrices=False)

# Compresi√≥n con diferentes k
ks = [5, 20, 50, 100]

plt.figure(figsize=(12, 8))

for i, k in enumerate(ks, 1):
    # Aproximar imagen
    Uk = U[:, :k]
    Sk = np.diag(S[:k])
    Vk = Vt[:k, :]
    A_k = Uk @ Sk @ Vk
    
    # Mostrar
    plt.subplot(2, 3, i)
    plt.imshow(A_k, cmap='gray')
    plt.title(f'k = {k}')
    plt.axis('off')

# Imagen original
plt.subplot(2, 3, 6)
plt.imshow(imagen, cmap='gray')
plt.title('Original')
plt.axis('off')

plt.tight_layout()
plt.show()
```

### üß† Ventajas

* üîπ **Compresi√≥n sin p√©rdidas perceptibles** (para valores bajos de $k$)
* üîπ **Reducci√≥n de ruido**
* üîπ **F√°cil de implementar con NumPy**

### üìä Visualizaci√≥n de error

Puedes calcular el error de reconstrucci√≥n con:

```python
error = np.linalg.norm(imagen - A_k) / np.linalg.norm(imagen)
```

Esto te permite elegir un $k$ que balancee calidad y compresi√≥n.

### üéØ Conclusi√≥n

La SVD permite:

* **Reducir almacenamiento**
* **Transmitir im√°genes comprimidas**
* **Controlar la calidad vs tama√±o con $k$**

## C√°lculo de la seudo inversa de Moore-Penrose en Python

La **seudoinversa de Moore-Penrose** es una generalizaci√≥n de la inversa de una matriz que puede aplicarse incluso si la matriz no es cuadrada o no es invertible. En Python, puedes calcularla f√°cilmente con NumPy.

### üìå ¬øQu√© es la seudo inversa?

Para una matriz $A \in \mathbb{R}^{m \times n}$, su seudoinversa $A^+$ satisface ciertas propiedades algebraicas. Se define mediante la descomposici√≥n en valores singulares (SVD):

$$
A = U \Sigma V^T \quad \Rightarrow \quad A^+ = V \Sigma^+ U^T
$$

Donde:

* $\Sigma^+$ se obtiene invirtiendo los valores singulares distintos de cero y transponiendo la matriz.

### üíª C√°lculo con NumPy

### ‚úÖ Usando `np.linalg.pinv`

```python
import numpy as np

A = np.array([[1, 2], [3, 4], [5, 6]])  # Matriz no cuadrada
A_pseudo = np.linalg.pinv(A)

print("Matriz original A:")
print(A)
print("\nSeudo inversa de A (Moore-Penrose):")
print(A_pseudo)
```

### üß† ¬øQu√© hace NumPy internamente?

`np.linalg.pinv` utiliza la **SVD** para calcular la seudo inversa:

```python
U, S, Vt = np.linalg.svd(A, full_matrices=False)

# Invertimos los valores singulares (evitando dividir por cero)
S_inv = np.diag(1 / S)

# Calculamos la seudoinversa manualmente
A_pseudo_manual = Vt.T @ S_inv @ U.T
```

### üìò Aplicaci√≥n: resolver sistemas sobredeterminados

Para sistemas $Ax = b$ donde $A$ no es cuadrada:

```python
x = np.linalg.pinv(A) @ b
```

Esto da la **soluci√≥n de m√≠nimos cuadrados**: la mejor aproximaci√≥n posible.

### ‚úÖ Ventajas

* Funciona con matrices **rectangulares o singulares**.
* Muy √∫til en **regresi√≥n lineal**, **ML**, **ajuste de curvas**, y **optimizaci√≥n**.

### Resumen

La **pseudoinversa de Moore Penrose** es una aplicaci√≥n directa de *singular value decomposition (*SVD), que nos permite resolver en determinados momentos sistemas de ecuaciones lineales con m√∫ltiples soluciones.

La matriz pseudoinversa es utilizada cuando en un sistema de ecuaciones lineales, representado por Ax = B, x no tiene inversa. Esta operaci√≥n es √∫nica y existe si se verifican 4 condiciones.

**Ejemplo de pseudoinversa de una matriz**

En el siguiente ejemplo, ver√°s las 4 condiciones para obtener una f√≥rmula Penrose.

![pseudoinversa](images/pseudoinversa.png)

**C√≥mo calcular la matriz pseudoinversa de Moore Penrose**

Para calcularla se siguen los siguientes pasos:

- Calcular las matrices U, D, y V (matrices SVD) de A.
- Construir D_pse: una matriz de ceros que tiene igual dimension de A, y que luego se transpone.
- Reemplazar la submatriz D_pse[: D.shape[0], : D.shape[0]] por np.linalg.inv(np.diag(D))
- Reconstruir pseudoinversa: A_pse = V.T.dot(D_pse).dot(U.T)

**C√≥mo calcular la pseudoinversa de Moore Penrose en Python**

Para calcularla autom√°ticamente por Python: np.linalg.pinv(A)Lo que obtenemos con A_pse es una matriz muy cercana a la inversa. Cercano en el sentido de que minimiza la norma dos de estas distancias. O sea, de estos errores que estamos cometiendo.

A_pse no es conmutativa, es decir, A_pse¬∑A ‚â† A¬∑A_pse

## Soluci√≥n de Sistemas Sobredeterminados con Pseudo-Inversa y Python

La **soluci√≥n de sistemas sobredeterminados** (m√°s ecuaciones que inc√≥gnitas) es com√∫n en estad√≠stica, machine learning y procesamiento de se√±ales. Cuando el sistema no tiene soluci√≥n exacta, usamos la **pseudo-inversa de Moore-Penrose** para obtener una **soluci√≥n por m√≠nimos cuadrados**.

### üßÆ Ejemplo de sistema sobredeterminado

Sup√≥n que tienes el sistema:

$$
A \cdot x = b
$$

Donde:

* $A$ es una matriz $m \times n$, con $m > n$ (m√°s ecuaciones que inc√≥gnitas).
* No siempre hay soluci√≥n exacta, pero queremos minimizar el error $\|Ax - b\|^2$.

La soluci√≥n por m√≠nimos cuadrados es:

$$
x = A^+ \cdot b
$$

Donde $A^+$ es la pseudo-inversa de Moore-Penrose de $A$.

### ‚úÖ Implementaci√≥n en Python con NumPy

```python
import numpy as np

# Matriz A (m√°s filas que columnas)
A = np.array([
    [1, 1],
    [1, 2],
    [1, 3]
])

# Vector b
b = np.array([1, 2, 2])

# C√°lculo de la pseudo-inversa de A
A_pseudo = np.linalg.pinv(A)

# Soluci√≥n por m√≠nimos cuadrados
x = A_pseudo @ b

print("Soluci√≥n x:")
print(x)

# Verificaci√≥n: Aproximaci√≥n de Ax
print("Aproximaci√≥n Ax:")
print(A @ x)
```

### üîç Salida esperada:

```text
Soluci√≥n x:
[0.5 0.5]

Aproximaci√≥n Ax:
[1.  1.5 2. ]
```

Esto minimiza la distancia entre el vector real $b = [1, 2, 2]$ y la estimaci√≥n $Ax$.

### üìå Alternativas con `np.linalg.lstsq` (m√°s estable):

```python
x_lstsq, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)
print("Soluci√≥n con lstsq:", x_lstsq)
```

### üîß ¬øD√≥nde se usa?

* Ajuste de modelos lineales
* Reconstrucci√≥n de im√°genes
* Soluci√≥n de ecuaciones inconsistentes en sistemas reales

### Resumen

#### ¬øQu√© es un sistema de ecuaciones y c√≥mo se resuelve?

Un sistema de ecuaciones es un conjunto de ecuaciones con varias inc√≥gnitas. Este sistema puede tener cero, una o infinitas soluciones. En situaciones donde el sistema tiene una soluci√≥n √∫nica, esto indica que existe una matriz inversa y que la matriz es cuadrada con vectores linealmente independientes. Cuando buscamos una soluci√≥n X tal que minimice una norma espec√≠fica, podemos recurrir a un m√©todo llamado pseudo inversa.

#### ¬øC√≥mo aplicar la pseudo inversa para resolver un sistema?

La pseudo inversa es √∫til para encontrar una soluci√≥n X que minimice la norma de a por X menos B en un sistema de ecuaciones lineales. Esto es importante en sistemas sobre determinados, donde hay m√°s ecuaciones que inc√≥gnitas.

#### Visualizaci√≥n inicial con Python

Para abordar este problema con c√≥digo, se pueden seguir estos pasos:

1. **Configuraci√≥n de entorno y librer√≠as**: Inicialmente, se importa `numpy` como `np` y se utiliza `matplotlib` para visualizar gr√°ficamente.

```python
import numpy as np
import matplotlib.pyplot as plt
```

2. **Definici√≥n del dominio**: Se establece un rango de valores de X para evaluar las ecuaciones del sistema:

```python
x = np.linspace(-5, 5, 1000)
```

3. **Definici√≥n de funciones**: Se crean las funciones correspondientes a las tres ecuaciones del sistema:

```python
y1 = -4 * x + 3
y2 = 2 * x + 5
y3 = -3 * x + 1
```

#### Graficaci√≥n del sistema

Las ecuaciones se grafican para visualizar su intersecci√≥n:

```python
plt.plot(x, y1, label='y1 = -4x + 3')
plt.plot(x, y2, label='y2 = 2x + 5')
plt.plot(x, y3, label='y3 = -3x + 1')
plt.xlim(-2, 2.5)
plt.ylim(-6, 6)
plt.legend()
plt.show()
```

En el gr√°fico, es evidente que las rectas definidas por estas ecuaciones no cruzan en un √∫nico punto com√∫n.

#### ¬øC√≥mo usar la pseudo inversa para encontrar el punto √≥ptimo?

Para resolver este problema gr√°ficamente y encontrar el punto que minimice la norma dos, seguimos estos pasos:

1. **Definir la matriz y el vector de soluciones**:

La matriz A se construye con los coeficientes de las ecuaciones:

```python
A = np.array([[4, 1], [-2, 1], [3, 1]])
b = np.array([3, 5, 1])  # Vector de soluciones
```

2. **Calcular la pseudo inversa**:

Utilizamos `numpy.linalg.pinv` para calcularla:

`A_pseudo_inv = np.linalg.pinv(A)`

3. **Encontrar la soluci√≥n X**:

Multiplicamos la pseudo inversa de A con el vector b:

`X = A_pseudo_inv @ b`

Este c√°lculo nos proporciona una soluci√≥n que podemos interpretar gr√°ficamente.

#### Interpretaci√≥n gr√°fica y an√°lisis

Al graficar nuevamente y a√±adir el punto encontrado:

```python
plt.plot(x, y1, label='y1 = -4x + 3')
plt.plot(x, y2, label='y2 = 2x + 5')
plt.plot(x, y3, label='y3 = -3x + 1')
plt.scatter(X[0], X[1], color='red', zorder=5)  # Punto soluci√≥n
plt.xlim(-2, 2.5)
plt.ylim(-6, 6)
plt.legend()
plt.show()
```

El punto soluci√≥n no siempre se encuentra en el centro del "tri√°ngulo" formado por las ecuaciones debido al diferente peso que cada una ejerce sobre √©l, como una suerte de centro de gravedad.

La pseudo inversa proporciona una herramienta efectiva para manejar sistemas sobre determinados, ayud√°ndonos a encontrar soluciones √≥ptimas que norman la minimizaci√≥n. Siguiendo este enfoque, podemos comprender y resolver problemas complejos en la matem√°tica aplicada. ¬°Sigue adelante y contin√∫a aprendiendo!

## Reducci√≥n de Dimensionalidad en An√°lisis de Datos: PCA Aplicado

### üìò ¬øQu√© es la Reducci√≥n de Dimensionalidad?

Es el proceso de transformar un conjunto de datos con muchas **variables (dimensiones)** en un conjunto m√°s peque√±o que retiene la **mayor cantidad de informaci√≥n posible**. Sirve para:

* Visualizar datos complejos.
* Eliminar ruido o redundancias.
* Acelerar algoritmos de aprendizaje autom√°tico.
* Evitar el problema de la **maldici√≥n de la dimensionalidad**.

### üîç ¬øQu√© es PCA?

El **An√°lisis de Componentes Principales (PCA)** es una t√©cnica lineal de reducci√≥n de dimensionalidad. Busca encontrar nuevas **variables no correlacionadas** (componentes principales) que:

1. Sean combinaciones lineales de las variables originales.
2. Ordenen los datos seg√∫n la **varianza** que aportan.

**Ejemplo:** si tienes 10 variables, puedes reducirlas a 2 o 3 que expliquen la mayor√≠a de la variaci√≥n.

### üß™ Ejemplo Pr√°ctico en Python

Vamos a usar el famoso conjunto de datos **Iris** para aplicar PCA.

```python
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import pandas as pd

# 1. Cargar datos
iris = load_iris()
X = iris.data
y = iris.target
labels = iris.target_names

# 2. Escalado de los datos
X_scaled = StandardScaler().fit_transform(X)

# 3. Aplicar PCA (reducir a 2 dimensiones)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# 4. Visualizar
plt.figure(figsize=(8, 6))
for i in range(len(labels)):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=labels[i])
plt.xlabel("Componente Principal 1")
plt.ylabel("Componente Principal 2")
plt.title("PCA aplicado al dataset Iris")
plt.legend()
plt.grid(True)
plt.show()
```

### üìä ¬øQu√© nos dice `pca.explained_variance_ratio_`?

Este atributo te muestra **cu√°nta informaci√≥n (varianza)** conserva cada componente.

```python
print(pca.explained_variance_ratio_)
```

Un ejemplo de salida:

```text
[0.72, 0.23]
```

Esto significa que el **95% de la informaci√≥n total** del dataset est√° capturada en los primeros 2 componentes.

### ‚úÖ Ventajas de PCA

* Reduce la complejidad computacional.
* Mejora la visualizaci√≥n de datos multivariados.
* Puede mejorar el rendimiento de modelos supervisados.
* Es √∫til para **preprocesar im√°genes, se√±ales, datos de sensores**, etc.

### üö´ Cu√°ndo *no* usar PCA

* Cuando necesitas interpretar el efecto directo de las variables originales (PCA transforma los datos).
* Cuando los datos no tienen relaciones lineales (para eso existen m√©todos no lineales como t-SNE o UMAP).

### Resumen

#### ¬øC√≥mo abordar la maldici√≥n de la dimensi√≥n en an√°lisis de datos? 
 La maldici√≥n de la dimensi√≥n es un problema com√∫n en estad√≠stica que surge al incrementar las variables en un conjunto de datos. Si a√±adimos m√°s dimensiones, necesitaremos exponencialmente m√°s muestras para mantener la relevancia estad√≠stica. Por lo tanto, comprender c√≥mo eliminar dimensiones innecesarias es crucial para mejorar el an√°lisis de datos.

#### ¬øC√≥mo generar una muestra de datos en Python?

En primer lugar, es fundamental configurar y definir correctamente nuestras muestras de datos antes de realizar experimentos. Utilizamos librer√≠as como `numpy` y `matplotlib` para este prop√≥sito. Aqu√≠ te mostramos c√≥mo puedes hacerlo:

```python
import numpy as np
import matplotlib.pyplot as plt

# Definimos la semilla para reproducir el experimento
np.random.seed(0)

# Generamos n√∫meros aleatorios
X = 3 * np.random.rand(200, 1)
y = 20 + 20 * X + 2 * np.random.rand(200, 1)

# Formateamos las matrices
X = X.reshape((200, 1))
y = y.reshape((200, 1))

# Concatenamos los vectores
XY = np.hstack((X, y))

# Visualizamos la dispersi√≥n de los puntos
plt.scatter(XY[:, 0], XY[:, 1], marker='o', color='b')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

Este c√≥digo genera muestras y visualiza la correlaci√≥n entre las variables.

#### ¬øC√≥mo transformar el sistema de referencia para reducir dimensiones?

El paso clave en la reducci√≥n de dimensiones es centrar los datos, lo que implica restar la media de los datos originales. Luego, usando las Componentes Principales (PCA), podemos identificar las direcciones de mayor varianza que permiten una representaci√≥n m√°s sencilla de los datos.

```python
# Centramos los datos
X_centered = X - np.mean(X, axis=0)
Y_centered = y - np.mean(y, axis=0)

# Recalculamos el producto interno
cov_matrix = np.dot(X_centered.T, X_centered)

# Calculamos los autovalores y autovectores
eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

# Graficamos los autovectores
vector_colors = ['r', 'b']
plt.quiver(*np.mean(X_centered, axis=0),
           *eigenvectors[:,0], scale=eigenvalues[0],
           color=vector_colors[0])
plt.quiver(*np.mean(y, axis=0),
           *eigenvectors[:,1], scale=eigenvalues[1],
           color=vector_colors[1])
plt.show()
```

En este c√≥digo se calcula y grafica la transformaci√≥n resultante, mostrando c√≥mo los autovectores definen la direcci√≥n de m√°xima varianza.

#### ¬øC√≥mo interpretar la proyecci√≥n en el espacio reducido?

La proyecci√≥n en un nuevo sistema de referencia revela mucho sobre la informaci√≥n subyacente en los datos. Si logramos reescribir nuestra nube de puntos en funci√≥n de los autovectores, podemos reducir las dimensiones sin perder demasiada informaci√≥n.

```python
# Proyectamos en el sistema de autovectores
X_transformed = np.dot(X_centered, eigenvectors)

# Visualizamos la transformaci√≥n
plt.scatter(X_transformed[:, 0], X_transformed[:, 1], marker='o', c='g')
plt.axhline(0, color='red', lw=2)
plt.axvline(0, color='red', lw=2)
plt.xlabel('PC 1')
plt.ylabel('PC 2')
plt.show()
```

Esta proyecci√≥n nos permite reducir nuestras dimensiones, centr√°ndonos en las variables que explican la mayor parte de la varianza.

#### Reflexion

Cuando se trabaja con conjuntos de datos extensos y variables interrelacionadas, la reducci√≥n de dimensiones, como la que se consigue mediante PCA, es esencial. Esto no solo mejora la eficiencia del an√°lisis, sino que tambi√©n facilita la interpretaci√≥n de los resultados. Un enfoque cuidadoso y metodol√≥gico garantiza que mantengamos la esencia de los datos mientras eliminamos la redundancia innecesaria.

Si te interesa profundizar y dominar estas t√©cnicas, te animo a seguir explorando y experimentando con diferentes conjuntos de datos. La pr√°ctica constante y el estudio te ayudar√°n a convertirte en un experto en an√°lisis de datos. ¬°Sigue adelante y nunca dejes de aprender!

## An√°lisis de Componentes Principales en Im√°genes: Reducci√≥n Dimensional

El **PCA en im√°genes** es una t√©cnica poderosa para **reducir el tama√±o y la complejidad** de una imagen sin perder (demasiada) informaci√≥n relevante. En lugar de operar sobre datos tabulares, se aplica sobre los **pixeles** de la imagen, que pueden tratarse como vectores de alta dimensi√≥n.

### üìå ¬øPor qu√© aplicar PCA a im√°genes?

* üîª **Reducci√≥n de tama√±o** (compresi√≥n).
* üîç **Eliminaci√≥n de ruido**.
* ‚ö° **Aceleraci√≥n de modelos de visi√≥n por computador**.
* üé® **Reconstrucci√≥n visual con menos informaci√≥n**.

### üñºÔ∏è ¬øC√≥mo se aplica?

Una imagen a color (RGB) es una **matriz 3D**: alto √ó ancho √ó 3 canales. Se puede aplicar PCA por canal o convertirla a escala de grises y luego aplicar PCA sobre la matriz 2D resultante.

### üß™ Ejemplo Pr√°ctico: PCA aplicado a una imagen con Python

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from skimage.color import rgb2gray
from skimage.io import imread

# 1. Cargar imagen y convertir a escala de grises
img = imread('tu_imagen.jpg')  # Reemplaza con una ruta v√°lida
gray_img = rgb2gray(img)

# 2. Visualizar la imagen original
plt.imshow(gray_img, cmap='gray')
plt.title("Imagen Original")
plt.axis("off")
plt.show()

# 3. Aplicar PCA
n_components = 50  # puedes cambiar este valor
pca = PCA(n_components=n_components)
transformed = pca.fit_transform(gray_img)
reconstructed = pca.inverse_transform(transformed)

# 4. Mostrar imagen reconstruida
plt.imshow(reconstructed, cmap='gray')
plt.title(f"Reconstruida con {n_components} componentes")
plt.axis("off")
plt.show()

# 5. Comparar varianza explicada
print(f"Varianza explicada acumulada: {np.sum(pca.explained_variance_ratio_):.4f}")
```

### üìâ Resultado:

* La imagen reconstruida con PCA tiene **menos informaci√≥n**, pero **visualmente se parece** a la original.
* Con menos de 50 componentes, puedes conservar **m√°s del 90% de la varianza** en muchos casos.

### üéØ Aplicaciones reales

* **Reconocimiento facial** (Eigenfaces).
* **Compresi√≥n** y almacenamiento eficiente.
* **Preprocesamiento para modelos de Deep Learning**.
* **Detecci√≥n de anomal√≠as visuales**.

### Resumen

#### ¬øQu√© es el an√°lisis de componentes principales?

El an√°lisis de componentes principales (PCA, por sus siglas en ingl√©s) es una t√©cnica invaluable en el mundo del procesamiento de datos para reducir la cantidad de dimensiones con las que trabajamos. Frecuentemente, enfrentamos conjuntos de datos con muchas variables, y PCA nos ayuda a conservar el 80% de la informaci√≥n m√°s relevante con menos variables. En s√≠ntesis, simplifica nuestros datos sin perder demasiada informaci√≥n esencial.

#### ¬øC√≥mo preparar datos de im√°genes para PCA?

Para ilustrar c√≥mo aplicar PCA, veamos un ejemplo con im√°genes. Utilizaremos un conjunto de datos de rostros del laboratorio Olivetti, creado entre 1992 y 1994 en los laboratorios de Cambridge.

#### Paso 1: Importar librer√≠as necesarias

Utilizaremos librer√≠as de Python como `numpy`, `matplotlib`, y `pandas` para realizar las operaciones requeridas:

```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
```

#### Paso 2: Leer y normalizar las im√°genes

Primero, cargamos las im√°genes desde un directorio espec√≠fico. Luego, normalizamos los valores de las im√°genes dividi√©ndolos por 255, el valor m√°ximo posible en una imagen en escala de grises.

```python
image = np.random.rand(112, 92)  # ejemplo de una imagen
image_normalizada = image / 255.0
```

#### Paso 3: Visualizaci√≥n sin ejes

Para graficar estas im√°genes sin mostrar los ejes cartesianos, los configuramos as√≠:

```python
fig, axs = plt.subplots(1, 2, figsize=(12, 12))
axs[0].imshow(image, cmap='gray')
axs[1].imshow(image_normalizada, cmap='gray')

for ax in axs:
    ax.set_xticks([])
    ax.set_yticks([])
plt.show()
```

##### ¬øC√≥mo gestionar m√∫ltiples im√°genes?
#### Paso 1: Leer m√∫ltiples im√°genes

Configuramos un DataFrame para conservar los datos de cada imagen a medida que las leemos. Esto requiere recorrer los archivos de im√°genes disponibles.

```python
from glob import glob

imagenes = glob('imagenes/*')
caras = pd.DataFrame()

for archivo in imagenes:
    img = plt.imread(archivo).flatten()  # aplanar la imagen
    caras = caras.append([img], ignore_index=True)
```

#### Paso 2: Visualizaci√≥n de un subconjunto de im√°genes

Podemos mostrar un conjunto de im√°genes seleccionando un n√∫mero de individuos y el n√∫mero de tomas para cada uno:

```python
fig, axs = plt.subplots(5, 10, figsize=(15, 8))

for i, ax in enumerate(axs.flatten()):
    img = caras.iloc[i].values.reshape(112, 92)  # reconstruir la forma original
    ax.imshow(img, cmap='gray')
    ax.set_xticks([])
    ax.set_yticks([])

plt.subplots_adjust(wspace=0.1, hspace=0.01)
plt.show()
```

Este setup nos permite tener una vista preliminar del conjunto de datos con el que trabajaremos en PCA.

##### Reflexiones finales

El an√°lisis de componentes principales se convertir√° en un aliado fundamental para el an√°lisis y procesamiento de datos, especialmente cuando se trabaja con im√°genes o datasets de alta dimensionalidad. Adem√°s, el uso correcto de librer√≠as como `numpy` y `pandas` optimiza la preparaci√≥n y limpieza de datos. ¬°Vamos a seguir explorando el potencial de estas herramientas y a comprender a fondo sus aplicaciones!

**Lecturas recomendadas**

[algebra aplicada](https://github.com/platzi/algebra-aplicada/tree/master/03%20-%20Algebra%20Lineal%20Aplicada%20-%20Analisis%20de%20Componentes%20Principales%20(PCA)/imagenes "algebra aplicada")

## Reducci√≥n de Dimensiones en Im√°genes con PCA

Vamos a aplicar **PCA (An√°lisis de Componentes Principales)** para reducir la dimensionalidad de un conjunto de im√°genes, lo cual es √∫til para:

* **Reducir el tama√±o de almacenamiento**.
* **Eliminar redundancia**.
* **Visualizar datos complejos**.
* **Extraer caracter√≠sticas para Machine Learning**.

### üì¶ Paso a paso en Python:

Primero cargamos las im√°genes como ya lo hiciste, luego aplicamos PCA para reducirlas y reconstruimos algunas para ver el efecto.

### ‚úÖ C√≥digo completo:

```python
import pandas as pd
import matplotlib.pyplot as plt
import imageio.v2 as imageio
from glob import iglob
from sklearn.decomposition import PCA

# === 1. Cargar las im√°genes ===
caras = []

for path in iglob('./imagenes/*/*.pgm'):
    im = imageio.imread(path)
    cara = pd.Series(im.flatten(), name=path)
    caras.append(cara)

caras = pd.concat(caras, axis=1).T  # Cada fila es una imagen

# === 2. Aplicar PCA ===
pca = PCA(n_components=50)  # Usa 50 componentes principales
caras_pca = pca.fit_transform(caras)

# === 3. Reconstrucci√≥n aproximada de las im√°genes ===
caras_reconstruidas = pca.inverse_transform(caras_pca)

# === 4. Mostrar originales y reconstruidas ===
fig, axes = plt.subplots(2, 10, figsize=(15, 4),
                         subplot_kw={'xticks': [], 'yticks': []},
                         gridspec_kw=dict(hspace=0.01, wspace=0.01))

for i in range(10):
    # Original
    axes[0, i].imshow(caras.iloc[i].values.reshape(112, 92), cmap='gray')
    # Reconstruida
    axes[1, i].imshow(caras_reconstruidas[i].reshape(112, 92), cmap='gray')

axes[0, 0].set_ylabel('Original', fontsize=12)
axes[1, 0].set_ylabel('Reconstruida', fontsize=12)
plt.suptitle('Reducci√≥n de Dimensiones con PCA', fontsize=14)
plt.tight_layout()
plt.show()
```

### üìâ Explicaci√≥n:

* **`n_components=50`**: Conservamos 50 dimensiones de las \~10,000 originales (112x92).
* **`fit_transform`**: Calcula los ejes principales (autovectores) y proyecta las im√°genes.
* **`inverse_transform`**: Reconstruye la imagen original desde las componentes principales.

### üìä Evaluaci√≥n:

Puedes ver cu√°nta **varianza** se conserva con:

```python
print("Varianza acumulada conservada:", sum(pca.explained_variance_ratio_))
```

Ejemplo de salida:

```
Varianza acumulada conservada: 0.92
```

Eso significa que est√°s reteniendo el 92% de la informaci√≥n con solo 50 dimensiones.

### Resumen

#### ¬øC√≥mo aplicar PCA para reducir la dimensionalidad de im√°genes?

La reducci√≥n de dimensionalidad es una t√©cnica crucial cuando trabajamos con conjuntos de datos grandes, y en el caso de im√°genes, el uso de PCA (An√°lisis de Componentes Principales) es especialmente √∫til. Aqu√≠ te mostrar√© c√≥mo hacerlo paso a paso utilizando Python.

#### ¬øQu√© es PCA y c√≥mo podemos usarlo?

PCA es un m√©todo estad√≠stico que permite reducir la dimensionalidad de un conjunto de datos, manteniendo la mayor parte de la variabilidad presente en √©l. Esto se logra mediante la transformaci√≥n de las variables originales en un nuevo conjunto de variables, llamadas componentes principales. He aqu√≠ c√≥mo puedes implementarlo:

```python
from sklearn.decomposition import PCA

# Instanciamos PCA para capturar el 50% de la varianza de los datos
caras_pca = PCA(n_components=0.5)

# Ajustamos y transformamos nuestro conjunto de im√°genes
componentes = caras_pca.fit_transform(imagenes)
```

#### ¬øC√≥mo visualizar los componentes?

Una parte importante del uso de PCA es la visualizaci√≥n de los componentes principales. Esto nos ayuda a entender cu√°ntas componentes son necesarias para retener la informaci√≥n deseada.

```python
import matplotlib.pyplot as plt

# Calculamos el n√∫mero de filas y columnas para el gr√°fico
filas = 3
columnas = caras_pca.n_components_ // filas

# Definimos la figura
fig, ax = plt.subplots(filas, columnas, figsize=(12, 6))

for i, axi in enumerate(ax.flat):
    # Obtenemos la imagen correspondiente a la componente i
    componente_i = componentes[:, i].reshape(altura, ancho)
    axi.imshow(componente_i, cmap='gray')
    axi.set_title(f'Componente {i}')
    axi.axis('off')

plt.show()
```

#### ¬øQu√© sucede al ajustar el porcentaje de varianza explicada?

Modificar el porcentaje de varianza explicada nos permite ajustar la cantidad de componentes mantenidos. Esto es crucial para balancear entre precisi√≥n y eficiencia computacional.

```python
# Por ejemplo, para capturar el 80% de la varianza
caras_pca_80 = PCA(n_components=0.8)
componentes_80 = caras_pca_80.fit_transform(imagenes)

# Con un 80% de varianza, obtenemos m√°s componentes:
# Requiere m√°s tiempo de procesamiento pero mejora la calidad de la representaci√≥n
```

#### ¬øC√≥mo afecta el porcentaje de informaci√≥n en los resultados?

Es interesante observar c√≥mo el porcentaje de varianza afecta la cantidad de componentes y la representaci√≥n de los datos. Al aumentar el porcentaje de informaci√≥n que queremos conservar, aumentamos el tiempo de computaci√≥n y la complejidad.

```python
caras_pca_999 = PCA(n_components=0.999)
componentes_999 = caras_pca_999.fit_transform(imagenes)

# Al graficar, notamos una representaci√≥n m√°s fiel, aunque con un costo computacional m√°s alto
```

Las principales conclusiones son que con solo seis componentes puedes capturar el 50% de la informaci√≥n, utilizando cuarenta y cuatro componentes puedes capturar hasta el 80%, y si deseas casi el 100% de precisi√≥n, necesitas la mayor√≠a de las im√°genes como componentes. Con PCA, no solo puedes identificar patrones y rasgos clave en tus datos, sino que tambi√©n mejoras la eficiencia del procesamiento, reduciendo la dimensionalidad de tus conjuntos de datos de manera efectiva.

Te animo a seguir explorando y experimentando con PCA y otras t√©cnicas de reducci√≥n de dimensionalidad, ya que estas herramientas son fundamentales para el an√°lisis de datos a gran escala y la comprensi√≥n de patrones complejos.

## Reducci√≥n de Dimensiones con Descomposici√≥n de Matrices

La **reducci√≥n de dimensiones con descomposici√≥n de matrices** es una t√©cnica fundamental en procesamiento de im√°genes y aprendizaje autom√°tico. Aqu√≠ te explico brevemente c√≥mo funciona, y luego te muestro un ejemplo pr√°ctico con c√≥digo en Python utilizando **PCA (An√°lisis de Componentes Principales)**, que es uno de los m√©todos m√°s usados y se basa en **descomposici√≥n en valores singulares (SVD)**.

### üîé ¬øQu√© es la Reducci√≥n de Dimensiones con Descomposici√≥n de Matrices?

**Objetivo:** Transformar datos de alta dimensi√≥n (por ejemplo, una imagen de 1024x1024 p√≠xeles) en una representaci√≥n de menor dimensi√≥n que conserve la mayor parte de la informaci√≥n relevante.

### üîß T√©cnicas comunes:

| M√©todo               | Tipo de descomposici√≥n                     | Utilidad                                               |
| -------------------- | ------------------------------------------ | ------------------------------------------------------ |
| **PCA**              | Descomposici√≥n en valores singulares (SVD) | An√°lisis lineal, reducci√≥n de ruido                    |
| **SVD directamente** | SVD pura: `A = UŒ£V·µÄ`                       | Compresi√≥n y reconstrucci√≥n                            |
| **NMF**              | Factorizaci√≥n no negativa                  | Interpretabilidad, im√°genes con s√≥lo valores positivos |
| **LDA**              | Descomposici√≥n basada en clases            | Reducci√≥n supervisada para clasificaci√≥n               |

### üì∑ Ejemplo: Reducci√≥n de Dimensiones de una Imagen con PCA

### Paso 1: Cargar imagen y convertirla a escala de grises

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from skimage.color import rgb2gray
from skimage.io import imread

# Cargar imagen y convertirla a escala de grises
img = imread('https://upload.wikimedia.org/wikipedia/commons/thumb/0/0a/600x600_black_and_white_mandelbrot_set.png/512px-600x600_black_and_white_mandelbrot_set.png')
gray_img = rgb2gray(img)

plt.imshow(gray_img, cmap='gray')
plt.title('Imagen Original')
plt.axis('off')
plt.show()
```

### Paso 2: Aplicar PCA para reducci√≥n de dimensiones

```python
# Aplicamos PCA fila por fila (columnas son las caracter√≠sticas)
pca = PCA(n_components=50)  # Elige cu√°ntas componentes principales conservar
img_transformed = pca.fit_transform(gray_img)
img_reconstructed = pca.inverse_transform(img_transformed)

plt.imshow(img_reconstructed, cmap='gray')
plt.title('Imagen Reconstruida con PCA (50 componentes)')
plt.axis('off')
plt.show()
```

### üìâ Comparaci√≥n visual

* **Imagen original**: contiene toda la informaci√≥n (512x512 p√≠xeles)
* **Imagen PCA (50 componentes)**: comprimida y reconstruida
* Puedes experimentar con `n_components=10`, `20`, `100`, etc.

### ‚úÖ Beneficios

* **Reducci√≥n de almacenamiento**: Ideal para compresi√≥n de im√°genes.
* **Filtrado de ruido**: PCA tiende a conservar la se√±al fuerte y reducir el ruido.
* **Velocidad**: Con datos m√°s peque√±os, los modelos aprenden m√°s r√°pido.

### Resumen

#### ¬øPor qu√© es importante la descomposici√≥n de matrices en el aprendizaje autom√°tico?

En el fascinante mundo del aprendizaje autom√°tico, la descomposici√≥n de matrices surge como una t√©cnica esencial para la reducci√≥n de dimensiones. Este m√©todo no solo optimiza los modelos al disminuir las dimensiones de un conjunto de datos, sino que tambi√©n ofrece ventajas significativas en el contexto de la computaci√≥n. ¬øQuieres saber por qu√© esta t√©cnica es vital? Vamos a explorarlo.

#### ¬øC√≥mo beneficia la reducci√≥n de dimensiones en el procesamiento de datos?

La reducci√≥n de dimensiones a trav√©s de la descomposici√≥n de matrices permite manejar grandes conjuntos de datos con eficacia. Este enfoque ayuda a:

- **Reducir tiempos computacionales**: Al trabajar con menos dimensiones, los resultados se obtienen m√°s r√°pido, lo que es crucial para modelos que manejan grandes vol√∫menes de datos.

- **Mejorar la precisi√≥n del modelo**: Al eliminar el ruido y los datos redundantes, el modelo puede centrarse en las caracter√≠sticas m√°s relevantes.

- **Facilitar la visualizaci√≥n**: Con menos dimensiones, es m√°s sencillo para los humanos interpretar y visualizar los datos.

#### ¬øQu√© t√©cnicas existen para la descomposici√≥n de matrices?

Existen varias t√©cnicas de descomposici√≥n que son ampliamente utilizadas en diversas aplicaciones de aprendizaje autom√°tico:

1. **Descomposici√≥n en Valores Singulares (SVD)**: Analiza y simplifica matrices complejas, lo cual es especialmente √∫til en la compresi√≥n de datos y en el reconocimiento de patrones.

2. **Descomposici√≥n QR**: Utilizada principalmente para resolver sistemas de ecuaciones lineales, es una herramienta clave para la regresi√≥n lineal.

3. **Descomposici√≥n LU**: Permite descomponer una matriz en matrices triangulares, facilitando el c√°lculo de determinantes y la realizaci√≥n de operaciones inversas.

#### ¬øCu√°les son los siguientes pasos en tu aprendizaje?

Ahora que entiendes la relevancia de la descomposici√≥n de matrices, es el momento de seguir avanzando. Te recomiendo profundizar en cursos de Maximum Learning o Deep Learning disponibles en plataformas educativas como Platzi. Estos cursos te ayudar√°n a aplicar estos conceptos de manera pr√°ctica y avanzada, mejorando a√∫n m√°s tus competencias.

No olvides realizar los ex√°menes para obtener la certificaci√≥n y consolidar tu conocimiento. Adem√°s, compartir tu conocimiento puede ser beneficioso; utiliza c√≥digos de referido, y si es posible, disfruta de las ventajas que te otorgan, como un mes gratis de suscripci√≥n, incentivando as√≠ a otros a unirse a esta comunidad de aprendizaje.
