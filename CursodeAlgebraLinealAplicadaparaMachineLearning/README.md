# Curso de √Ålgebra Lineal Aplicada para Machine Learning

## Descomposici√≥n de Matrices y Su Aplicaci√≥n en Machine Learning

La **descomposici√≥n de matrices** es una herramienta fundamental en **√°lgebra lineal aplicada al Machine Learning**, ya que permite simplificar c√°lculos complejos, reducir la dimensionalidad y extraer informaci√≥n estructural de los datos. A continuaci√≥n te explico los **principales tipos**, sus **aplicaciones** y ejemplos pr√°cticos.

### ‚úÖ ¬øQu√© es la Descomposici√≥n de Matrices?

Consiste en **descomponer una matriz en varios factores** o submatrices con propiedades especiales que facilitan:

* La soluci√≥n de sistemas de ecuaciones
* La reducci√≥n de dimensiones
* La compresi√≥n de datos
* La mejora del rendimiento en modelos de machine learning

### üîç Tipos Principales de Descomposici√≥n

### 1. **Descomposici√≥n LU (Lower-Upper)**

* Descompone una matriz cuadrada A en:

  $$
  A = L \cdot U
  $$

  donde `L` es triangular inferior y `U` es triangular superior.

* ‚úÖ **Aplicaciones**:

  * Resolver sistemas de ecuaciones lineales.
  * Acelerar algoritmos num√©ricos.

### 2. **Descomposici√≥n QR**

* Descompone A en:

  $$
  A = Q \cdot R
  $$

  donde `Q` es ortogonal (o unitario) y `R` es triangular superior.

* ‚úÖ **Aplicaciones**:

  * Soluciones num√©ricas estables.
  * Regresi√≥n lineal.

### 3. **Descomposici√≥n SVD (Singular Value Decomposition)**

* Factoriza A en:

  $$
  A = U \cdot \Sigma \cdot V^T
  $$

  donde `U` y `V` son ortogonales y `Œ£` contiene los valores singulares.

* ‚úÖ **Aplicaciones**:

  * **Reducci√≥n de dimensionalidad (PCA)**
  * **Recomendadores**
  * **Compresi√≥n de im√°genes**
  * **Detecci√≥n de patrones latentes**

### 4. **Descomposici√≥n Eig (de autovalores)**

* Para matrices cuadradas:

  $$
  A = V \cdot D \cdot V^{-1}
  $$

  donde `D` es diagonal (autovalores) y `V` contiene los autovectores.

* ‚úÖ **Aplicaciones**:

  * An√°lisis de componentes principales (PCA)
  * Estabilidad de sistemas
  * M√©todos espectrales

### üí° Ejemplo Pr√°ctico en Python (SVD con NumPy)

```python
import numpy as np

A = np.array([[3, 2], [2, 3]])
U, S, VT = np.linalg.svd(A)

print("Matriz U:\n", U)
print("Valores singulares Œ£:\n", S)
print("Matriz V^T:\n", VT)
```

### ü§ñ Aplicaciones en Machine Learning

| Tipo de descomposici√≥n | Aplicaci√≥n ML destacada                          |
| ---------------------- | ------------------------------------------------ |
| **SVD**                | Recomendadores, PCA                              |
| **LU / QR**            | Soluci√≥n eficiente de sistemas, regresi√≥n lineal |
| **Eig**                | PCA, clustering espectral                        |
| **NMF (No Negativa)**  | Modelado de temas (topic modeling)               |

### Resumen

#### ¬øPor qu√© es importante entender las matrices en data science?

Comprender el uso de las matrices en data science es fundamental para abordar problemas complejos y optimizar procesos. Las matrices permiten realizar transformaciones lineales, facilitando la manipulaci√≥n y el an√°lisis de datos en gran escala. En muchos casos, especialmente en √°reas como machine learning, entender las matrices es clave para mejorar la eficiencia computacional debido a la reducci√≥n de dimensiones y al manejo de datos de alta densidad.

#### ¬øQu√© conceptos previos necesitas?

Es crucial recordar ciertos conceptos que ser√°n tu base para avanzar en este curso. Entre estos:

- **Matrices e Identidad**: Comprender qu√© es una matriz y las operaciones b√°sicas que puedes realizar.
- **Inversa de una matriz cuadrada**: Saber c√≥mo calcularla y las condiciones bajo las cuales existe.

Estos fundamentos te permitir√°n ir m√°s all√° y aventurarte en el c√°lculo de autovalores y autovectores, y c√≥mo estos permiten descomponer una matriz. Adem√°s, entender√°s qu√© es el SVD y la descomposici√≥n en valores singulares.

#### ¬øC√≥mo se relaciona el √Ålgebra Lineal con Machine Learning?

La relaci√≥n del √°lgebra lineal con el machine learning es directa, ya que muchos de los algoritmos utilizados en esta √°rea requieren manipular y transformar grandes vol√∫menes de datos. Aqu√≠ algunos puntos clave:

- **Reducci√≥n de dimensionalidad:** Disminuir el n√∫mero de dimensiones puede llevar a procesos m√°s eficientes sin perder informaci√≥n significativa.
- **Optimizaci√≥n de algoritmos**: Al reducir dimensionalidades, disminuye el tiempo computacional necesario, lo cual es esencial cuando se manejan grandes conjuntos de datos.
- **Transformaciones lineales**: Las matrices permiten transformar y manipular datos eficazmente, lo que es crucial para entrenar modelos de machine learning.

Trabajar con matrices y entender su aplicaci√≥n pr√°ctica te dar√° ventaja al manejar sistemas de machine learning m√°s complejos, asegurando que tu enfoque sea tanto preciso como eficiente.

#### ¬øQu√© m√°s aprender√°s en este curso?

El prop√≥sito de este curso es ir m√°s all√° de los fundamentos y explorar temas avanzados de √°lgebra lineal aplicados a data science. Esto incluye:

- **C√°lculo de Pseudo-inversas**: O inversas generalizadas, √∫tiles en sistemas que no tienen una soluci√≥n √∫nica o bien definida.
- **Algoritmo PCA (An√°lisis de Componentes Principales)**: Este es un m√©todo muy utilizado para la reducci√≥n de dimensionalidad y an√°lisis exploratorio de datos.
- **Aplicaciones pr√°cticas**: Implementaci√≥n de estos conceptos en problemas reales, que te permitir√° ver en acci√≥n las t√©cnicas aprendidas.

Este curso est√° dise√±ado no solo para enriquecer tu conocimiento te√≥rico, sino para empoderarte a aplicar estas herramientas de manera efectiva en tus proyectos de ciencia de datos. ¬°Sigue adelante y descubre el potencial del √°lgebra lineal en el mundo del machine learning y data science!

## Transformaciones Lineales con Matrices en Python: Visualizaci√≥n y An√°lisis

Vamos a abordar el tema **Transformaciones Lineales con Matrices en Python** con un enfoque pr√°ctico: entender, visualizar y analizar c√≥mo una matriz puede transformar vectores en el plano.


### üß† ¬øQu√© es una transformaci√≥n lineal?

Una **transformaci√≥n lineal** es una funci√≥n que lleva vectores de un espacio a otro respetando suma y multiplicaci√≥n escalar. Se representa mediante **multiplicaci√≥n de una matriz por un vector**.

Si $A$ es una matriz y $\vec{v}$ un vector, entonces:

$$
\text{Transformaci√≥n: } T(\vec{v}) = A \cdot \vec{v}
$$

### üõ†Ô∏è Herramientas en Python

Usaremos:

```python
import numpy as np
import matplotlib.pyplot as plt
```

### üß™ Ejemplo 1: Transformaci√≥n en 2D

```python
# Vectores originales (cuadrado unitario)
original = np.array([
    [0, 0], [1, 0], [1, 1], [0, 1], [0, 0]
]).T

# Matriz de transformaci√≥n
A = np.array([
    [2, 1],
    [1, 3]
])

# Aplicar la transformaci√≥n
transformado = A @ original

# Visualizar
plt.figure(figsize=(6,6))
plt.plot(*original, label='Original', color='blue')
plt.plot(*transformado, label='Transformado', color='red')
plt.axhline(0, color='gray', lw=0.5)
plt.axvline(0, color='gray', lw=0.5)
plt.grid(True)
plt.axis('equal')
plt.legend()
plt.title('Transformaci√≥n Lineal con Matriz A')
plt.show()
```

### üîç An√°lisis

* La matriz **A** cambia la forma del cuadrado unitario.
* Esta transformaci√≥n puede **escalar, rotar, reflejar o sesgar** los vectores originales dependiendo de los valores de la matriz.

### üß≠ Ejemplo 2: Escalamiento y rotaci√≥n

```python
from math import cos, sin, pi

# Escalamiento
S = np.array([
    [2, 0],
    [0, 0.5]
])

# Rotaci√≥n 45 grados
theta = pi / 4
R = np.array([
    [cos(theta), -sin(theta)],
    [sin(theta), cos(theta)]
])

# Combinar: escalar y luego rotar
T = R @ S
resultado = T @ original

plt.figure(figsize=(6,6))
plt.plot(*original, label='Original', color='blue')
plt.plot(*resultado, label='Escala + Rotaci√≥n', color='green')
plt.grid(True)
plt.axis('equal')
plt.legend()
plt.title('Transformaci√≥n Lineal: Escala y Rotaci√≥n')
plt.show()
```

### üìö Aplicaciones

* **Compresi√≥n de im√°genes** (PCA).
* **Animaci√≥n y gr√°ficos por computadora**.
* **Simulaci√≥n f√≠sica** y geometr√≠a computacional.
* **Machine learning** (transformaciones en espacios latentes).

### Resumen

#### ¬øC√≥mo entendemos las matrices como transformaciones lineales?

Las matrices pueden entenderse como transformaciones lineales que, al aplicarse a un espacio o un vector, generan una transformaci√≥n. Cuando aplicamos una matriz, podemos afectar a un vector modificando su tama√±o o incluso rot√°ndolo. En el mundo de la programaci√≥n, podemos llevar esto a la pr√°ctica utilizando Python y librer√≠as como NumPy y Matplotlib para representar gr√°ficamente estos cambios.

#### ¬øC√≥mo configuramos nuestro entorno en Python para visualizaciones?

Para empezar, necesitamos importar las librer√≠as necesarias. Aqu√≠ va un peque√±o fragmento de c√≥digo en Python que nos permitir√° ver los gr√°ficos debajo de cada celda de nuestro notebook:

```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
```

Posteriormente, definimos nuestras matrices y vectores usando `numpy`.

#### ¬øC√≥mo definimos y aplicamos una transformaci√≥n con matrices?

Supongamos que tenemos la siguiente matriz:

`A = np.array([[-1, 3], [2, -2]])`

Queremos investigar qu√© transformaci√≥n genera esta matriz al aplicarla al siguiente vector:

`v = np.array([2, 1])`

La transformaci√≥n de un vector `v` usando una matriz `A` se realiza a trav√©s del producto interno de la matriz y el vector. Pero antes de eso, definamos una funci√≥n para graficar los vectores.

#### ¬øC√≥mo graficamos vectores en Python?

Es √∫til tener una funci√≥n vers√°til para graficar m√∫ltiples vectores. Aqu√≠ hay una base de c√≥mo podemos definir y utilizar esta funci√≥n:

```python
def graficar_vectores(vectores, colores, alpha=1):
    plt.figure()
    plt.axvline(x=0, color='grey', lw=1)
    plt.axhline(y=0, color='grey', lw=1)
    for i in range(len(vectores)):
        x = np.concatenate([[0, 0], vectores[i]])
        plt.quiver(*x[::2], *x[1::2], angles='xy', scale_units='xy', scale=1, color=colores[i], alpha=alpha)
    plt.xlim(-3, 3)
    plt.ylim(-3, 3)
    plt.show()

v_flaten = v.flatten()
graficar_vectores([v_flaten], ['blue'])
```

Esta funci√≥n ayuda a visualizar c√≥mo han cambiado los vectores al ser transformados por la matriz. Los ejes cruzan en `x=0` y el color de las l√≠neas es `gris`.

#### ¬øC√≥mo se transforman los vectores utilizando matrices?

Cuando aplicamos la matriz `A` al vector `v`, podemos ver el cambio que se produce. Primero, realizamos el c√°lculo del producto interno:

```python
v_transformado = A.dot(v)
graficar_vectores([v.flatten(), v_transformado.flatten()], ['blue', 'orange'])
```

Aqu√≠, graficamos el vector original `v` junto con el vector `v_transformado` para observar la transformaci√≥n visualmente, comparando sus posiciones y direcciones.

#### ¬øPor qu√© es importante entender estas transformaciones en Aprendizaje Autom√°tico?

Las transformaciones de matrices son fundamentales en el aprendizaje autom√°tico, especialmente cuando trabajamos con im√°genes o datos que tienen representaciones matriciales. Las matrices permiten transformar estos datos de manera que pueden ser procesados m√°s eficientemente por algoritmos de Deep Learning o Machine Learning.

Entender las representaciones de vectores y matrices y c√≥mo podemos alargar, rotar o modificar su escala es clave para manipular datos estructurados como im√°genes, en las cuales cada pixel puede ser parte de una matriz mayor. Cuando llevamos matrices a vectores, este proceso se llama "flatten" y es crucial para el tratamiento de datos en modelos computacionales.

#### ¬øQu√© papel juegan los determinantes en estas transformaciones?

El determinante de una matriz nos ofrece informaci√≥n valiosa sobre la transformaci√≥n. Un determinante negativo (como el `-4` en nuestro ejemplo) puede indicarnos que la transformaci√≥n involucra una inversi√≥n o un giro. Por otro lado, si las normas de los vectores antes y despu√©s de la transformaci√≥n se mantienen iguales, puede se√±alar que hay vectores en el espacio que no cambian su longitud.

```python
determinante = np.linalg.det(A)
print(f"Determinante de la matriz A: {determinante}")
```

En esta exploraci√≥n, continuamos profundizando en c√≥mo las operaciones de matriz y producto escalar nos ayudan a dar forma a datos y patrones, sentando las bases para descubrimientos m√°s intrincados en el vasto campo del an√°lisis de datos y Machine Learning.