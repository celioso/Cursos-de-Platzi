# Curso de Regresi√≥n Lineal con Python y scikit-learn

## Tu primera regresi√≥n lineal con scikit-learn

Vamos a hacer **tu primera regresi√≥n lineal** usando `scikit-learn`, una de las bibliotecas m√°s populares de Python para machine learning.

### üìà ¬øQu√© es una regresi√≥n lineal?

Es un modelo matem√°tico que **relaciona una variable independiente (X)** con **una variable dependiente (y)** a trav√©s de una **recta**:

$$
y = mX + b
$$

Donde:

* `m` es la **pendiente**
* `b` es la **intersecci√≥n** o t√©rmino independiente

### ‚úÖ Ejemplo paso a paso con c√≥digo

### 1. Importar bibliotecas necesarias

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
```

### 2. Crear datos de ejemplo

```python
# Datos artificiales
X = np.array([[1], [2], [3], [4], [5]])  # variable independiente
y = np.array([2, 4, 5, 4, 5])           # variable dependiente
```

### 3. Crear y entrenar el modelo

```python
modelo = LinearRegression()
modelo.fit(X, y)
```

### 4. Ver los coeficientes del modelo

```python
print("Pendiente (coef_):", modelo.coef_)
print("Intersecci√≥n (intercept_):", modelo.intercept_)
```

### 5. Hacer predicciones

```python
X_nuevos = np.array([[6], [7]])
predicciones = modelo.predict(X_nuevos)
print("Predicciones:", predicciones)
```

### 6. Visualizar la regresi√≥n

```python
plt.scatter(X, y, color='blue', label='Datos reales')
plt.plot(X, modelo.predict(X), color='red', label='L√≠nea de regresi√≥n')
plt.xlabel("X")
plt.ylabel("y")
plt.legend()
plt.title("Regresi√≥n Lineal con scikit-learn")
plt.grid(True)
plt.show()
```

### üéì ¬øQu√© aprendiste?

Con esto lograste:

* Crear un modelo de regresi√≥n lineal
* Ajustarlo con datos reales
* Predecir nuevos valores
* Visualizar la l√≠nea de mejor ajuste

**Archivos de la clase**

[2-slides-1.pdf](https://static.platzi.com/media/public/uploads/2-slides-1_86e64012-0cea-4015-8896-adb2f9d8c966.pdf)
[primera-regresion-lineal.ipynb](https://static.platzi.com/media/public/uploads/primera_regresion_lineal_755953b4-3c16-42de-a9e3-78fe9742244f.ipynb)
[primera-regresion-lineal-template.ipynb](https://static.platzi.com/media/public/uploads/primera_regresion_lineal_template_42c71d99-6fdb-4af7-98f2-e3f8e2e76b7a.ipynb)
[housing.data](https://static.platzi.com/media/public/uploads/housing_edf82b23-51e2-4f57-887d-6be67da34d27.data)

## An√°lisis de datos para tu primera regresi√≥n lineal

vamos a realizar un **an√°lisis de datos** para nuestra primera **regresi√≥n lineal** utilizando el famoso dataset de **Housing de Boston** (aunque oficialmente retirado de `sklearn` por temas √©ticos, a√∫n puede usarse con cuidado desde UCI).

Este dataset contiene 506 filas y 14 columnas. La variable objetivo (`target`) es el precio medio de las viviendas en miles de d√≥lares.

### üîπ Paso 1: Cargar los datos

```python
import pandas as pd

# Cargar el dataset desde UCI
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'
columnas = [
    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',
    'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'
]
data = pd.read_csv(url, header=None, sep=r'\s+', names=columnas)
```

### üîπ Paso 2: Inspeccionar el dataset

```python
# Ver primeras filas
print(data.head())

# Resumen estad√≠stico
print(data.describe())

# Ver si hay valores nulos
print(data.isnull().sum())
```

### üîπ Paso 3: Visualizaci√≥n de correlaciones

Podemos usar `seaborn` para ver c√≥mo se relacionan las variables con la variable objetivo `MEDV`.

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Mapa de calor de correlaciones
plt.figure(figsize=(12, 10))
sns.heatmap(data.corr(), annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Matriz de Correlaci√≥n")
plt.show()
```

### üîπ Paso 4: Selecci√≥n de una variable para regresi√≥n simple

Vamos a elegir la variable m√°s correlacionada con `MEDV`. Por lo general, `LSTAT` (porcentaje de poblaci√≥n con bajos ingresos) tiene una fuerte correlaci√≥n negativa con el precio.

```python
sns.scatterplot(x='LSTAT', y='MEDV', data=data)
plt.title("Relaci√≥n entre LSTAT y MEDV")
plt.xlabel("LSTAT (% poblaci√≥n de bajos ingresos)")
plt.ylabel("Precio medio (MEDV)")
plt.show()
```

### ¬øQu√© sigue?

Con esta base ya podemos:

1. Dividir el dataset en entrenamiento y prueba.
2. Crear un modelo de regresi√≥n lineal con `scikit-learn`.
3. Entrenar el modelo.
4. Hacer predicciones y evaluar el desempe√±o.

## Entrenando un modelo de regresi√≥n lineal con scikit-learn

Entrenar un modelo de regresi√≥n lineal con **Scikit-learn** es sencillo. A continuaci√≥n te muestro un ejemplo completo, paso a paso, con explicaciones:

### ‚úÖ 1. Importar las librer√≠as necesarias

```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
```

### ‚úÖ 2. Crear o cargar los datos

Aqu√≠ vamos a crear datos sint√©ticos como ejemplo:

```python
# Datos de ejemplo (relaci√≥n lineal con algo de ruido)
np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)
```

### ‚úÖ 3. Dividir en entrenamiento y prueba

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### ‚úÖ 4. Entrenar el modelo

```python
model = LinearRegression()
model.fit(X_train, y_train)
```

### ‚úÖ 5. Ver los coeficientes del modelo

```python
print(f"Intercepto: {model.intercept_}")
print(f"Coeficientes: {model.coef_}")
```

### ‚úÖ 6. Realizar predicciones

```python
y_pred = model.predict(X_test)
```

### ‚úÖ 7. Evaluar el modelo

```python
print(f"Error cuadr√°tico medio (MSE): {mean_squared_error(y_test, y_pred):.2f}")
print(f"Coeficiente de determinaci√≥n (R¬≤): {r2_score(y_test, y_pred):.2f}")
```

### ‚úÖ 8. (Opcional) Visualizar resultados

```python
import matplotlib.pyplot as plt

plt.scatter(X_test, y_test, color='blue', label='Datos reales')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='Regresi√≥n lineal')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.title('Modelo de Regresi√≥n Lineal')
plt.show()
```

**Lecturas recomendadas**

[scikit-learn: machine learning in Python ‚Äî scikit-learn 1.0.2 documentation](https://scikit-learn.org/stable/)

[sklearn.linear_model.LinearRegression ‚Äî scikit-learn 1.0.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)

[sklearn.preprocessing.StandardScaler ‚Äî scikit-learn 1.0.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)

[Primera_regresi√≥n_lineal.ipynb - Google Drive](https://drive.google.com/file/d/1wh1T5AE1AgmgDcTaiziaeKYSuovVFQPk/view?usp=sharing)

[Primera_regresi√≥n_lineal_Template.ipynb - Google Drive](https://drive.google.com/file/d/1NkcOQg-BTEAD0ttGDcZcAZwYpD0jXPJd/view?usp=sharing)

## ¬øQu√© es la regresi√≥n lineal?

La **regresi√≥n lineal** es un m√©todo estad√≠stico y de aprendizaje autom√°tico que se utiliza para **modelar la relaci√≥n entre una variable dependiente (o de salida) y una o m√°s variables independientes (o de entrada)**.

### üîπ ¬øQu√© hace?

Intenta **ajustar una l√≠nea recta** (en el caso simple) que **mejor explique** c√≥mo cambia la variable dependiente a medida que cambian las independientes.

### üî∏ Ejemplo simple (una sola variable):

Sup√≥n que tienes datos de estudio:

| Horas de estudio | Nota en el examen |
| ---------------- | ----------------- |
| 1                | 55                |
| 2                | 65                |
| 3                | 70                |
| 4                | 75                |

La regresi√≥n lineal busca encontrar una **l√≠nea** del tipo:

$$
y = b_0 + b_1 x
$$

Donde:

* $y$ es la nota (variable dependiente),
* $x$ son las horas de estudio (variable independiente),
* $b_0$ es el **intercepto** (valor cuando $x = 0$),
* $b_1$ es el **coeficiente** (pendiente de la l√≠nea, cu√°nto cambia $y$ por cada unidad de $x$).

### üî∏ ¬øPara qu√© sirve?

* **Predicci√≥n**: Estimar valores futuros.
* **Interpretaci√≥n**: Entender qu√© variables afectan a otra.
* **Reducci√≥n**: En modelos complejos, ayuda a simplificar relaciones.

### üîπ Tipos de regresi√≥n lineal:

1. **Simple**: Una variable independiente.
2. **M√∫ltiple**: Varias variables independientes.

### üî∏ Ejemplo gr√°fico (regresi√≥n simple):

Imagina que trazas una l√≠nea sobre un conjunto de puntos dispersos. Esa l√≠nea representa la **mejor estimaci√≥n promedio** del comportamiento de esos datos.

## Cu√°ndo utilizar un modelo de regresi√≥n lineal

Puedes utilizar un **modelo de regresi√≥n lineal** cuando se cumplen las siguientes condiciones o se busca alguno de estos objetivos:

### ‚úÖ **CU√ÅNDO USARLO:**

#### 1. **Relaci√≥n lineal entre variables**

* Cuando crees que hay una **relaci√≥n lineal** (aproximadamente recta) entre la variable dependiente y una o m√°s independientes.
* Ejemplo: A m√°s horas de estudio ‚Üí mayor nota.

#### 2. **Variables num√©ricas**

* Es ideal cuando las **variables de entrada (independientes)** y la **variable objetivo (dependiente)** son **num√©ricas continuas**.

#### 3. **Pocos datos y modelo interpretable**

* Cuando necesitas un modelo **sencillo, r√°pido y f√°cil de interpretar**.
* Puedes ver claramente qu√© variable tiene m√°s influencia en el resultado.

#### 4. **El objetivo es predecir o explicar**

* Puedes usarlo tanto para:

  * **Predecir** valores (como ingresos, temperatura, precios).
  * **Explicar** c√≥mo influye cada variable sobre otra.

#### 5. **No hay demasiada colinealidad**

* Las variables independientes **no deben estar altamente correlacionadas** entre s√≠ (porque confunden al modelo).

#### 6. **Errores con distribuci√≥n normal (idealmente)**

* Aunque no es obligatorio para predecir, si vas a hacer inferencia estad√≠stica (como tests de hip√≥tesis), los **errores (residuos)** deben seguir una **distribuci√≥n normal**.

### ‚ùå **CU√ÅNDO NO USARLO:**

* Cuando la relaci√≥n entre variables **no es lineal**.
* Si tienes **muchas variables categ√≥ricas** y no las has convertido correctamente (one-hot encoding, etc.).
* Cuando hay **outliers extremos** que afectan mucho la pendiente.
* Si hay **relaciones complejas o no lineales** entre las variables ‚Üí mejor usar √°rboles, redes neuronales, etc.

### üìå Ejemplos de uso real:

| Caso                  | Variable dependiente | Variables independientes                  |
| --------------------- | -------------------- | ----------------------------------------- |
| Predicci√≥n de precios | Precio de una casa   | Tama√±o, ubicaci√≥n, n√∫mero de habitaciones |
| Medicina              | Nivel de colesterol  | Edad, peso, dieta                         |
| Negocios              | Ventas mensuales     | Gasto en publicidad, precio del producto  |

## Funci√≥n de p√©rdida y optimizaci√≥n: m√≠nimos cuadrados

En **regresi√≥n lineal**, la **funci√≥n de p√©rdida** m√°s com√∫n es la de **m√≠nimos cuadrados**. Aqu√≠ te explico qu√© es y c√≥mo se usa para la **optimizaci√≥n del modelo**:

### üéØ ¬øQu√© es la funci√≥n de p√©rdida de m√≠nimos cuadrados?

Es una funci√≥n que **mide el error** entre los valores predichos por el modelo y los valores reales. La idea es **minimizar ese error** durante el entrenamiento.

### üìê Definici√≥n matem√°tica

Dada una muestra de datos con `n` observaciones:

$$
\text{P√©rdida} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Donde:

* $y_i$ = valor real
* $\hat{y}_i$ = valor predicho por el modelo
* La diferencia $y_i - \hat{y}_i$ se llama **residuo**
* Se eleva al cuadrado para:

  * Penalizar m√°s los errores grandes
  * Evitar que errores positivos y negativos se cancelen

Esta p√©rdida tambi√©n se conoce como **Error Cuadr√°tico Total (SSE)** o **Suma de los errores al cuadrado**.

### üõ† ¬øC√≥mo se optimiza?

El modelo de regresi√≥n lineal busca los **coeficientes (pendientes y t√©rmino independiente)** que **minimizan esta funci√≥n de p√©rdida**.

Esto se puede hacer con:

* **Soluci√≥n anal√≠tica (ecuaci√≥n normal)**:
  Para modelos peque√±os o simples.
* **Descenso del gradiente**:
  M√©todo iterativo que ajusta los coeficientes paso a paso en la direcci√≥n que reduce el error.

### üìâ ¬øPor qu√© m√≠nimos cuadrados?

Porque es:

* R√°pido y computacionalmente eficiente.
* F√°cil de interpretar.
* Funciona bien si los **errores siguen una distribuci√≥n normal**.

### üìå En Python con `scikit-learn`:

Cuando usas:

```python
from sklearn.linear_model import LinearRegression
modelo = LinearRegression()
modelo.fit(X, y)
```

Internamente se est√° minimizando la funci√≥n de **m√≠nimos cuadrados** para encontrar los mejores coeficientes.

**Lecturas recomendadas**

[Regresi√≥n Lineal | Aprende Machine Learning](https://www.aprendemachinelearning.com/tag/regresion-lineal/)

[¬øQu√© es el descenso del gradiente? - Platzi](https://platzi.com/clases/2155-calculo-data-science/35480-que-es-el-descenso-del-gradiente/)

## Evaluando el modelo: R^2 y MSE

Al evaluar un modelo de **regresi√≥n lineal**, es fundamental medir qu√© tan bien predice los valores. Dos m√©tricas ampliamente utilizadas son:

### üìä 1. Coeficiente de Determinaci√≥n: **R¬≤ (R-squared)**

### ¬øQu√© es?

* R¬≤ mide la **proporci√≥n de la varianza** en la variable dependiente $y$ que es explicada por las variables independientes $X$.
* Es una m√©trica de **bondad de ajuste**.

### F√≥rmula:

$$
R^2 = 1 - \frac{SSE}{SST}
$$

Donde:

* $SSE = \sum (y_i - \hat{y}_i)^2$: **Suma de errores al cuadrado (residuos)**
* $SST = \sum (y_i - \bar{y})^2$: **Varianza total del modelo**

### Interpretaci√≥n:

* $R^2 = 1$: predicci√≥n perfecta.
* $R^2 = 0$: el modelo no explica nada mejor que la media.
* Puede ser **negativo** si el modelo es peor que predecir con la media.

### üìâ 2. Error Cuadr√°tico Medio: **MSE (Mean Squared Error)**

### ¬øQu√© es?

* Mide el **promedio de los errores al cuadrado** entre los valores reales y los predichos.

### F√≥rmula:

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

### Interpretaci√≥n:

* Cuanto menor sea el MSE, **mejor el rendimiento** del modelo.
* Tiene las **mismas unidades** al cuadrado que la variable objetivo.
* Penaliza fuertemente los errores grandes.

### üß™ Ejemplo pr√°ctico con scikit-learn:

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Modelo
modelo = LinearRegression()
modelo.fit(X_train, y_train)

# Predicciones
y_pred = modelo.predict(X_test)

# M√©tricas
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("MSE:", mse)
print("R¬≤:", r2)
```

**Lecturas recomendadas**

[3.3. Metrics and scoring: quantifying the quality of predictions ‚Äî scikit-learn 1.0.2 documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)

[Difference between Adjusted R-squared and R-squared](https://www.listendata.com/2014/08/adjusted-r-squared.html)

[Interpreting Residual Plots to Improve Your Regression](https://www.qualtrics.com/support/stats-iq/analyses/regression-guides/interpreting-residual-plots-improve-regression/)

## Regresi√≥n lineal multivariable

La **regresi√≥n lineal multivariable** (o **regresi√≥n lineal m√∫ltiple**) es una extensi√≥n de la regresi√≥n lineal simple. En lugar de tener una sola variable independiente (input), se tienen **dos o m√°s variables independientes** para predecir una variable dependiente.

### üßÆ Forma general del modelo

$$
y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_nx_n + \varepsilon
$$

* $y$: variable objetivo (dependiente)
* $x_1, x_2, ..., x_n$: variables predictoras (independientes)
* $\beta_0$: intercepto (bias)
* $\beta_1, ..., \beta_n$: coeficientes de cada variable
* $\varepsilon$: error o ruido aleatorio

### ‚úÖ ¬øCu√°ndo usar regresi√≥n lineal multivariable?

* Cuando tienes **m√°s de una caracter√≠stica** (feature) que afecta la variable que deseas predecir.
* Ejemplo: predecir el precio de una casa usando el n√∫mero de habitaciones, superficie, ubicaci√≥n, etc.

### üõ†Ô∏è ¬øC√≥mo se entrena con `scikit-learn`?

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import pandas as pd

# Sup√≥n que df tiene columnas: 'habitaciones', 'metros_cuadrados', 'precio'
X = df[['habitaciones', 'metros_cuadrados']]  # variables independientes
y = df['precio']  # variable dependiente

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

modelo = LinearRegression()
modelo.fit(X_train, y_train)

print(modelo.coef_)      # Coeficientes de las variables
print(modelo.intercept_) # Intercepto
```

### üìà Evaluaci√≥n

Se puede evaluar usando:

* $R^2$: coeficiente de determinaci√≥n
* MSE: error cuadr√°tico medio

```python
from sklearn.metrics import r2_score, mean_squared_error

y_pred = modelo.predict(X_test)
print("R2:", r2_score(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))
```

**Archivos de la clase**

[ejercicio-multivariable.ipynb](https://static.platzi.com/media/public/uploads/ejercicio_multivariable_32b1d1b6-256d-48ce-9bcc-10bb8e108079.ipynb)

**Lecturas recomendadas**

[Ejercicio_Multivariable.ipynb - Google Drive](https://drive.google.com/file/d/1ONeY0MvSBuSml6zp_g1un_febp1g4NgL/view?usp=sharing)

## Regresi√≥n lineal para predecir los gastos m√©dicos de pacientes

La **regresi√≥n lineal** es una t√©cnica muy √∫til para predecir los **gastos m√©dicos** de pacientes si cuentas con variables num√©ricas relevantes como:

* Edad (`age`)
* IMC (`bmi`)
* N√∫mero de hijos (`children`)
* Sexo (`sex`)
* Fumador (`smoker`)
* Regi√≥n (`region`)

Estas variables se pueden usar como caracter√≠sticas (`X`) para predecir el gasto m√©dico (`charges`).

### üß† ¬øPor qu√© usar regresi√≥n lineal?

Porque es una forma de modelar c√≥mo distintas caracter√≠sticas influyen en el resultado (en este caso, los gastos m√©dicos). Por ejemplo:

* Fumar puede aumentar el gasto.
* Mayor edad tambi√©n suele estar asociada a mayores gastos.
* Un IMC alto podr√≠a correlacionarse con m√°s problemas de salud.

### ‚úÖ Ejemplo b√°sico con Python y scikit-learn

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Carga de datos (puedes usar un dataset como el de 'insurance.csv')
df = pd.read_csv('insurance.csv')

# Variables num√©ricas y categ√≥ricas
numeric = ['age', 'bmi', 'children']
categorical = ['sex', 'smoker', 'region']

# Separar variables predictoras y objetivo
X = df[numeric + categorical]
y = df['charges']

# Preprocesamiento: estandarizar num√©ricas y one-hot encoding a categ√≥ricas
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numeric),
    ('cat', OneHotEncoder(drop='first'), categorical)
])

# Pipeline con regresi√≥n lineal
model = Pipeline([
    ('preprocess', preprocessor),
    ('regressor', LinearRegression())
])

# Divisi√≥n de los datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Entrenamiento
model.fit(X_train, y_train)

# Evaluaci√≥n
from sklearn.metrics import r2_score, mean_squared_error

y_pred = model.predict(X_test)
print('R¬≤:', r2_score(y_test, y_pred))
print('MSE:', mean_squared_error(y_test, y_pred))
```

### üìä Interpretaci√≥n

* `R¬≤` indica qu√© tan bien el modelo explica los datos (m√°s cerca a 1 = mejor).
* `MSE` indica el error medio al predecir los gastos (menor = mejor).

**Lecturas recomendadas**

[Medical Cost Personal Datasets | Kaggle](https://www.kaggle.com/mirichoi0218/insurance)

## Exploraci√≥n y preparaci√≥n de datos

Para aplicar **regresi√≥n lineal a gastos m√©dicos**, primero debes hacer una **exploraci√≥n y preparaci√≥n de datos** adecuada. Aqu√≠ te muestro paso a paso c√≥mo hacerlo con el famoso dataset `insurance.csv`:

### üóÇÔ∏è 1. **Cargar los datos**

```python
import pandas as pd

df = pd.read_csv('insurance.csv')
print(df.head())
```

### üìã 2. **Exploraci√≥n inicial (EDA: Exploratory Data Analysis)**

```python
# Tama√±o del dataset
print(df.shape)

# Tipos de datos
print(df.dtypes)

# Estad√≠sticas b√°sicas
print(df.describe())

# Ver si hay valores nulos
print(df.isnull().sum())
```

### üìä 3. **Visualizaciones √∫tiles**

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Distribuci√≥n de la variable objetivo
sns.histplot(df['charges'], kde=True)
plt.title('Distribuci√≥n de gastos m√©dicos')
plt.show()

# Relaci√≥n entre edad e IMC con gastos
sns.scatterplot(x='age', y='charges', data=df)
plt.title('Edad vs Gastos m√©dicos')
plt.show()

sns.scatterplot(x='bmi', y='charges', data=df)
plt.title('IMC vs Gastos m√©dicos')
plt.show()

# Boxplots para variables categ√≥ricas
sns.boxplot(x='smoker', y='charges', data=df)
plt.title('Gastos seg√∫n h√°bito de fumar')
plt.show()

sns.boxplot(x='region', y='charges', data=df)
plt.title('Gastos por regi√≥n')
plt.show()
```

### üîß 4. **Codificaci√≥n de variables categ√≥ricas**

```python
# Codificaci√≥n one-hot para 'sex', 'smoker', 'region'
df_encoded = pd.get_dummies(df, drop_first=True)
print(df_encoded.head())
```

### ‚öñÔ∏è 5. **Escalado de variables num√©ricas (opcional pero recomendado)**

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
num_cols = ['age', 'bmi', 'children']
df_encoded[num_cols] = scaler.fit_transform(df_encoded[num_cols])
```

### ‚úÖ 6. **Separaci√≥n de variables predictoras y objetivo**

```python
X = df_encoded.drop('charges', axis=1)
y = df_encoded['charges']
```

### üß™ 7. **Divisi√≥n en entrenamiento y prueba**

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

Este preprocesamiento te deja listo para entrenar un modelo de **regresi√≥n lineal** u otro m√°s avanzado.

## An√°lisis de correlaci√≥n de los datos

El **an√°lisis de correlaci√≥n** es una t√©cnica fundamental para comprender las relaciones entre variables num√©ricas en un conjunto de datos. Es especialmente √∫til antes de aplicar un modelo de regresi√≥n lineal, como el que est√°s construyendo para predecir los gastos m√©dicos.

Aqu√≠ tienes una gu√≠a paso a paso usando `pandas`, `seaborn` y `matplotlib` para un an√°lisis de correlaci√≥n claro y visual:

### ‚úÖ Paso 1: Cargar los datos

```python
import pandas as pd

# Por ejemplo, si usas el dataset de gastos m√©dicos
df = pd.read_csv('insurance.csv')
```

### ‚úÖ Paso 2: Convertir variables categ√≥ricas (si es necesario)

```python
# Convertir columnas categ√≥ricas con one-hot encoding
df_encoded = pd.get_dummies(df, drop_first=True)
```

### ‚úÖ Paso 3: Calcular la matriz de correlaci√≥n

```python
correlation_matrix = df_encoded.corr()
print(correlation_matrix)
```

### ‚úÖ Paso 4: Visualizar la matriz de correlaci√≥n con un heatmap

```python
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Matriz de Correlaci√≥n")
plt.show()
```

### ‚úÖ ¬øQu√© observar?

* Busca **correlaciones altas (cercanas a +1 o -1)** entre la variable objetivo (`charges`) y las otras.
* Por ejemplo:

  * Si `bmi`, `smoker_yes`, o `age` tienen una correlaci√≥n fuerte con `charges`, son buenos candidatos como predictores.
* Ten cuidado con **multicolinealidad** (cuando dos predictores est√°n fuertemente correlacionados entre s√≠).

## Entrenamiento del modelo

¬°Perfecto! Una vez explorados y preparados los datos, el siguiente paso es **entrenar el modelo de regresi√≥n lineal**. A continuaci√≥n te explico c√≥mo hacerlo paso a paso con `scikit-learn`:

### ‚úÖ Paso 1: Importar librer√≠as necesarias

```python
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
```

### ‚úÖ Paso 2: Cargar y preparar los datos

```python
# Cargar el dataset
df = pd.read_csv('insurance.csv')

# Codificar variables categ√≥ricas (por ejemplo: sexo, fumador, regi√≥n)
df_encoded = pd.get_dummies(df, drop_first=True)

# Separar variables independientes (X) y variable objetivo (y)
X = df_encoded.drop('charges', axis=1)
y = df_encoded['charges']
```

### ‚úÖ Paso 3: Dividir el conjunto de datos

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### ‚úÖ Paso 4: Crear y entrenar el modelo

```python
model = LinearRegression()
model.fit(X_train, y_train)
```

### ‚úÖ Paso 5: Ver los coeficientes del modelo

```python
# Mostrar los coeficientes junto con los nombres de las variables
coeff_df = pd.DataFrame(model.coef_, X.columns, columns=['Coeficiente'])
print(coeff_df)
```

### ‚úÖ Paso 6: Realizar predicciones

```python
y_pred = model.predict(X_test)
```

### ‚úÖ Paso 7: Evaluar el modelo

Puedes usar R¬≤ y el Error Cuadr√°tico Medio (MSE):

```python
from sklearn.metrics import r2_score, mean_squared_error

print('R¬≤:', r2_score(y_test, y_pred))
print('MSE:', mean_squared_error(y_test, y_pred))
```

**Archivos de la clase**

[proyecto-final.ipynb](https://static.platzi.com/media/public/uploads/proyecto_final_4a65bcf4-a423-4ec8-b844-614430462fb4.ipynb)

[proyecto-final-template.ipynb](https://static.platzi.com/media/public/uploads/proyecto_final_template_4b44140a-4417-42eb-b260-378da1179ab3.ipynb)

**Lecturas recomendadas**

[sklearn.linear_model.LinearRegression ‚Äî scikit-learn 1.0.2 documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)

[Proyecto_Final.ipynb - Google Drive](https://drive.google.com/file/d/1UDV8y9eyZuKEDSIRKY7w93Bdo7EBqz5Z/view?usp=sharing)

[Proyecto_Final_Template.ipynb - Google Drive](https://drive.google.com/file/d/1PrSPTAVz6lXKAqNcTlUvWc_de8TRILOx/view?usp=sharing)

## Evaluando el modelo

¬°Perfecto! Una vez entrenado el modelo de regresi√≥n lineal, es fundamental **evaluar su rendimiento** para saber qu√© tan bien est√° prediciendo los gastos m√©dicos.

Aqu√≠ tienes una gu√≠a completa para evaluar el modelo:

### ‚úÖ 1. Importar m√©tricas de evaluaci√≥n

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
```

### ‚úÖ 2. Calcular m√©tricas de desempe√±o

```python
# Predicciones
y_pred = model.predict(X_test)

# R¬≤ (coeficiente de determinaci√≥n)
r2 = r2_score(y_test, y_pred)

# Error absoluto medio
mae = mean_absolute_error(y_test, y_pred)

# Error cuadr√°tico medio
mse = mean_squared_error(y_test, y_pred)

# Ra√≠z del error cuadr√°tico medio
rmse = np.sqrt(mse)

# Mostrar resultados
print(f'R¬≤: {r2:.4f}')
print(f'MAE: {mae:.2f}')
print(f'MSE: {mse:.2f}')
print(f'RMSE: {rmse:.2f}')
```

### ‚úÖ 3. Interpretar resultados

| M√©trica  | Interpretaci√≥n                                                                                                                                    |
| -------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| **R¬≤**   | Mide qu√© porcentaje de la variabilidad en los gastos se explica por las variables independientes. 1 es perfecto, 0 significa que no explica nada. |
| **MAE**  | Promedio del error absoluto entre las predicciones y los valores reales.                                                                          |
| **MSE**  | Promedio de los errores al cuadrado. Penaliza m√°s los errores grandes.                                                                            |
| **RMSE** | Ra√≠z cuadrada del MSE. M√°s interpretable que el MSE, est√° en las mismas unidades que `charges`.                                                   |

### ‚úÖ 4. (Opcional) Gr√°fico de comparaci√≥n

```python
import matplotlib.pyplot as plt

plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel('Valores reales (charges)')
plt.ylabel('Predicciones (charges)')
plt.title('Predicciones vs Valores reales')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # L√≠nea ideal
plt.grid()
plt.show()
```

Si deseas mejorar la precisi√≥n del modelo, podr√≠as probar con **modelos m√°s avanzados** como:

* Ridge, Lasso (regresi√≥n regularizada)
* Random Forest o Gradient Boosting
* XGBoost

**Archivos de la clase**

[proyecto-final-template.ipynb](https://static.platzi.com/media/public/uploads/proyecto_final_template_ae2e550d-3eb9-46b0-83f7-9d83ede0a8e7.ipynb)
[proyecto-final.ipynb](https://static.platzi.com/media/public/uploads/proyecto_final_ca80d638-ee4b-42ae-9ddf-6881f2c7a43a.ipynb)

**Lecturas recomendadas**

[3.3. Metrics and scoring: quantifying the quality of predictions ‚Äî scikit-learn 1.0.2 documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)

[Proyecto_Final_Template.ipynb - Google Drive](https://drive.google.com/file/d/1PrSPTAVz6lXKAqNcTlUvWc_de8TRILOx/view?usp=sharing)

[Proyecto_Final.ipynb - Google Drive](https://drive.google.com/file/d/1UDV8y9eyZuKEDSIRKY7w93Bdo7EBqz5Z/view?usp=sharing)

## Mejorando el modelo

Para **mejorar tu modelo de regresi√≥n lineal para predecir los gastos m√©dicos de pacientes**, puedes aplicar varias estrategias. Aqu√≠ te dejo un flujo de trabajo guiado, con ejemplos en c√≥digo, asumiendo que ya tienes los datos listos (`X_train`, `X_test`, `y_train`, `y_test`):

### üß† 1. **Agregar caracter√≠sticas polin√≥micas**

Puede ayudar si la relaci√≥n entre variables no es lineal:

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Grado 2 (cuadr√°tico)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

model_poly = LinearRegression()
model_poly.fit(X_poly, y_train)

y_pred_poly = model_poly.predict(X_test_poly)
print("R¬≤ (polin√≥mica):", r2_score(y_test, y_pred_poly).round(4))
```

### üîÑ 2. **Escalar las variables (si no lo has hecho)**

Muy √∫til si combinas variables con diferentes escalas (por ejemplo, edad y n√∫mero de hijos):

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

model = LinearRegression()
model.fit(X_train_scaled, y_train)
y_pred_scaled = model.predict(X_test_scaled)
print("R¬≤ (escalado):", r2_score(y_test, y_pred_scaled).round(4))
```

### üî• 3. **Probar modelos m√°s potentes (como regularizaci√≥n)**

Para controlar el sobreajuste o mejorar con variables no relevantes:

#### a) **Ridge Regression** (L2)

```python
from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1.0)
ridge.fit(X_train_scaled, y_train)
y_pred_ridge = ridge.predict(X_test_scaled)
print("R¬≤ Ridge:", r2_score(y_test, y_pred_ridge).round(4))
```

#### b) **Lasso Regression** (L1 - hace selecci√≥n de variables)

```python
from sklearn.linear_model import Lasso

lasso = Lasso(alpha=0.01)
lasso.fit(X_train_scaled, y_train)
y_pred_lasso = lasso.predict(X_test_scaled)
print("R¬≤ Lasso:", r2_score(y_test, y_pred_lasso).round(4))
```

### üìä 4. **Ingenier√≠a de caracter√≠sticas**

* Convertir variables categ√≥ricas a dummies con `pd.get_dummies`
* Probar interacciones entre variables
* Agregar transformaciones no lineales (log, ra√≠z, etc.)

Ejemplo:

```python
import numpy as np
X_train["bmi_log"] = np.log(X_train["bmi"])
X_test["bmi_log"] = np.log(X_test["bmi"])
```

### üß™ 5. **Validaci√≥n cruzada para comparar**

Te permite evaluar cu√°l modelo generaliza mejor:

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')
print("R¬≤ promedio (CV):", scores.mean().round(4))
```

## ¬øQu√© hay m√°s all√° de la linealidad?

M√°s all√° de la **linealidad** en los modelos de regresi√≥n, existen m√©todos que permiten capturar **relaciones m√°s complejas y no lineales** entre las variables. Aqu√≠ te explico las principales alternativas y conceptos clave:

### üîπ 1. **Regresi√≥n Polin√≥mica**

* Transforma las variables originales en potencias (cuadrado, cubo, etc.).
* Ejemplo: en vez de ajustar una l√≠nea recta, ajusta una **curva**.
* √ötil cuando la relaci√≥n entre X e Y es curvil√≠nea.

```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
model = LinearRegression().fit(X_poly, y)
```

### üîπ 2. **Modelos No Param√©tricos**

* **√Årboles de Decisi√≥n**, **Random Forest**, **Gradient Boosting** y **XGBoost**: modelan relaciones complejas sin suponer una forma funcional expl√≠cita.
* Flexibles, pero pueden sobreajustar si no se regulan.

### üîπ 3. **Regresi√≥n con Splines**

* Divide el dominio de la variable en tramos y ajusta funciones diferentes (por ejemplo, polinomios) en cada tramo.
* Es suave y flexible para capturar formas no lineales.

### üîπ 4. **Modelos Basados en Kernels**

* **SVM con kernel RBF**, **Kernel Ridge Regression**, entre otros.
* Usan transformaciones no lineales impl√≠citas para separar/predicir datos en espacios de mayor dimensi√≥n.

### üîπ 5. **Redes Neuronales**

* Capturan relaciones altamente no lineales.
* √ötiles con muchos datos y relaciones complejas.
* Requieren m√°s recursos y tiempo de entrenamiento.

### üîπ 6. **Transformaciones de Variables**

* Aplicar funciones como logaritmos, ra√≠ces, exponenciales a las variables para linealizar relaciones no lineales.

### üìå En resumen:

Cuando la relaci√≥n entre variables no es lineal, puedes:

* Usar **regresi√≥n polin√≥mica**.
* Aplicar **transformaciones**.
* O directamente cambiar a **modelos m√°s flexibles**, como √°rboles o redes neuronales.